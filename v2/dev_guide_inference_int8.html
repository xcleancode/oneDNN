<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN: Int8 Inference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library (oneDNN)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library (oneDNN)</div>
    </a>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
   <div id="projectnumber">1.96.0</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('dev_guide_inference_int8.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Int8 Inference </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>To push higher performance during inference computations, recent work has focused on computing at a lower precision (that is, shrinking the size of data for activations and weights) to achieve higher throughput. Eight-bit computations (referred to as int8) offer improved performance over higher-precision types because they enable packing more data into a single instruction, at the cost of reduced (but acceptable) accuracy.</p>
<h2>Int8 Workflow</h2>
<p>There are different ways to use lower precision to perform inference. The <a class="el" href="dev_guide_attributes_quantization.html">Primitive Attributes: Quantization</a> page describes what kind of quantization model oneDNN supports.</p>
<h3>Quantization Process</h3>
<p>To operate with int8 data types from a higher-precision format (for example, 32-bit floating point), data must first be <em>quantized</em>. The quantization process converts a given input into a lower-precision format. The precision and accuracy factors are determined by the scaling factors.</p>
<h3>Range of the Data</h3>
<p>The data range is usually obtained by sampling the dataset of previous executions in the original data type (for example, the activations and weights from training in f32):</p>
<ul>
<li>\( R = \max(abs(T))\).</li>
</ul>
<p>Here \(T\) is a tensor corresponding to either the weights or the activations. Establishing the range of values used in the computation, and selecting a proper scaling factor, prevents over- or underflows during computation of the lower-precision results.</p>
<h3>Scaling Factor</h3>
<p>The <b>quantization factor</b> is used to convert the original values into the corresponding int8 range and is calculated as:</p>
<ul>
<li>\( Q_{\alpha} = \frac{255}{R_{\alpha}}\) is the quantization factor for activations with non-negative values.</li>
<li>\( Q_{w} = \frac{127}{R_{w}}\) is the quantization factor for weights.</li>
</ul>
<p>The <b>quantized</b> activation, weights, and bias values are calculated as:</p>
<ul>
<li>\(\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil \in [0,255]\),</li>
<li>\(W_{s8} = \lceil Q_{w} W_{f32} \rceil \in [-127,127]\),</li>
<li>\(b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil \in [-2^{31},2^{31}-1]\).</li>
</ul>
<p>Here \( \lceil \rceil \) denotes rounding according to the active rounding mode (typically determined by the MXCSR register; the default value is RoundNearestEven).</p>
<p>When the destination value is stored as a signed 32-bit integer, the result is bound to the same quantization <b>scaling factors</b>:</p>
<ul>
<li>\(X_{s32} = W_{s8} \cdot \alpha{u8} + b_{s32} \approx Q_{\alpha} Q_{\omega} X_{f32}\),</li>
<li>where \(X_{f32} = W_{f32} \cdot \alpha_{f32} + b_{f32}\).</li>
</ul>
<p>Here the approximation is used to denote rounding.</p>
<p>The dequantized value is calculated as</p>
<ul>
<li>\(X_{f32} \approx \frac{1}{Q_{\alpha} Q_{\omega}} X_{s32} \).</li>
</ul>
<h3>Quantization Example</h3>
<p>To show how the quantization parameters are obtained, suppose we first start with a set of high-precision input and output values. These values come from sampling a previously executed training run and are stored as 32-bit floating point values:</p>
<ul>
<li>activations: \( T_{\alpha} = [15, 14, 15, \ldots, 8, 11 ]\) where \( \max(abs(T_{\alpha})) = 15\)</li>
<li>weights: \( T_{\omega} = [-5.1 , 6.8, \ldots, -1.2, 9.8 ]\) where \( \max(abs(T_{\omega})) = 9.8\)</li>
<li>bias: \( T_{\alpha} = [ 2.4, -5.2, \ldots, -8 ]\) where \( \max(abs(T_{\alpha})) = 8\)</li>
</ul>
<p>The scaling factors are:</p>
<ul>
<li>\( Q_{\alpha} = \frac{255}{R_{\alpha}} = \frac{255}{15} = 17 \)</li>
<li>\( Q_{w} = \frac{127}{R_{w}} = \frac{127}{9.8} = 12.96\)</li>
</ul>
<p>Finally, the quantized input values for the int8 operation are calculated as:</p>
<ul>
<li>\(\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil\) \( = \Bigl \lceil 17 \cdot [15, 14, \ldots, 11 ] \Bigr \rceil = [255, 238, \ldots, 187] \)</li>
<li>\(W_{s8} = \lceil Q_{w} W_{f32} \rceil = \Bigl \lceil 12.96 \cdot [-5.1 , 6.8, \ldots, -1.2, 9.8 ] \Bigr \rceil = [-66, 88, \ldots, -15, 127] \)</li>
<li>\(b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil = \Bigl \lceil 17 \cdot 12.96 \cdot [ 2.4, -5.2 \ldots, -8 ] \Bigr \rceil = [528, -1145, \ldots, -1762] \)</li>
</ul>
<p>These arrays are the new inputs for the int8 net.</p>
<h2>int8 Support</h2>
<p>oneDNN supports int8 computations for inference by allowing one to specify that primitives' input and output memory objects use int8 data types. int8 primitive implementations are optimized for high performance on the compatible hardware (see <a class="el" href="dev_guide_data_types.html">Data Types</a>).</p>
<h3>Attributes</h3>
<p>Scaling factors can be configured using <b>primitive attributes</b>. It is also possible to specify fused post-ops. All primitives support the attributes, but not all combinations of parameters are supported. In the case of an unsupported combination, the library returns an error.</p>
<p>In oneDNN, the scaling factor is applied to the output of a primitive. Moreover, to perform input transformations (for example, source, bias, and weights), oneDNN performs quantizing and dequantizing of data for int8 using the <b>reorder primitive</b>.</p>
<p>oneDNN has two formats for defining the output scaling factor. Depending on the configuration set by the scaling mask, either the output is scaled uniformly across all the dimensions (<em>mask = 0</em>) or a set of scaling values is applied to specific dimensions, as explained below:</p>
<ul>
<li>A <em>single floating point value</em> shared across the tensor <div class="image">
<img src="img_singlescalar.png" alt="img_singlescalar.png"/>
<div class="caption">
Single-value scaling format</div></div>
</li>
<li>An array of floating point values each corresponding to a specific output channel <div class="image">
<img src="img_multiscalar.png" alt="img_multiscalar.png"/>
<div class="caption">
Multi-value scaling format</div></div>
 The <b>mask</b> parameter determines the dimension to which the scales array is applied. The \(i\)-th bit of the mask selects the dimension \(D_i\) of an \(n\)-dimensional output tensor \(T[D_0, \ldots, D_{n-1}]\). For example:</li>
<li>The single-scale format always has mask = 0.</li>
<li>For a 5-dimensional tensor \(T[G_0, O_1, I_2, H_3, W_4]\) where the indices correspond to the positions of bits in the mask:<ul>
<li>A \(mask = 2 = 2^1\) selects the output channel for scaling.</li>
<li>A \(mask = 3 = 2^1 | 2^0\) selects the group and output channels.</li>
</ul>
</li>
</ul>
<p>Fused <a class="el" href="dev_guide_attributes_post_ops.html">post-ops</a> allow chaining computations. Note that the resulting output value from post-ops is always affected by the scaling factor.</p>
<h2>Example</h2>
<p><a class="el" href="cnn_inference_int8_cpp.html">CNN int8 inference example</a> example walks through the steps of int8 inference. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>