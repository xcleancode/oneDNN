<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN: Transition from v0.x to v1.x</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library (oneDNN)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library (oneDNN)</div>
    </a>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
   <div id="projectnumber">1.96.0</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('dev_guide_transition_to_v1.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Transition from v0.x to v1.x </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p><b>NOTE</b></p>
<p>Starting with version 1.4 Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN) is renamed to oneAPI Deep Neural Network Library (oneDNN). For consistency, only this guide uses Intel MKL-DNN nomenclature. </p>
</blockquote>
<h2>Introduction</h2>
<p>This article describes user-visible and some important internal changes to Intel MKL-DNN that occurred between v0.20 and v1.0.</p>
<p>The v0.x branch (<a href="https://github.com/oneapi-src/oneDNN/tree/mnt-v0">mnt-v0</a>) is deprecated and users are strongly encouraged to migrate to <a href="https://github.com/oneapi-src/oneDNN">v1.x</a>.</p>
<dl class="section see"><dt>See also</dt><dd>Discussion on the API changes occurred in PR #384: <a href="https://github.com/oneapi-src/oneDNN/pull/384">RFC: API changes for the upcoming v1.0</a>.</dd></dl>
<h2>Summary of Changes</h2>
<p>We tried to keep changes minimal to make migration as simple as possible. In particular, the Intel MKL-DNN programming model stays the same. Nevertheless, the new version brings a lot of incompatible changes requiring developers to revisit significant portions of the integrated code.</p>
<p>All changes can be split into the following groups:</p><ol type="1">
<li>Minor API changes</li>
<li>Improving the library robustness</li>
<li>Simplified execution model</li>
<li>Changes in memory description</li>
<li>Changes in the build system</li>
</ol>
<p>These groups are discussed in detail below.</p>
<h2>1. Minor API Changes</h2>
<h3>1.1. Remove deprecated functionality</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Deprecated functionality  </th><th class="markdownTableHeadLeft">Repl   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">ReLU primitive  </td><td class="markdownTableBodyLeft"><a class="el" href="structdnnl_1_1eltwise__forward.html">Eltwise</a> with algorithm kind <a class="el" href="group__dnnl__api__attributes.html#gga00377dd4982333e42e8ae1d09a309640aba09bebb742494255b90b43871c01c69">ReLU</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">ConvolutionReLU (single primitive)  </td><td class="markdownTableBodyLeft">Convolution with ReLU as a <a class="el" href="dev_guide_attributes_post_ops.html">post operation</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">Double precision scales  </td><td class="markdownTableBodyLeft">Single precision scales   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">RNN backward pd w/o forward pd hint  </td><td class="markdownTableBodyLeft">RNN backward pd w/ forward pd hint   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><code>mkldnn_omit_stats</code> batch norm. flag  </td><td class="markdownTableBodyLeft"><code>mkldnn_use_global_stats</code>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><code>mkldnn_eltwise_desc_t.negative_slope</code>  </td><td class="markdownTableBodyLeft"><code>mkldnn_eltwise_desc_t.alpha</code>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><code>mkldnn_rnn_cell_flags_t</code>  </td><td class="markdownTableBodyLeft">Not available anymore &ndash; RNN primitives are separated into RNN, LSTM, and GRU   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><code>mkldnn_padding_kind_t</code>  </td><td class="markdownTableBodyLeft">Not used anymore   </td></tr>
</table>
<p>The complete list of the removed C functions: </p><div class="fragment"><div class="line">mkldnn_relu_forward_desc_init(...);</div><div class="line">mkldnn_relu_backward_desc_init(...);</div><div class="line">mkldnn_convolution_relu_desc_init(...);</div><div class="line">mkldnn_rnn_cell_desc_init(...);</div><div class="line">mkldnn_rnn_cell_get_gates_count(...);</div><div class="line">mkldnn_rnn_cell_get_states_count(...);</div><div class="line">mkldnn_rnn_forward_desc_init(...);</div><div class="line">mkldnn_rnn_backward_desc_init(...);</div></div><!-- fragment --><p>The complete list of the removed C++ classes and functions: </p><div class="fragment"><div class="line"><span class="keyword">struct </span>mkldnn::convolution_relu_forward {}</div><div class="line"><span class="keyword">struct </span>mkldnn::relu_forward {}</div><div class="line"><span class="keyword">struct </span>mkldnn::relu_backward {}</div><div class="line"><span class="keyword">struct </span>mkldnn::rnn_cell {}</div><div class="line"><span class="keyword">struct </span>mkldnn::rnn_forward {}</div><div class="line"><span class="keyword">struct </span>mkldnn::rnn_backward {}</div><div class="line"></div><div class="line">mkldnn::sum::primitive_desc(<span class="keyword">const</span> memory::desc &amp;output, std::vector&lt;double&gt; scale, std::vector&lt;memory::primitive_desc&gt; inputs);</div><div class="line">mkldnn::sum::primitive_desc(std::vector&lt;double&gt; scale, std::vector&lt;memory::primitive_desc&gt; inputs);</div><div class="line">mkldnn::eltwise_forward::desc(<a class="code" href="group__dnnl__api__attributes.html#gac7db48f6583aa9903e54c2a39d65438f">prop_kind</a> aprop_kind, <span class="keyword">const</span> memory::desc &amp;src_desc, T negative_slope);</div><div class="line">mkldnn::eltwise_backward::desc(<span class="keyword">const</span> memory::desc &amp;diff_data_desc, <span class="keyword">const</span> memory::desc &amp;data_desc, T negative_slope);</div></div><!-- fragment --><h3>1.2. Rename <code>foo_v2()</code> to <code>foo()</code> and remove old <code>foo()</code> (C API only)</h3>
<p>The functions like: </p><div class="fragment"><div class="line">mkldnn_primitive_desc_create_v2(...);</div></div><!-- fragment --><p> were renamed to: </p><div class="fragment"><div class="line">mkldnn_primitive_desc_create(...);</div></div><!-- fragment --><p>In v0.x, the <code>foo_v2()</code> functions typically were used to pass <a class="el" href="dev_guide_attributes.html">attributes</a>, and <code>foo()</code> assumed empty attributes. In v1.0, the attributes parameter is mandatory. A user can still pass <code>NULL</code> to indicate that the default (empty) attributes should be used.</p>
<p>The list of functions that had the <code>_v2</code> suffix:</p>
<div class="fragment"><div class="line">mkldnn_primitive_desc_iterator_create_v2(...);</div><div class="line">mkldnn_primitive_desc_create_v2(...);</div><div class="line">mkldnn_reorder_primitive_desc_create_v2(...);</div></div><!-- fragment --><h3>1.3. Remove s16 (int16_t) data type support</h3>
<p>The experimental <code>s16</code> data type is not supported any more and has been dropped.</p>
<h3>1.4. Disallow setting the rounding mode</h3>
<p>Rounding mode that was a part of attributes has been dropped. All computations respect the MXCSR register when performing rounding. Unless the rounding mode is set explicitly, rounding to the nearest even integer (RNE) is used.</p>
<h3>1.5. Rename a few types, enumerations, and functions</h3>
<h4>1.5.1. Types</h4>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">API  </th><th class="markdownTableHeadLeft">v0.x  </th><th class="markdownTableHeadLeft">v1.   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">C  </td><td class="markdownTableBodyLeft">mkldnn_batch_normalization_flag_t  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#ga301f673522a400c7c1e75f518431c9a3">mkldnn_normalization_flags_t</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">C  </td><td class="markdownTableBodyLeft">mkldnn_format_t  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__memory.html#ga395e42b594683adb25ed2d842bb3091d">mkldnn_format_tag_t</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">C++  </td><td class="markdownTableBodyLeft">mkldnn::batch_normalization_flag  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#ggad8ef0fcbb7b10cae3d67dd46892002bea95768ff8afb8ee75dc24be0d307627f8">mkldnn::normalization_flags</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">C++  </td><td class="markdownTableBodyLeft">mkldnn::memory::format  </td><td class="markdownTableBodyLeft"><a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3f">mkldnn::memory::format_tag</a>   </td></tr>
</table>
<h4>1.5.2. Enumerations</h4>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">API  </th><th class="markdownTableHeadLeft">v0.x  </th><th class="markdownTableHeadLeft">v1.   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">C  </td><td class="markdownTableBodyLeft">mkldnn_fuse_bn_relu  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#gga301f673522a400c7c1e75f518431c9a3a7150bdb66ef194e6ee11fbaa85a34ada">mkldnn_fuse_norm_relu</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">C++  </td><td class="markdownTableBodyLeft">mkldnn::fuse_bn_relu  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#ggad8ef0fcbb7b10cae3d67dd46892002bea898ce555425ee54271096bc9c8e0400c">mkldnn::normalization_flags::fuse_norm_relu</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">C++  </td><td class="markdownTableBodyLeft">mkldnn::query::eengine  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#gga94efdd650364f4d9776cfb9b711cbdc1aad1943a9fd6d3d7ee1e6af41a5b0d3e7">mkldnn::query::engine</a>   </td></tr>
</table>
<h4>1.5.3. Functions</h4>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">API  </th><th class="markdownTableHeadLeft">v0.x  </th><th class="markdownTableHeadLeft">v1.   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">C  </td><td class="markdownTableBodyLeft">mkldnn_memory_desc_init()  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__memory.html#gaff696e368aeefb3036a0419c508dc6be">mkldnn_memory_desc_init_by_tag()</a>   </td></tr>
</table>
<h3>1.6. Unscoped enumerations become scoped (C++ API only)</h3>
<p>All <code>enum</code> became <code>enum class</code>. This requires the following changes:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Type  </th><th class="markdownTableHeadLeft">Value in v0.x  </th><th class="markdownTableHeadLeft">Val   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">mkldnn::prop_kind  </td><td class="markdownTableBodyLeft">mkldnn::forward_inference  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa3b9fad4f80d45368f856b5403198ac4c">mkldnn::prop_kind::forward_inference</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">mkldnn::algorithm  </td><td class="markdownTableBodyLeft">mkldnn::eltwise_tanh  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__attributes.html#gga00377dd4982333e42e8ae1d09a309640a38dd7159307eab45742c78e72f06abb0">mkldnn::algorithm::eltwise_tanh</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">mkldnn::normalization_flags  </td><td class="markdownTableBodyLeft">mkldnn::fuse_bn_norm_relu  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#ggad8ef0fcbb7b10cae3d67dd46892002bea898ce555425ee54271096bc9c8e0400c">mkldnn::normalization_flags::fuse_norm_relu</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">mkldnn::query  </td><td class="markdownTableBodyLeft">mkldnn::eengine  </td><td class="markdownTableBodyLeft"><a class="el" href="group__dnnl__api__primitives__common.html#gga94efdd650364f4d9776cfb9b711cbdc1aad1943a9fd6d3d7ee1e6af41a5b0d3e7">mkldnn::query::engine</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">mkldnn::memory::data_type  </td><td class="markdownTableBodyLeft">mkldnn::memory::f32  </td><td class="markdownTableBodyLeft"><a class="el" href="structdnnl_1_1memory.html#a8e83474ec3a50e08e37af76c8c075dcea512dc597be7ae761876315165dc8bd2e">mkldnn::memory::data_type::f32</a>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">mkldnn::memory::format_tag  </td><td class="markdownTableBodyLeft">mkldnn::memory::nchw  </td><td class="markdownTableBodyLeft"><a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3faded7ac40158367123c5467281d44cbeb">mkldnn::memory::format_tag::nchw</a>   </td></tr>
</table>
<h3>1.7. Remove view primitive</h3>
<p>Version 0.x had an implementation of view that was simply an alias for memory. In Intel MKL-DNN v1.0, we removed view as a type and replaced it with a memory descriptor directly. In order to initialize sub-memory, use <a class="el" href="structdnnl_1_1memory_1_1desc.html#a7de2abef3b34e94c5dfa16e1fc3f3aab">mkldnn::memory::desc::submemory_desc()</a>.</p>
<dl class="section see"><dt>See also</dt><dd>For more detail, refer to section <a href="https://github.com/oneapi-src/oneDNN/tree/rfc-api-changes-v1.0/doc/rfc/api-v1.0#4-view-rework">4. View rework</a> of the <a href="https://github.com/oneapi-src/oneDNN/pull/384">RFC for v1.0</a>.</dd></dl>
<h3>1.8. RNN-specific changes</h3>
<p>Each type of <a class="el" href="group__dnnl__api__rnn.html">RNN</a> (Vanilla RNN, LSTM, and two types of GRU) is now initialized by a separate function/operation descriptor constructor.</p>
<p>For instance, instead of using mkldnn::rnn_forward with specified RNN types a user is expected to use:</p><ul>
<li><a class="el" href="structdnnl_1_1vanilla__rnn__forward.html">mkldnn::vanilla_rnn_forward</a> for Vanilla RNN</li>
<li><a class="el" href="structdnnl_1_1lstm__forward.html">mkldnn::lstm_forward</a> for LSTM</li>
<li><a class="el" href="structdnnl_1_1gru__forward.html">mkldnn::gru_forward</a> for GRU</li>
<li><a class="el" href="structdnnl_1_1lbr__gru__forward.html">mkldnn::lbr_gru_forward</a> for the linear-before-reset variant of GRU</li>
</ul>
<p>Also, the hidden and cell states in LSTM are now separated. This means that instead of one <code>src_iter</code> tensor of shape <code>(layers, directions, states, batch, channels)</code> a user passes <code>src_iter</code> tensor of shape <code>(layers, directions, batch, channels)</code> for hidden states and <code>src_iter_c</code> tensor of shape <code>(layers, directions, batch, channels)</code> for cell states. The same applies to <code>dst_iter</code>; the hidden state and the cell state are split into <code>dst_iter</code> and <code>dst_iter_c</code> respectively.</p>
<h3>1.9. GEMM API changes</h3>
<p>Intel MKL-DNN provides three GEMM-like functions:</p><ul>
<li><a class="el" href="group__dnnl__api__blas.html#ga75ee119765bdac249200fda42c0617f8">mkldnn_sgemm()</a> &ndash; Single precision matrix-matrix multiply</li>
<li><a class="el" href="group__dnnl__api__blas.html#gaef24848fd198d8a178d3ad95a78c1767">mkldnn_gemm_u8s8s32()</a> &ndash; u8/s8 integer matrix-matrix multiply</li>
<li><a class="el" href="group__dnnl__api__blas.html#ga2b763b7629846913507d88fba875cc26">mkldnn_gemm_s8s8s32()</a> &ndash; s8/s8 integer matrix-matrix multiply</li>
</ul>
<p>With version 1.0 we switched from a Fortran-style to a C-style API, meaning that the parameters are passed by value rather than by address, and matrices are assumed to be in row-major format rather than column-major format.</p>
<p>Moreover, to broaden the applicability of integer matrix-matrix multiply functions we changed the formula from: </p><p class="formulaDsp">
\[ C_{s32} = \alpha \cdot (op(A_{i8}) + o_A) \cdot (op(B_{s8}) + o_B) + \beta \cdot C_{s32} + o_C \]
</p>
<p> to </p><p class="formulaDsp">
\[ C_{s32} = \alpha \cdot (op(A_{i8}) - o_A) \cdot (op(B_{s8}) - o_B) + \beta \cdot C_{s32} + o_C \]
</p>
<p>where for both <a class="el" href="group__dnnl__api__blas.html#gaef24848fd198d8a178d3ad95a78c1767">mkldnn_gemm_u8s8s32()</a> and <a class="el" href="group__dnnl__api__blas.html#ga2b763b7629846913507d88fba875cc26">mkldnn_gemm_s8s8s32()</a> the types of offsets for matrices A and B correspond to the type of the matrices themselves; that is:</p><ul>
<li><code>typeof(o_A) == typeof(*A)</code> and</li>
<li><code>typeof(o_B) == typeof(*B)</code>.</li>
</ul>
<h3>1.10. Primitive descriptor queries for memory descriptors</h3>
<p>In version 0.x when querying the primitive descriptor for a memory descriptor that is not used, the C API returned NULL and the C++ API threw an exception. In version 1.0, both the C and C++ APIs return a zero memory descriptor.</p>
<p>Zero memory descriptor means that the number of dimensions equals 0 and all the fields are set to zero. A memory object created with such a memory descriptor does not require any buffer allocations.</p>
<p>These changes enable simplifying the code that handles <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_workspace">workspace</a> or <a class="el" href="dev_guide_attributes_scratchpad.html">scratchpad</a>:</p>
<div class="fragment"><div class="line">    <span class="comment">// The code works fine even if scratchpad is not required.</span></div><div class="line">    <span class="comment">// In this case the memory would be just zero memory.</span></div><div class="line"></div><div class="line">    auto scratchpad_md = pd.scratchpad_desc();</div><div class="line">    auto scratchpad = memory(scratchpad_md, pd.get_engine());</div><div class="line"></div><div class="line">    primitive.execute(stream, {</div><div class="line">        ...,</div><div class="line">        {MKLDNN_SCRATCHPAD, scratchpad}};</div></div><!-- fragment --><h3>1.11. Default constructors for C++ classes (C++ API only)</h3>
<p>In Intel MKL-DNN v1.0, all C++ objects (primitives, memory objects, engines, and streams) now have default empty constructors. This enables defining the object, and then initializing it later on. An attempt to use any methods of an uninitialized object will result in the throwing of an exception.</p>
<p>This improvement can be especially useful when Intel MKL-DNN objects are members of the user's classes. For example:</p>
<div class="fragment"><div class="line"><span class="keyword">class </span>RELU_layer {</div><div class="line"><span class="keyword">public</span>:</div><div class="line">    RELU_layer() {} <span class="comment">// no need to initialize eltwise here</span></div><div class="line"></div><div class="line">    <span class="keywordtype">void</span> init() {</div><div class="line">        ...</div><div class="line">        <span class="comment">// deferred initialization</span></div><div class="line">        eltwise = eltwise_forward(...);</div><div class="line">    }</div><div class="line"></div><div class="line"><span class="keyword">private</span>:</div><div class="line">    eltwise_forward eltwise;</div><div class="line">};</div></div><!-- fragment --><h2>2. Improving the Library Robustness</h2>
<h3>2.1. Memory allocation in the C API</h3>
<p>In Intel MKL-DNN v1.0, constructing a memory object using special value <code>MKLDNN_MEMORY_ALLOCATE</code> for a handle results in the buffer being allocated by the library. This makes the behavior of the C API memory object constructor aligned with its C++ API <code>mkldnn::memory</code> counterpart. Note that the C++ API memory object class still has an extra constructor that does not take a handle at all, and asks the library to allocate the buffer (that is, the same behavior as calling with the handle equal to <code>MKLDNN_MEMORY_ALLOCATE</code>).</p>
<h3>2.2. Explicit scratchpad management</h3>
<p>Intel MKL-DNN primitives may require temporary <a class="el" href="dev_guide_attributes_scratchpad.html">scratchpad memory</a> for storing intermediate computational results. For instance, convolution backward by weights typically requires extra space to perform a reduction of the <code>diff_weights</code> computed by different threads (the work is divided across images). Starting with version 1.0, the library supports two modes:</p><ol type="1">
<li>Implicit scratchpad, managed by the library (<b>default</b>). See <a href="#dnnl::scratchpad_mode::library">mkldnn::scratchpad_mode::library</a>.</li>
<li>Explicit scratchpad, provided by the user. See <a href="#dnnl::scratchpad_mode::user">mkldnn::scratchpad_mode::user</a>.</li>
</ol>
<p>The former mode matches the behavior of Intel MKL-DNN v0.x. It is kept for user convenience and cases in which memory is not a concern.</p>
<p>In the explicit scratchpad mode, a new <code>mkldnn_query_scratchpad_md</code> query will return the amount of scratchpad memory needed for a primitive, and the user will be responsible for allocating and providing the scratchpad memory to a primitive at runtime. The explicit scratchpad mode should be <em>explicitly</em> enabled by passing an attribute with <code>mkldnn::scratchpad_mode::user</code> to primitive descriptors.</p>
<dl class="section warning"><dt>Warning</dt><dd><a class="el" href="dev_guide_attributes_scratchpad.html">Scratchpad</a> memory is not the same as <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_workspace">workspace</a>.</dd></dl>
<p>With explicit scratchpad it is possible to make Intel MKL-DNN primitives stateless and hence thread safe: the same primitive can be executed in multiple independent threads as long as different threads use different scratchpads.</p>
<p>However, if a user chooses implicit scratchpad mode, there is no thread-safety guarantee.</p>
<h2>3. Simplified Execution Model</h2>
<p>This is the most notable change in the library. The main idea was to change the execution API so that memory arguments are specified at primitive execution time and not at primitive creation time. This leads to the following changes.</p>
<h3>3.1. Memory is not a primitive anymore</h3>
<p>In version 0.x, memory had a type of primitive. With the new API, memory becomes a distinct data type. Moreover, a memory primitive descriptor becomes redundant and has been dropped. The functions that use memory primitive descriptors now take memory descriptor and (optionally) engine, if the latter cannot be inferred.</p>
<p>These changes bring new data types and functions, such as:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#define MKLDNN_NATIVE_HANDLE_ALLOCATE  ((void *)-1)</span></div><div class="line"><span class="preprocessor">#define MKLDNN_NATIVE_HANDLE_NONE      ((void *)0)</span></div><div class="line"></div><div class="line"><span class="keyword">struct </span>mkldnn_memory_t; <span class="comment">// memory type, no more equal to mkldnn_primitive_t</span></div><div class="line"></div><div class="line"><span class="comment">// create a memory</span></div><div class="line"><span class="comment">// native_handle can:</span></div><div class="line"><span class="comment">//  - point to the user allocated memory, i.e. valid handle. In this case the</span></div><div class="line"><span class="comment">//    library does not own allocated memory.</span></div><div class="line"><span class="comment">//  - be MKLDNN_NATIVE_HANDLE_ALLOCATE to ask the library to allocate and</span></div><div class="line"><span class="comment">//    attach memory. In this case the library owns allocated memory.</span></div><div class="line"><span class="comment">//  - be MKLDNN_NATIVE_HANDLE_NONE to create mkldnn_memory w/o attached memory.</span></div><div class="line">mkldnn_status_t mkldnn_memory_create(mkldnn_memory_t *mem,</div><div class="line">        <span class="keyword">const</span> mkldnn_memory_desc_t *md, mkldnn_engine_t engine,</div><div class="line">        <span class="keywordtype">void</span> *handle);</div></div><!-- fragment --><h3>3.2. Operation primitives cannot be used as inputs (use memory instead)</h3>
<p>Version 0.x allowed passing an operation primitive as an input to another primitive. For instance, a convolution primitive could be passed as an input to a consequent ReLU. During the execution the ReLU primitive queried the convolution for its output memory and used it as an input.</p>
<p>In version 1.0, users are allowed to pass only memory type as inputs and outputs for primitives.</p>
<h3>3.3. Remove the <code>mkldnn_primitive_at_t</code> type</h3>
<p>Another consequence is that <code>mkldnn_primitive_at_t</code>, which is logically equivalent to <code>{primitive, output_index}</code>, becomes redundant. Previously the type was used to specify the exact memory to use (if a primitive had several outputs).</p>
<h3>3.4. Passing stream and input/output memories at primitive execution</h3>
<p>Finally, users are now able to directly run primitives by calling an <code>execute</code> function instead of putting primitives into a stream and running the latter. This change affects how primitives interact with streams and input/output memory objects: with the new API they become arguments to be passed to the primitive execution function.</p>
<p>The change significantly simplifies primitive creation, which now requires a primitive descriptor only:</p>
<div class="fragment"><div class="line">mkldnn_status_t mkldnn_primitive_create(mkldnn_primitive_t *primitive,</div><div class="line">        const_mkldnn_primitive_desc_t *pd);</div></div><!-- fragment --><p>To remove the ambiguity in which order input and output memories need to be passed, we introduced a map-like argument in which each memory argument is paired with a tag indicating what kind of argument it is: destination, source, weights, and so on.</p>
<div class="fragment"><div class="line"><span class="comment">// types</span></div><div class="line"><span class="preprocessor">#define MKLDNN_ARG_SRC_0 1</span></div><div class="line"><span class="preprocessor">#define MKLDNN_ARG_SRC   MKLDNN_ARG_SRC_0</span></div><div class="line"><span class="preprocessor">#define MKLDNN_ARG_FROM  MKLDNN_ARG_SRC_0</span></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// C API</span></div><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span>{</div><div class="line">    <span class="keywordtype">int</span> arg; <span class="comment">// MKLDNN_ARG_SRC, ...</span></div><div class="line">    mkldnn_memory_t memory;</div><div class="line">} mkldnn_exec_arg_t;</div><div class="line"></div><div class="line">mkldnn_status_t mkldnn_primitive_execute(mkldnn_primitive_t prim,</div><div class="line">        mkldnn_stream_t stream, <span class="keywordtype">int</span> nargs, <span class="keyword">const</span> mkldnn_exec_arg_t *args);</div><div class="line"></div><div class="line"><span class="comment">// C++ API</span></div><div class="line">convolution_forward::execute(mkldnn::stream &amp;stream,</div><div class="line">        <span class="keyword">const</span> std::map&lt;int, mkldnn::memory&gt; &amp;exec_args);</div><div class="line"><span class="comment">// ... other primitives ...</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// example C, convolution forward w/ bias</span></div><div class="line">mkldnn_exec_arg_t conv_exec_args[] = {</div><div class="line">    {MKLDNN_ARG_SRC, src_mem},</div><div class="line">    {MKLDNN_ARG_WEIGHTS, weights_mem},</div><div class="line">    {MKLDNN_ARG_BIAS, bias_mem},</div><div class="line">    {MKLDNN_ARG_DST, dst_mem},</div><div class="line">};</div><div class="line">mkldnn_primitive_execute(conv_fwd, stream, 4, conv_exec_args);</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// example C++, in-place eltwise</span></div><div class="line">eltwise.execute(stream, {{MKLDNN_ARG_SRC, mem}, {MKLDNN_ARG_DST, mem}});</div></div><!-- fragment --><h3>3.5 Short summary</h3>
<p>The example below shows conceptual code transformations between versions. The C++ API is used for brevity.</p>
<p>#### Version 0.x: </p><div class="fragment"><div class="line"><span class="comment">// create a convolution, specify all inputs and outputs</span></div><div class="line"><span class="keyword">auto</span> conv = convolution(conv_pd,</div><div class="line">            {src_mem, 0}, {wei_mem, 0}, dst_conv_mem);</div><div class="line"></div><div class="line"><span class="comment">// create a relu (note that one of inputs is the convolution)</span></div><div class="line"><span class="keyword">auto</span> relu = relu(relu_pd,</div><div class="line">            {conv, 0}, dst_relu_mem);</div><div class="line"></div><div class="line"><span class="comment">// create a stream, submit convolution and relu, and wait for the result</span></div><div class="line">stream().submit({conv, relu}).wait();</div></div><!-- fragment --><p>#### Version 1.0: </p><div class="fragment"><div class="line"><span class="comment">// create convolution and relu. no inputs/outputs</span></div><div class="line"><span class="keyword">auto</span> conv = convolution(conv_pd);</div><div class="line"><span class="keyword">auto</span> relu = relu(relu_pd);</div><div class="line"></div><div class="line"><span class="comment">// create stream (based on engine)</span></div><div class="line">stream s(engine, 0);</div><div class="line"></div><div class="line"><span class="comment">// execute the convolution with given inputs, outputs</span></div><div class="line">conv.execute(s, {</div><div class="line">        {MKLDNN_ARG_SRC, src_mem},</div><div class="line">        {MKLDNN_ARG_WEIGHTS, wei_mem},</div><div class="line">        {MKLDNN_ARG_DST, dst_conv_mem}});</div><div class="line"></div><div class="line"><span class="comment">// execute the relu. cannot pass convolution as an input, only memory is allowed</span></div><div class="line">relu.execute(s, {</div><div class="line">        {{MKLDNN_ARG_SRC, dst_conv_mem},</div><div class="line">        {MKLDNN_ARG_DST, dst_relu_mem}});</div><div class="line"></div><div class="line">s.wait(); <span class="comment">// wait for async streams</span></div></div><!-- fragment --><h2>4. Changes in Memory Description</h2>
<p>The way of describing memory format in version 0.x had multiple issues. From the user's perspective, the main issues were:</p><ul>
<li>Some memory formats were missing. For example, the <code>iohw</code> format was not available.</li>
<li>There were multiple ambiguous ways to describe memory. For example, <code>oihw</code> described memory in the same way as <code>nchw</code>, but these formats were different (see <a href="https://github.com/oneapi-src/oneDNN/issues/153">gh#153</a>).</li>
<li>Support for custom formats was limited.</li>
<li>Support for memory views was limited.</li>
</ul>
<p>There were more substantial issues from the library development perspective: code bloat to support special cases, etc.</p>
<p>We addressed the issues above by reworking memory descriptors. From the user's perspective, the main changes are:</p><ol type="1">
<li>Memory descriptors support arbitrary strides for plain layouts. For example, initializing a memory descriptor with <code>strides={h*w, o*h*w, w, 1}</code> should be a valid way to define <code>iohw</code> format even if Intel MKL-DNN does not support it explicitly. Functions to use:<ul>
<li>C++ API: <a class="el" href="structdnnl_1_1memory_1_1desc.html#a2a12f9b43aae8c214d695b321b543b5c">mkldnn::memory::desc::desc(dims, data_type, strides)</a>,</li>
<li>C API: <a class="el" href="group__dnnl__api__memory.html#ga77c4ac2c6c59730ade594b954c145f73">mkldnn_memory_desc_init_by_strides()</a>.</li>
</ul>
</li>
<li>Dimensions are of type <code>int64_t</code> instead of int, and the maximum number of tensor dimensions is decreased from 16 to 12. The <code>mkldnn_strides_t</code> is removed; use <code>mkldnn_dims_t</code> instead.</li>
<li>The <code>memory_desc_t.format</code> field is replaced with <code>memory_desc_t.format_kind</code>, which also has different semantics.</li>
</ol>
<p>While the first two items are self-explanatory, the last one requires some elaboration.</p>
<p>In version 0.x, most memory formats could be described directly by using appropriate format names (for example, <code>nchw</code>) that fully describe how data is laid out in memory. However, Intel MKL-DNN also had the <code>blocked</code> memory format and the corresponding <code>memory_desc_t.layout_desc.blocking_desc</code> structure, which could describe a memory format in a unified fashion by specifying block sizes and strides. The original idea was to use format tags like <code>nchw</code> during memory descriptor initialization only, and always use the <code>blocked</code> format internally. Unfortunately, that was never implemented.</p>
<p>With the new design, Intel MKL-DNN starts distinguishing between the actual memory format and convenience memory format tags that can be used to describe memory format concisely.</p>
<p>Users are still able to initialize memory descriptors with format tags like <code>nchw</code> using <a class="el" href="structdnnl_1_1memory_1_1desc.html#a2a12f9b43aae8c214d695b321b543b5c">mkldnn::memory::desc::desc(dims, data_type, format_tag)</a> or <a class="el" href="group__dnnl__api__memory.html#gaff696e368aeefb3036a0419c508dc6be">mkldnn_memory_desc_init_by_tag()</a>, but the <code>memory_desc_t.format_kind</code> is set to a canonicalized kind like <code>blocked</code>, and the format name is not recorded in the memory descriptor structure. Initialization with strides will always result in <code>blocked</code> format. The API also uses different types for memory format tags and kinds to aid correctness.</p>
<p>For more details, refer to the https://github.com/oneapi-src/oneDNN/blob/rfc-api-changes-v1.0/doc/rfc/api-v1.0/rfc_memory_desc.md "Memory descriptor article" of the <a href="https://github.com/oneapi-src/oneDNN/pull/384">RFC for v1.0</a>.</p>
<h2>5. Changes in the Build System</h2>
<p>The build options were slightly changed in the new version of Intel MKL-DNN. That was done mainly to avoid name collisions with other projects that include Intel MKL-DNN as a subproject and to accommodate future extensions to the library. The change are:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Old option  </th><th class="markdownTableHeadLeft">New option  </th><th class="markdownTableHeadLeft">Notes   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">WITH_EXAMPLE  </td><td class="markdownTableBodyLeft">MKLDNN_BUILD_EXAMPLES  </td><td class="markdownTableBodyLeft"></td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">WITH_TEST  </td><td class="markdownTableBodyLeft">MKLDNN_BUILD_TESTS  </td><td class="markdownTableBodyLeft"></td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">MKLDNN_THREADING  </td><td class="markdownTableBodyLeft">MKLDNN_CPU_RUNTIME  </td><td class="markdownTableBodyLeft"></td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">MKLDNN_USE_MKL  </td><td class="markdownTableBodyLeft">N/A  </td><td class="markdownTableBodyLeft">Intel MKL-DNN does not use Intel MKL anymore   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">VTUNEROOT  </td><td class="markdownTableBodyLeft">N/A  </td><td class="markdownTableBodyLeft">Not required, as Intel MKL-DNN contains all the necessary code internally   </td></tr>
</table>
<p>By default, the <code>-Werror</code> flag is disabled. <code>MKLDNN_WERROR</code> controls the behavior.</p>
<p>For more information about build options, refer to <a class="el" href="dev_guide_build_options.html">Build Options</a>. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>