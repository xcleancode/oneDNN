<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Memory format propagation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">1.0.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Memory format propagation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Upon compiling and run the example the output should be just:</p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="cpu_memory_format_propagation_8cpp-example.html">cpu_memory_format_propagation.cpp</a></p>
</blockquote>
<p>Format propagation is one of the central notions that needs to be well-understood to use Intel MKL-DNN correctly.</p>
<p>Convolution and inner product primitives choose the memory format when you create them with the placeholder memory format <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec">mkldnn::memory::format_tag::any</a> for input or output. The memory format chosen is based on different circumstances such as hardware and convolutional parameters. Using the placeholder memory format is the recommended practice for convolutions, since they are the most compute-intensive operations in most topologies where they are present.</p>
<p>Other primitives, such as ReLU, LRN, batch normalization and other, should use the same memory format as the preceding layer thus propagating the memory format through multiple Intel MKL-DNN primitives. This avoids unnecessary reorders which may be expensive and should be avoided unless a compute-intensive primitive requires a different format.</p>
<p>Additional format synchronization is required between forward and backward computations when running training workloads. This topic is covered in <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_training">Training-Specific Aspects</a>.</p>
<p>For better understanding of the architecture and design of Intel MKL-DNN as well as the concepts used in the library, please refer to <a class="el" href="dev_guide_understanding_memory_formats.html">Understanding Memory Formats</a>.</p>
<h1><a class="anchor" id="cpu_memory_format_propagation_intro"></a>
Introduction to the tutorial</h1>
<p>This C++ API example demonstrates how to use optimized memory formats supported by Intel MKL-DNN:</p><ul>
<li>How to configure primitives to use optimized memory formats.</li>
<li>How to determine whether data needs to be reordered from/to optimized memory formats.</li>
</ul>
<p>This tutorial assumes that the reader has already reviewed the <a class="el" href="cpu_getting_started_cpp.html">Getting started</a> tutorial.</p>
<p>The example is built around a CNN consisting of a convolution followed by a pooling and consists of the following steps:</p><ol type="1">
<li>Create a pooling primitive descriptor based on the memory format chosen by the convolution primitive.</li>
<li>Create memory descriptors for input and output data in the NHWC memory format.</li>
<li>Determine if input and output data needs to be reordered from/to the optimized memory format.</li>
<li>Create memory objects; and necessary primitives and execute them.</li>
</ol>
<p>These steps are implemented in the <a class="el" href="cpu_memory_format_propagation_cpp.html#cpu_memory_format_propagation_tutorial">cpu_memory_format_propagation() function</a> which in turn is called from <code>main()</code> which is also responsible for error handling.</p>
<h1><a class="anchor" id="cpu_memory_format_propagation_tutorial"></a>
cpu_memory_format_propagation() function</h1>
<h2><a class="anchor" id="cpu_memory_format_propagation_sub1"></a>
Initialization</h2>
<p>We start by creating a CPU engine and a stream that we will use when creating primitive descriptors and executing primitives.</p>
<div class="fragment"><div class="line">    <a class="code" href="group__cpp__api__enums.html#gga6d88ff11a07bae09a5c348d314c5d1d9aad1943a9fd6d3d7ee1e6af41a5b0d3e7">engine</a> cpu_engine(engine::kind::cpu, 0);</div><div class="line">    stream cpu_stream(cpu_engine);</div></div><!-- fragment --> <h2><a class="anchor" id="cpu_memory_format_propagation_sub2"></a>
Create convolution and pooling primitives</h2>
<p>To specify that a primitive should pick an optimized format for the specified computation parameters, we create memory descriptors with memory format set to <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec">mkldnn::memory::format_tag::any</a>.</p>
<p>This approach works only for a limited set of primitives: convolutions and inner products. Additionally, <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec">mkldnn::memory::format_tag::any</a> can be specified for destination memory descriptors which implies that destination will have the same memory format as the source.</p>
<div class="fragment"><div class="line">    <span class="comment">// Tensor and kernel dimensions. We use the same 3x3 kernel with padding=1</span></div><div class="line">    <span class="comment">// for both convolution and pooling primitives, which means that the</span></div><div class="line">    <span class="comment">// activation tensor shapes do not change.</span></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> N = 1, H = 14, W = 14, IC = 256, OC = IC, KH = 3, KW = 3;</div><div class="line">    <span class="keyword">auto</span> conv_src_md = memory::desc(</div><div class="line">            {N, IC, H, W}, memory::data_type::f32,</div><div class="line">            memory::format_tag::any <span class="comment">// let convolution choose memory format</span></div><div class="line">            );</div><div class="line">    <span class="keyword">auto</span> conv_weights_md = memory::desc(</div><div class="line">            {IC, OC, KH, KW}, memory::data_type::f32,</div><div class="line">            memory::format_tag::any <span class="comment">// let convolution choose memory format</span></div><div class="line">            );</div><div class="line">    <span class="keyword">auto</span> conv_dst_md = conv_src_md; <span class="comment">// shape does not change</span></div><div class="line">    <span class="keyword">auto</span> pool_dst_md = conv_dst_md; <span class="comment">// shape does not change</span></div></div><!-- fragment --><p> Next, we pass the memory descriptors to primitive descriptors constructors.</p>
<h2><a class="anchor" id="cpu_memory_format_propagation_sub3"></a>
Create source and destination memory objects</h2>
<p>We assume that the 'user' source and destination memory format is NHWC. Since there is no result validation in this tutorial, we do not bother with filling the data with some values and let the Intel MKL-DNN library to allocate the memory.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> src_mem = memory({{N, IC, H, W},</div><div class="line">            memory::data_type::f32, memory::format_tag::nchw},</div><div class="line">            cpu_engine);</div><div class="line">    <span class="keyword">auto</span> weights_mem = memory({{IC, OC, KH, KW},</div><div class="line">            memory::data_type::f32, memory::format_tag::oihw},</div><div class="line">            cpu_engine);</div><div class="line">    <span class="keyword">auto</span> dst_mem = memory({{N, IC, H, W},</div><div class="line">            memory::data_type::f32, memory::format_tag::nchw},</div><div class="line">            cpu_engine);</div></div><!-- fragment --> <h2><a class="anchor" id="cpu_memory_format_propagation_sub4"></a>
Determine if source and destination need to be reordered</h2>
<p>The idiomatic way to check if a reorder is necessary between the memory format expected a primitive (the convolution in our case) and the available memory format is to compare the corresponding memory descriptors.</p>
<div class="fragment"><div class="line">    <span class="keywordtype">bool</span> need_reorder_src = conv_pd.src_desc() != src_mem.get_desc();</div></div><!-- fragment --> <dl class="section warning"><dt>Warning</dt><dd>It is by design that it is not possible to just compare memory tags. The reason behind this is that a memory format tags only provide a partial description of how data is laid out in memory and do not, for example, describe memory objects obtained via sub-memory constructor.</dd></dl>
<p>We repeat the process for the weights and destination memory format descriptors as well.</p>
<div class="fragment"><div class="line">    <span class="keywordtype">bool</span> need_reorder_weights = conv_pd.weights_desc() != weights_mem.get_desc();</div><div class="line">    <span class="keywordtype">bool</span> need_reorder_dst = conv_pd.dst_desc() != dst_mem.get_desc();</div></div><!-- fragment --> <h2><a class="anchor" id="cpu_memory_format_propagation_sub45"></a>
Allocate intermediate buffers if necessary</h2>
<p>Based on the flags computed before, we can now decide if we need extra intermediate buffers to hold the source and weights data for the convolution and the output of the pooling.</p>
<p>Memory objects for the intermediate buffers are created based on the memory descriptors obtained from the primitive descriptors to ensure consistency.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_src_mem = need_reorder_src</div><div class="line">        ? memory(conv_pd.src_desc(), cpu_engine)</div><div class="line">        : src_mem;</div><div class="line">    <span class="keyword">auto</span> conv_weights_mem = need_reorder_weights</div><div class="line">        ? memory(conv_pd.weights_desc(), cpu_engine)</div><div class="line">        : weights_mem;</div><div class="line">    <span class="keyword">auto</span> conv_dst_mem = memory(conv_pd.dst_desc(), cpu_engine);</div><div class="line">    <span class="keyword">auto</span> pool_dst_mem = need_reorder_dst</div><div class="line">        ? memory(pool_pd.dst_desc(), cpu_engine)</div><div class="line">        : dst_mem;</div></div><!-- fragment --> <h2><a class="anchor" id="cpu_memory_format_propagation_sub5"></a>
Perform reorders for source data if necessary</h2>
<p>Now we get to the part where we actually start executing things. We check if reorders are necessary based on the flags computed before and create and execute them immediately.</p>
<dl class="section note"><dt>Note</dt><dd>We call <a class="el" href="structmkldnn_1_1stream.html#a48181ce0f5eb3bcd75778b5aa8866df6">mkldnn::stream::wait()</a> before reorder primitives get out of scope and destroyed to accommodate for potentially asynchronous execution.</dd></dl>
<div class="fragment"><div class="line">    <span class="keywordflow">if</span> (need_reorder_src) {</div><div class="line">        <span class="keyword">auto</span> reorder_src = reorder(src_mem, conv_src_mem);</div><div class="line">        reorder_src.execute(cpu_stream, {</div><div class="line">                {MKLDNN_ARG_FROM, src_mem},</div><div class="line">                {MKLDNN_ARG_TO, conv_src_mem}</div><div class="line">                });</div><div class="line">        cpu_stream.wait(); <span class="comment">// wait for the reorder to complete</span></div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (need_reorder_weights) {</div><div class="line">        <span class="keyword">auto</span> reorder_weights = reorder(weights_mem, conv_weights_mem);</div><div class="line">        reorder_weights.execute(cpu_stream, {</div><div class="line">                {MKLDNN_ARG_FROM, weights_mem},</div><div class="line">                {MKLDNN_ARG_TO, conv_weights_mem}</div><div class="line">                });</div><div class="line">        cpu_stream.wait(); <span class="comment">// wait for the reorder to complete</span></div><div class="line">    }</div></div><!-- fragment --> <h2><a class="anchor" id="cpu_memory_format_propagation_sub6"></a>
Create and execute convolution and pooling primitives</h2>
<p>After the reorders, we are now ready to compute convolution and pooling.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_scratchpad_mem = memory(conv_pd.scratchpad_desc(), cpu_engine);</div><div class="line">    <span class="keyword">auto</span> conv = convolution_forward(conv_pd);</div><div class="line">    conv.execute(cpu_stream, {</div><div class="line">            {MKLDNN_ARG_SRC, conv_src_mem},</div><div class="line">            {MKLDNN_ARG_WEIGHTS, conv_weights_mem},</div><div class="line">            {MKLDNN_ARG_DST, conv_dst_mem}</div><div class="line">            });</div><div class="line">    <span class="keyword">auto</span> pool_scratchpad_mem = memory(pool_pd.scratchpad_desc(), cpu_engine);</div><div class="line">    <span class="keyword">auto</span> pool = pooling_forward(pool_pd);</div><div class="line">    pool.execute(cpu_stream, {</div><div class="line">            {MKLDNN_ARG_SRC, conv_dst_mem},</div><div class="line">            {MKLDNN_ARG_DST, pool_dst_mem}</div><div class="line">            });</div><div class="line">    cpu_stream.wait();</div></div><!-- fragment --> <h2><a class="anchor" id="cpu_memory_format_propagation_sub7"></a>
Reorder destination data if necessary</h2>
<p>The only potentially remaining operation is a reorder from the pooling destination memory object to the users's one. Similarly to the reorders for the source and weights memory objects, it is performed depending on the value of the previously computed flag.</p>
<div class="fragment"><div class="line">    <span class="keywordflow">if</span> (need_reorder_dst) {</div><div class="line">        <span class="keyword">auto</span> reorder_dst = reorder(pool_dst_mem, dst_mem);</div><div class="line">        reorder_dst.execute(cpu_stream, {</div><div class="line">                {MKLDNN_ARG_FROM, pool_dst_mem},</div><div class="line">                {MKLDNN_ARG_TO, dst_mem}</div><div class="line">                });</div><div class="line">        cpu_stream.wait();</div><div class="line">    }</div></div><!-- fragment --><p> Upon compiling and run the example the output should be just:</p>
<div class="fragment"><div class="line">Example passes</div></div><!-- fragment --><p>It may be interesting to check what really happens during the run. We can use <code>MKLDNN_VERBOSE</code> environment variable for that (see also <a class="el" href="dev_guide_verbose.html">Verbose Mode</a>). Here's example output on a system that has an Intel(R) AVX2-capable processor (line breaks added for readability):</p>
<div class="fragment"><div class="line">$ MKLDNN_VERBOSE=1 ./cpu_memory_format_propagation</div><div class="line">mkldnn_verbose,info,Intel(R) MKL-DNN &lt;ver&gt; (Git Hash &lt;hash&gt;),Intel(R) Advanced Vector Extensions 2 (Intel(R) AVX2)</div><div class="line">mkldnn_verbose,exec,reorder,jit:uni,undef,</div><div class="line">    src_f32::blocked:abcd:f0 dst_f32::blocked:aBcd8b:f0,num:1,1x256x14x14,1.03101</div><div class="line">mkldnn_verbose,exec,reorder,jit:uni,undef,</div><div class="line">    src_f32::blocked:abcd:f0 dst_f32::blocked:ABcd8b8a:f0,num:1,256x256x3x3,5.69678</div><div class="line">mkldnn_verbose,exec,convolution,jit:avx2,forward_inference,</div><div class="line">    src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 dst_f32::blocked:aBcd8b:f0,</div><div class="line">    alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.65698</div><div class="line">mkldnn_verbose,exec,pooling,jit:avx,forward_inference,</div><div class="line">    src_f32::blocked:aBcd8b:f0 dst_f32::blocked:aBcd8b:f0,</div><div class="line">    alg:pooling_max,mb1ic256_ih14oh14kh3sh1ph1_iw14ow14kw3sw1pw1,0.322021</div><div class="line">mkldnn_verbose,exec,reorder,jit:uni,</div><div class="line">    undef,src_f32::blocked:aBcd8b:f0 dst_f32::blocked:abcd:f0,num:1,1x256x14x14,0.333008</div><div class="line">Example passes</div></div><!-- fragment --><p>From this output we can deduce that:</p><ul>
<li>The convolution primitive picked up <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda448a7fc9219294ce172b0edf9498b5c4">mkldnn::memory::format_tag::aBcd8b</a> optimized memory format for activations. In this format the channels dimension (denoted by letter B since it is the second dimension; see also <a class="el" href="dev_guide_conventions.html">Naming Conventions</a>) is blocked by a factor of 8. Because of this memory format is different from the NHWC format the tutorial uses, the source and destination had to be reordered to and from this optimized memory layout.</li>
<li>The convolution primitive picked up <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4cedabcbce50e9c241458767871fa053e1ba0">mkldnn::memory::format_tag::ABcd8b8a</a> optimized memory format (output (A) and input (B) channel dimensions blocked by 8) which we also had to reorder the initial weights to since they are in the OIHW memory format. </li>
</ul>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>