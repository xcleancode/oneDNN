<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Primitive Attributes: Quantization</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">1.0.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Primitive Attributes: Quantization </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="dgaq_intro"></a></p><h2>Introduction</h2>
<p>Some primitives in the library support input/output tensors with the INT8 (either signed or unsigned) data type. The primary goal is to support reduced precision inference on the compatible hardware.</p>
<p>Related materials:</p><ul>
<li><a href="https://software.intel.com/en-us/articles/lower-numerical-precision-deep-learning-inference-and-training">Lower Numerical Precision Deep Learning Inference and Training</a></li>
<li>An example with annotations: <a class="el" href="dev_guide_inference_int8.html">Int8 Inference</a></li>
</ul>
<h2>Quantization Model</h2>
<p>The primary quantization model that the library assumes is the following: </p><p class="formulaDsp">
\[ x_{f32}(:) = scale_{f32} \cdot (x_{int8}(:) - 0_{x\_int8}) \]
</p>
<p>where \(scale_{f32}\) is somehow known in advance (typically, the process of obtaining these scale factors is called the <em>calibration</em> process). This might be counter-intuitive, but the library cannot compute any of the scale factors at run-time dynamically. Hence, the model is sometimes called a <em>static</em> quantization model. The main rationale to support only <em>static</em> quantization out-of-the-box is higher performance. Those who want to use <em>dynamic</em> quantization can do so in a few steps:</p><ol type="1">
<li>Compute the result in higher precision, like <a class="el" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57baa860868d23f3a68323a2e3f6563d7f31" title="32-bit signed integer. ">mkldnn::memory::data_type::s32</a>.</li>
<li>Find the required characteristics, like min and max values, and derive the scale factor.</li>
<li>Re-quantize to the lower precision data type.</li>
</ol>
<p>It is also worth mentioning that the library supports fixed zero position. For most of the primitives, real zero value is mapped to zero for quantized values; that is, \(0_{x\_int8} = 0\). For example, this is the only model that <a class="el" href="dev_guide_convolution.html">Convolution</a> and <a class="el" href="dev_guide_inner_product.html">Inner Product</a> currently support. The <a class="el" href="dev_guide_rnn.html">RNN</a> primitives have limited support of shifted zero (for details, refer to the corresponding section in <a class="el" href="dev_guide_rnn.html">RNN</a>).</p>
<p>For the rest of this guide, we will assume that \(0_{x\_int8} = 0\).</p>
<dl class="section warning"><dt>Warning</dt><dd>Depending on the architecture, the behavior of int8 computations might slightly vary. For more details, refer to <a class="el" href="dev_guide_int8_computations.html">Int8 Computation Aspects</a>.</dd></dl>
<p>This guide doesn't cover how the appropriate scaling factor can be found. Refer to the materials in the <a class="el" href="dev_guide_attributes_quantization.html#dgaq_intro">Introduction</a>.</p>
<h3>Example: Convolution Quantization Workflow</h3>
<p>Let's consider a simple example: a convolution without bias. The tensors are represented as:</p><ul>
<li>\(src_{f32}(:) = scale_{src} \cdot src_{int8}(:)\)</li>
<li>\(weights_{f32}(:) = scale_{weights} \cdot weights_{int8}(:)\)</li>
<li>\(dst_{f32}(:) = scale_{dst} \cdot dst_{int8}(:)\)</li>
</ul>
<p>Here the \(src_{f32}, weights_{f32}, dst_{f32}\) are not computed at all, the whole work happens with INT8 tensors. As mentioned above, we also somehow know all the scaling factors: \(scale_{src}, scale_{weights}, scale_{dst}\).</p>
<p>So the task is to compute the \(dst_{int8}\) tensor.</p>
<p>Mathematically, the computations are pretty straightforward: </p><p class="formulaDsp">
\[ dst_{int8}(:) = downconvert\_f32\_to\_int8( output\_scale \cdot conv_{s32}(src_{int8}, weights_{int8}) ), \]
</p>
<p>where:</p><ul>
<li>\(output\_scale := \frac{scale_{src} \cdot scale_{weights}}{scale_{dst}}\);</li>
<li>\(conv_{s32}\) is just a regular convolution which takes source and weights with INT8 data type and compute the result in INT32 data type (INT32 is chosen to avoid overflows during the computations);</li>
<li>\(downconvert\_f32\_to\_s8()\) converts an <code>f32</code> value to <code>s8</code> with potential saturation if the values are out of the range of the INT8 data type.</li>
</ul>
<p>Note that in order to perform the operation, one doesn't need to know the exact scaling factors for all the tensors; it is enough to know only the \(output\_scale\). The library utilizes this fact; a user needs to provide only this one extra parameter (see the <a class="el" href="dev_guide_attributes_quantization.html#dev_guide_attributes_quantization_output_scare">Output Scaling Attribute</a> section below) to perform the convolution.</p>
<h3>Per-Channel Scaling</h3>
<p>Some of the primitives have limited support of multiple scales for a quantized tensor. The most popular use-case is a <a class="el" href="dev_guide_convolution.html">Convolution</a> primitive that supports per-output-channel scaling factors for the weights, meaning that the actual convolution computations would need to scale different output channels differently. This is possible without significant performance drop because the per-output-channel re-quantization only required at the very end of the computations. It seems impossible to implement the same trick for the input channels, since that would require re-quantization for every input data point.</p>
<p>Assume we have (the scales are designated as \(\alpha\) to simplify reading):</p><ul>
<li>\(src_{f32}(n, ic, ih, iw) = \alpha_{src} \cdot src_{int8}(n, ic, ih, iw)\)</li>
<li>\(weights_{f32}(oc, ic, kh, kw) = \alpha_{weights}(oc) \cdot weights_{int8}(oc, ic, kh, kw)\)</li>
<li>\(dst_{f32}(n, oc, oh, ow) = scale_{dst} \cdot dst_{int8}(n, oc, oh, ow)\)</li>
</ul>
<p>Note that now the weights' scaling factor depends on the \(oc\).</p>
<p>To compute the \(dst_{int8}\) we need to perform the following:</p>
<p class="formulaDsp">
\[ dst_{int8}(n, oc, oh, ow) = downconvert\_f32\_to\_int8( output\_scale(oc) \cdot conv_{s32}(src_{int8}, weights_{int8})|_{(n, oc, oh, ow)} ), \]
</p>
<p>where now</p><ul>
<li>\(output\_scale(oc) := \frac{\alpha_{src} \cdot \alpha_{weights}(oc)}{\alpha_{dst}}\).</li>
</ul>
<p>It is worth mentioning that a user has to prepare quantized weights accordingly. For Intel MKL-DNN provides reorders that can perform per-channel scaling:</p>
<p class="formulaDsp">
\[ weights_{int8}(oc, ic, kh, kw) = downconvert\_f32\_to\_int8( output\_scale(oc) \cdot weights_{f32}(oc, ic, kh, kw) ), \]
</p>
<p>where:</p><ul>
<li>\(output\_scale(oc) := \frac{1}{\alpha_{weights}(oc_{})}\).</li>
</ul>
<h2>API</h2>
<p>The library API to support for INT8 was designed for the model described above. However, it doesn't require users to follow exactly this model. As long as users can fit their model into the given functionality everything should work fine. Having this in mind we tried to design a minimal and simple yet powerful enough quantization API.</p>
<p>The most common data type for data tensors during INT8 inference is <a class="el" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57ba3e8d88fdd85d7153525e0647cdd97686" title="8-bit signed integer. ">mkldnn::memory::data_type::s8</a> and <a class="el" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57ba077393852be20e37026d6281827662f2" title="8-bit unsigned integer. ">mkldnn::memory::data_type::u8</a>. All the scaling factors related to tensors are not attached in any way to the Intel MKL-DNN memory objects and should be maintained by users.</p>
<p>The library essentially extends the ability of the primitives to scale the output before storing the result to the memory with the destination data type. That's exactly the minimum that we need to support INT8 inference (check the equations above&ndash;only \(output\_scale\) is non-standard).</p>
<p>The scaling happens in the single precision floating point data type (<a class="el" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57ba512dc597be7ae761876315165dc8bd2e" title="32-bit/single-precision floating point. ">mkldnn::memory::data_type::f32</a>). Before storing, the result is downconverted to the destination data type with saturation if required. The rounding happens according to the current HW setting (for instance, on CPU according to the MXCSR register).</p>
<p><a class="anchor" id="dev_guide_attributes_quantization_output_scare"></a></p><h3>Output Scaling Attribute</h3>
<p>The library uses <a class="el" href="dev_guide_attributes.html">Primitive Attributes</a> API for setting the scaling factors for most of the primitives. The supporting attributes can be found in the documentation for each primitive. The unsupported cases are handled according to the <a class="el" href="dev_guide_attributes.html#dev_guide_attributes_error_handling">attributes error handling section</a>.</p>
<p>API:</p><ul>
<li>C: <a class="el" href="group__c__api__attributes.html#ga415451b28b726c0641c3d32b98815fc2">mkldnn_primitive_attr_set_output_scales</a></li>
<li>C++: <a class="el" href="structmkldnn_1_1primitive__attr.html#a1c2ba432512cf9950b81febce265e35d">mkldnn::primitive_attr::set_output_scales</a></li>
</ul>
<p>The primitives do not support output scales if source (and weights) tensors are of the int8 data type. In other words, regular <code>f32</code> convolution cannot scale the output result.</p>
<p>The parameters (C++ API for simplicity): </p><div class="fragment"><div class="line"><span class="keywordtype">void</span> <a class="code" href="structmkldnn_1_1primitive__attr.html#a1c2ba432512cf9950b81febce265e35d">mkldnn::primitive_attr::set_output_scales</a>(</div><div class="line">        <span class="keywordtype">int</span> mask,</div><div class="line">        <span class="keyword">const</span> std::vector&lt;float&gt; &amp;scales</div><div class="line">        );</div></div><!-- fragment --><p>In the simplest case, when there is only one common scale the attribute changes the op behavior from </p><p class="formulaDsp">
\[ dst(:) = Op(...) \]
</p>
<p>to</p>
<p class="formulaDsp">
\[ dst(:) = scale \cdot Op(...). \]
</p>
<p>To support scales per one or several dimensions, users must set the appropriate mask.</p>
<p>Say the destination is \(D_0 \times ... \times D_{n-1}\) tensor and we want to have output scales per \(d_i\) dimension (where \(0 \le d_i &lt; n\)).</p>
<p>Then the mask should be set to:</p><ul>
<li>\(mask = \sum \limits_{d_i} 2^{d_i}\),</li>
</ul>
<p>and the number of scales should be:</p><ul>
<li><code>scales.size()</code> = \(\prod\limits_{d_i}D_{d_i}\).</li>
</ul>
<h4>Example 1: weights quantization with per-output-channel-and-group scaling</h4>
<div class="fragment"><div class="line"><span class="comment">// weights dimensions</span></div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> G, OC, IC, KH, KW;</div><div class="line"></div><div class="line"><span class="comment">// original f32 weights in user&#39;s format</span></div><div class="line"><a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> wei_user_f32_md(</div><div class="line">        {G, OC/G, IC/G, KH, KW},            <span class="comment">// dims</span></div><div class="line">        <a class="code" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57ba512dc597be7ae761876315165dc8bd2e">mkldnn::memory::data_type::f32</a>,     <span class="comment">// the data originally in f32</span></div><div class="line">        mkldnn::memory::format_tag::hwigo   <span class="comment">// the memory format a user uses</span></div><div class="line">        );</div><div class="line"></div><div class="line"><span class="comment">// the scaling factors for quantized weights</span></div><div class="line"><span class="comment">// An unique scale for each group and output-channel.</span></div><div class="line">std::vector&lt;float&gt; wei_scales(G * OC/G) = {...};</div><div class="line"></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// int8 convolution primitive descriptor (will create it in the next example)</span></div><div class="line"><a class="code" href="structmkldnn_1_1convolution__forward_1_1primitive__desc.html">mkldnn::convolution_forward::primitive_desc</a> conv_pd(...);</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// query the convolution weights memory descriptor</span></div><div class="line"><a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> wei_conv_s8_md = conv_pd.weights_desc();</div><div class="line"></div><div class="line"><span class="comment">// prepare the inverse of the scales (f32 = scale * int8 --&gt; int8 = 1/scale * f32)</span></div><div class="line">std::vector&lt;float&gt; inv_wei_scales(wei_scales.size());</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; wei_scales.size(); ++i)</div><div class="line">    inv_wei_scales[i] = 1.f / wei_scales[i];</div><div class="line"></div><div class="line"><span class="comment">// prepare the attributes for the reorder</span></div><div class="line"><a class="code" href="structmkldnn_1_1primitive__attr.html">mkldnn::primitive_attr</a> attr;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> mask = 0</div><div class="line">    | (1 &lt;&lt; 0)  <span class="comment">// scale per  G dimension, which is the dim #0</span></div><div class="line">    | (1 &lt;&lt; 1); <span class="comment">// scale per OC dimension, which is the dim #1</span></div><div class="line">attr.<a class="code" href="structmkldnn_1_1primitive__attr.html#a1c2ba432512cf9950b81febce265e35d">set_output_scales</a>(mask, inv_wei_scales);</div><div class="line"></div><div class="line"><span class="comment">// create reorder that would perform:</span></div><div class="line"><span class="comment">//   wei_s8(g, oc, ic, kh, kw) &lt;- 1/scale(g, oc) * wei_f32(g, oc, ic, kh, kw)</span></div><div class="line"><span class="comment">// including the data format transformation.</span></div><div class="line"><span class="keyword">auto</span> wei_reorder_pd = mkldnn::reorder::primitive_desc(</div><div class="line">        wei_user_f32_md, engine, <span class="comment">// source</span></div><div class="line">        wei_conv_s8_md, engine, <span class="comment">// destination,</span></div><div class="line">        attr);</div><div class="line"><span class="keyword">auto</span> wei_reorder = <a class="code" href="structmkldnn_1_1reorder.html">mkldnn::reorder</a>(wei_reorder_pd);</div><div class="line"></div><div class="line"><span class="comment">// ...</span></div></div><!-- fragment --><h4>Example 2: convolution with groups, with per-output-channel quantization</h4>
<p>This example is complementary to the previous example (which should ideally be the first one). Let's say we want to have an INT8 convolution with per-output channel scaling.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">float</span> src_scale; <span class="comment">// source scale factor: src_f32[:] = src_scale * src_s8[:]</span></div><div class="line"><span class="keyword">const</span> <span class="keywordtype">float</span> dst_scale; <span class="comment">// destination scale factor: dst_f32[:] = dst_scale * dst_s8[:]</span></div><div class="line"></div><div class="line"><span class="comment">// the scaling factors for quantized weights (as declared above)</span></div><div class="line"><span class="comment">// An unique scale for each group and output-channel.</span></div><div class="line">std::vector&lt;float&gt; wei_scales(G * OC/G) = {...};</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">// Src, weights, and dst memory descriptors for convolution,</span></div><div class="line"><span class="comment">// with memory format tag == any to allow a convolution implementation</span></div><div class="line"><span class="comment">// to chose the appropriate memory format</span></div><div class="line"></div><div class="line"><a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> src_conv_s8_any_md(</div><div class="line">        {BATCH, IC, IH, IW},            <span class="comment">// dims</span></div><div class="line">        <a class="code" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57ba3e8d88fdd85d7153525e0647cdd97686">mkldnn::memory::data_type::s8</a>,  <span class="comment">// the data originally in s8</span></div><div class="line">        <a class="code" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec">mkldnn::memory::format_tag::any</a> <span class="comment">// let convolution to choose</span></div><div class="line">        );</div><div class="line"></div><div class="line"><a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> wei_conv_s8_any_md(</div><div class="line">        {G, OC/G, IC/G, KH, KW},        <span class="comment">// dims</span></div><div class="line">        <a class="code" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57ba3e8d88fdd85d7153525e0647cdd97686">mkldnn::memory::data_type::s8</a>,  <span class="comment">// the data originally in s8</span></div><div class="line">        <a class="code" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec">mkldnn::memory::format_tag::any</a> <span class="comment">// let convolution to choose</span></div><div class="line">        );</div><div class="line"></div><div class="line"><a class="code" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> dst_conv_s8_any_md(...);  <span class="comment">// ditto</span></div><div class="line"></div><div class="line"><span class="comment">// Create a convolution operation descriptor</span></div><div class="line"><a class="code" href="structmkldnn_1_1convolution__forward_1_1desc.html">mkldnn::convolution_forward::desc</a> conv_d(</div><div class="line">        <a class="code" href="group__cpp__api__enums.html#ggaeb087eae78f70a4d249a90aefa165cf8a3b9fad4f80d45368f856b5403198ac4c">mkldnn::prop_kind::forward_inference</a>,</div><div class="line">        <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a5028ad8f818a45333a8a0eefad35c5c0">mkldnn::algorithm::convolution_direct</a>,</div><div class="line">        src_conv_s8_any_md,                     <span class="comment">// what&#39;s important is that</span></div><div class="line">        wei_conv_s8_any_md,                     <span class="comment">// we specified that we want</span></div><div class="line">        dst_conv_s8_any_md,                     <span class="comment">// computations in s8</span></div><div class="line">        strides, padding_l, padding_r,</div><div class="line">        mkldnn::padding_kind::zero</div><div class="line">        );</div><div class="line"></div><div class="line"><span class="comment">// prepare the attributes for the convolution</span></div><div class="line"><a class="code" href="structmkldnn_1_1primitive__attr.html">mkldnn::primitive_attr</a> attr;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> mask = 0</div><div class="line">    | (1 &lt;&lt; 1); <span class="comment">// scale per OC dimension, which is the dim #1 on dst tensor:</span></div><div class="line">                <span class="comment">// (BATCH, OC, OH, OW)</span></div><div class="line">                <span class="comment">//    0     1   2   3</span></div><div class="line">std::vector&lt;float&gt; conv_output_scales(G * OC/G);</div><div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> g_oc = 0; G * OC/G; ++g_oc)</div><div class="line">    conv_output_scales[g_oc] = src_scale * wei_scales(g_oc) / dst_scale;</div><div class="line">attr.<a class="code" href="structmkldnn_1_1primitive__attr.html#a1c2ba432512cf9950b81febce265e35d">set_output_scales</a>(mask, conv_output_scales);</div><div class="line"></div><div class="line"><span class="comment">// create a convolution primitive descriptor with the scaling factors</span></div><div class="line"><span class="keyword">auto</span> conv_pd = <a class="code" href="structmkldnn_1_1convolution__forward_1_1primitive__desc.html">mkldnn::convolution_forward::primitive_desc</a>(</div><div class="line">        conv_d, <span class="comment">// general (non-customized) operation descriptor</span></div><div class="line">        attr,   <span class="comment">// the attributes contain the output scaling</span></div><div class="line">        engine);</div><div class="line"></div><div class="line"><span class="comment">// ...</span></div></div><!-- fragment --><h4>Interplay of output scales with post-ops</h4>
<p>In general, the <a class="el" href="dev_guide_attributes_post_ops.html">post-ops</a> are independent from the output scales. The output scales are applied to the result first; then post-ops will take effect.</p>
<p>For details, refer to the <a class="el" href="dev_guide_attributes_post_ops.html#dev_guide_attributes_post_ops_with_scales">Tanh -&gt; Sum -&gt; ScaleShift</a> example.</p>
<p>That has an implication on the scaling factors passed to the library, however. Consider the following example of a convolution with \(\tanh\) as a post-op:</p>
<p class="formulaDsp">
\[ dst_{s8}(:) = \frac{1}{scale_{dst}} \cdot \tanh( scale_{src} \cdot scale_{weights} \cdot conv_{s32}(src_{s8}, wei_{s8}) ) \]
</p>
<p>As you can see:</p><ul>
<li>The convolution output scales are now \(conv\_output\_scale = scale_{src} \cdot scale_{weights}\), i.e. no division by \(scale_{dst}\);</li>
<li>And the post-ops scale for \(\tanh\) is set to \(scale\_tanh\_post\_op = \frac{1}{scale_{dst}}\). </li>
</ul>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>