<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Convolution</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">1.0.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Convolution </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p></p>
<p>API reference: <a class="el" href="group__c__api__convolution.html">C</a>, <a class="el" href="group__cpp__api__convolution.html">C++</a></p>
<p></p>
</blockquote>
<p>The convolution primitive computes forward, backward, or weight update for a batched convolution operation on 1D, 2D, or 3D spatial data with bias.</p>
<p>The convolution operation is defined by the following formulas. We show formulas only for 2D spatial data which are straightforward to generalize to cases of higher and lower dimensions. Variable names follow the standard <a class="el" href="dev_guide_conventions.html">Naming Conventions</a>.</p>
<p>Let \(src\), \(weights\) and \(dst\) be \(N \times IC \times IH \times IW\), \(OC \times IC \times KH \times KW\), and \(N \times OC \times OH \times OW\) tensors respectively. Let \(bias\) be a 1D tensor with \(OC\) elements.</p>
<p>The following formulas show how Intel MKL-DNN computes convolutions. They are broken down into several types to simplify the exposition, but in reality the convolution types can be combined.</p>
<p>To further simplify the formulas, we assume that \(src(n, ic, ih, iw) = 0\) if \(ih &lt; 0\), or \(ih \geq IH\), or \(iw &lt; 0\), or \(iw \geq IW\).</p>
<h3>Forward</h3>
<h4>Regular Convolution</h4>
<p class="formulaDsp">
\[dst(n, oc, oh, ow) = bias(oc) + \\ + \sum_{ic=0}^{IC-1}\sum_{kh=0}^{KH-1}\sum_{kw=0}^{KW-1} src(n, ic, oh \cdot SH + kh - ph_0, ow \cdot SW + kw - pw_0) \cdot weights(oc, ic, kh, kw).\]
</p>
<p>Here:</p>
<ul>
<li>\(OH = \left\lfloor{\frac{IH \cdot SH - KH + ph_0 + ph_1}{sh}} \right\rfloor + 1,\)</li>
<li>\(OW = \left\lfloor{\frac{IW \cdot SW - KW + pw_0 + pw_1}{sw}} \right\rfloor + 1.\)</li>
</ul>
<h4>Convolution with Groups</h4>
<p>In the API, Intel MKL-DNN adds a separate groups dimension to memory objects representing weights tensors and represents weights as \(G \times OC_G \times IC_G \times KH \times KW \) 5D tensors for 2D convolutions with groups.</p>
<p class="formulaDsp">
\[ dst(n, g \cdot OC_G + oc_g, oh, ow) = bias(g \cdot OC_G + oc_g) + \\ + \sum_{ic_g=0}^{IC_G-1}\sum_{kh=0}^{KH-1}\sum_{kw=0}^{KW-1} src(n, g \cdot IC_G + ic_g, oh + kh - ph_0, ow + kw - pw_0) \cdot weights(g, oc_g, ic_g, kh, kw), \]
</p>
<p>where</p><ul>
<li>\(IC_G = \frac{IC}{G}\),</li>
<li>\(OC_G = \frac{OC}{G}\),</li>
<li>\(ic_g \in [0, IC_G)\) and</li>
<li>\(oc_g \in [0, OC_G).\)</li>
</ul>
<p>The case when \(OC_G = IC_G = 1\) is also known as <em>a depthwise convolution</em>.</p>
<h4>Convolution with Dilation</h4>
<p class="formulaDsp">
\[ dst(n, oc, oh, ow) = bias(oc) + \\ + \sum_{ic=0}^{IC-1}\sum_{kh=0}^{KH-1}\sum_{kw=0}^{KW-1} src(n, ic, oh + kh \cdot dh - ph_0, ow + kw \cdot dw - pw_0) \cdot weights(oc, ic, kh, kw). \]
</p>
<p>Here:</p>
<ul>
<li>\(OH = \left\lfloor{\frac{IH - DKH + ph_0 + ph_1}{sh}} \right\rfloor + 1,\) where \(DKH = 1 + (KH - 1) \cdot (DH + 1)\), and</li>
<li>\(OW = \left\lfloor{\frac{IW - DKW + pw_0 + pw_1}{sw}} \right\rfloor + 1,\) where \(DKW = 1 + (KW - 1) \cdot (DW + 1)\).</li>
</ul>
<h4>Deconvolution (Transposed Convolution)</h4>
<p>Deconvolutions (also called fractionally strided convolutions or transposed convolutions) work by swapping the forward and backward passes of a convolution. One way to put it is to note that the weights define a convolution, but whether it is a direct convolution or a transposed convolution is determined by how the forward and backward passes are computed.</p>
<h4>Difference Between <a href="#mkldnn_forward_training">Forward Training</a> and <a href="#mkldnn_forward_inference">Forward Inference</a></h4>
<p>There is no difference between the <a class="el" href="group__c__api__types__generic.html#gga5b98c8059c2aff8861157bf070c3f520a0bb8fb5a8f3ae67cf8d9c8d13667507f" title="Forward data propagation (training mode). ">mkldnn_forward_training</a> and <a class="el" href="group__c__api__types__generic.html#gga5b98c8059c2aff8861157bf070c3f520a58a923fb6c4e1214e1505b5fa0b1e3fa" title="Forward data propagation (inference mode). ">mkldnn_forward_inference</a> propagation kinds.</p>
<h3>Backward</h3>
<p>The backward propagation computes \(diff\_src\) based on \(diff\_dst\) and \(weights\).</p>
<p>The weights update computes \(diff\_weights\) and \(diff\_bias\) based on \(diff\_dst\) and \(src\).</p>
<dl class="section note"><dt>Note</dt><dd>The <em>optimized</em> memory formats \(src\) and \(weights\) might be different on forward propagation, backward propagation, and weights update.</dd></dl>
<h2>Implementation Details</h2>
<h3>General Notes</h3>
<p>N/A.</p>
<h3>Data Types</h3>
<p>Convolution primitive supports the following combination of data types for source, destination, and weights memory objects:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Propagation  </th><th class="markdownTableHeadLeft">Source  </th><th class="markdownTableHeadLeft">Weights  </th><th class="markdownTableHeadLeft">Destination  </th><th class="markdownTableHeadLeft">Bia   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">forward / backward  </td><td class="markdownTableBodyLeft">f32  </td><td class="markdownTableBodyLeft">f32  </td><td class="markdownTableBodyLeft">f32  </td><td class="markdownTableBodyLeft">f32   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">forward  </td><td class="markdownTableBodyLeft">f16  </td><td class="markdownTableBodyLeft">f16  </td><td class="markdownTableBodyLeft">f16  </td><td class="markdownTableBodyLeft">f16   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">forward  </td><td class="markdownTableBodyLeft">u8, s8  </td><td class="markdownTableBodyLeft">s8  </td><td class="markdownTableBodyLeft">u8, s8, s32, f32  </td><td class="markdownTableBodyLeft">u8, s8, s32, f32   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">forward  </td><td class="markdownTableBodyLeft">bf16  </td><td class="markdownTableBodyLeft">bf16  </td><td class="markdownTableBodyLeft">f32, bf16  </td><td class="markdownTableBodyLeft">f32, bf16   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">backward  </td><td class="markdownTableBodyLeft">f32, bf16  </td><td class="markdownTableBodyLeft">f32, bf16  </td><td class="markdownTableBodyLeft">bf16  </td><td class="markdownTableBodyLeft">f32, bf16   </td></tr>
</table>
<dl class="section warning"><dt>Warning</dt><dd>There might be hardware and/or implementation specific restrictions. Check <a class="el" href="dev_guide_convolution.html#dg_conv_impl_limits">Implementation Limitations</a> section below.</dd></dl>
<h3>Data Representation</h3>
<p>Like other CNN primitives, the convolution primitive expects the following tensors:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Spatial  </th><th class="markdownTableHeadLeft">Source / Destination  </th><th class="markdownTableHeadLeft">Wei   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">1D  </td><td class="markdownTableBodyLeft">\(N \times C \times W\)  </td><td class="markdownTableBodyLeft">\([G \times ] OC \times IC \times KW\)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">2D  </td><td class="markdownTableBodyLeft">\(N \times C \times H \times W\)  </td><td class="markdownTableBodyLeft">\([G \times ] OC \times IC \times KH \times KW\)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">3D  </td><td class="markdownTableBodyLeft">\(N \times C \times D \times H \times W\)  </td><td class="markdownTableBodyLeft">\([G \times ] OC \times IC \times KD \times KH \times KW\)   </td></tr>
</table>
<p>Physical format of data and weights memory objects is critical for convolution primitive performance. In the Intel MKL-DNN programming model, convolution is one of the few primitives that support the placeholder memory format tag <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">mkldnn::memory::format_tag::any</a> (shortened to <code>any</code> from now on) and can define data and weight memory objects format based on the primitive parameters. When using <code>any</code> it is necessary to first create a convolution primitive descriptor and then query it for the actual data and weight memory objects formats.</p>
<p>While convolution primitives can be created with memory formats specified explicitly, the performance is likely to be suboptimal.</p>
<p>The table below shows the combinations for which <b>plain</b> memory formats the convolution primitive is optimized for.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Spatial  </th><th class="markdownTableHeadLeft">Convolution Type  </th><th class="markdownTableHeadLeft">Data / Weights logical tensor  </th><th class="markdownTableHeadLeft">Imp   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">1D, 2D, 3D  </td><td class="markdownTableBodyLeft"></td><td class="markdownTableBodyLeft"><code>any</code>  </td><td class="markdownTableBodyLeft"><em>optimized</em>   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">1D  </td><td class="markdownTableBodyLeft">f32, bf16  </td><td class="markdownTableBodyLeft">NCW / OIW, GOIW  </td><td class="markdownTableBodyLeft"><a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2baa9593899799f130c7f511f09b5c1ea9d" title="3D CNN activations tensor, an alias to mkldnn_abc ">mkldnn_ncw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba3f9bf8e8dfb966d5fb76844fb8cc758d" title="plain 3D tensor ">mkldnn_abc</a>) / <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba6a18734c3be6fcf4b2c0ce501816cb61" title="3D CNN weights tensor, an alias to mkldnn_abc ">mkldnn_oiw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba3f9bf8e8dfb966d5fb76844fb8cc758d" title="plain 3D tensor ">mkldnn_abc</a>), <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bac4ed5ec860141abb6e922d4b9db8b509" title="4D CNN weights tensor (incl. groups), an alias to mkldnn_abcd ">mkldnn_goiw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba63203c48f8a6ff36dbf21e8c81bca060" title="plain 4D tensor ">mkldnn_abcd</a>)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">1D  </td><td class="markdownTableBodyLeft">int8  </td><td class="markdownTableBodyLeft">NCW / OIW  </td><td class="markdownTableBodyLeft"><a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba23e0a15956f5406d591d776d852446a3" title="3D CNN activations tensor, an alias to mkldnn_acb ">mkldnn_nwc</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba975488028c2512fda82be106e36c8b35" title="permuted 3D tensor ">mkldnn_acb</a>) / <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba32b0c647332e8d67f0c9146a78bd2e7a" title="3D CNN weights tensor, an alias to mkldnn_cba ">mkldnn_wio</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2baa64ff960aa3225ad2aeef83e237d3b5f" title="permuted 3D tensor ">mkldnn_cba</a>)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">2D  </td><td class="markdownTableBodyLeft">f32, bf16  </td><td class="markdownTableBodyLeft">NCHW / OIHW, GOIHW  </td><td class="markdownTableBodyLeft"><a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2baed4a51a8db128a648b942882369a06d6" title="4D CNN activations tensor, an alias to mkldnn_abcd ">mkldnn_nchw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba63203c48f8a6ff36dbf21e8c81bca060" title="plain 4D tensor ">mkldnn_abcd</a>) / <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bacc2eeaa304a91dc0d8ec301a8a13fa4a" title="4D CNN weights tensor, an alias to mkldnn_abcd ">mkldnn_oihw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba63203c48f8a6ff36dbf21e8c81bca060" title="plain 4D tensor ">mkldnn_abcd</a>), <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2baf55cbade6e10b15037abaa67e6933833" title="5D CNN weights tensor (incl. groups), an alias to mkldnn_abcde ">mkldnn_goihw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bab5a0a1dc7021e96ed7e174f76f8decbc" title="plain 5D tensor ">mkldnn_abcde</a>)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">2D  </td><td class="markdownTableBodyLeft">int8  </td><td class="markdownTableBodyLeft">NCHW / OIHW, GOIHW  </td><td class="markdownTableBodyLeft"><a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2badf1fb06f73cf8dd23ae620eea6516526" title="4D CNN activations tensor, an alias to mkldnn_acdb ">mkldnn_nhwc</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba5bdb52f009d9c8b46aaf05119cf18592" title="permuted 4D tensor ">mkldnn_acdb</a>) / <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba56fdb8a888dba3cd5bb8e5a0bd521f8f" title="4D CNN weights tensor, an alias to mkldnn_cdba ">mkldnn_hwio</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba4b652226a3878ad2c7471f1519da6c41" title="permuted 4D tensor ">mkldnn_cdba</a>), <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2badb01b4e8fff1203545a188f05cb89caa" title="5D CNN weights tensor (incl. groups), an alias to mkldnn_decab ">mkldnn_hwigo</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2baa1f28c19cea52b858c9fb8f30db3631c" title="permuted 5D tensor ">mkldnn_decab</a>)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">3D  </td><td class="markdownTableBodyLeft">f32, bf16  </td><td class="markdownTableBodyLeft">NCDHW / OIDHW, GOIDHW  </td><td class="markdownTableBodyLeft"><a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba55268cf202bb4367b19fd0aeb13739ce" title="5D CNN activations tensor, an alias to mkldnn_abcde ">mkldnn_ncdhw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bab5a0a1dc7021e96ed7e174f76f8decbc" title="plain 5D tensor ">mkldnn_abcde</a>) / <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bafac4917edc105faa444307270eb0d1a5" title="5D CNN weights tensor, an alias to mkldnn_abcde ">mkldnn_oidhw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bab5a0a1dc7021e96ed7e174f76f8decbc" title="plain 5D tensor ">mkldnn_abcde</a>), <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba643d2d2b24f526447a6852940fdca46a" title="6D CNN weights tensor (incl. groups), an alias to mkldnn_abcdef ">mkldnn_goidhw</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba0614d8d2ebeac1daa92a4cf5e96e0ca4" title="plain 6D tensor ">mkldnn_abcdef</a>)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">3D  </td><td class="markdownTableBodyLeft">int8  </td><td class="markdownTableBodyLeft">NCDHW / OIDHW  </td><td class="markdownTableBodyLeft"><a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bafe9f8381139e749050fda6fb70808e1c" title="5D CNN activations tensor, an alias to mkldnn_acdeb ">mkldnn_ndhwc</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2bae659343dce6f6dd2eaf8f7a9455be26d" title="permuted 5D tensor ">mkldnn_acdeb</a>) / <a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba6669e57588b920dcd21b951ac7eb1682" title="5D CNN weights tensor, an alias to mkldnn_cdeba ">mkldnn_dhwio</a> (<a class="el" href="group__c__api__types__generic.html#ggacc2844e341ab1c4f5b7ae1c6068f2a2ba0a04f3c51661b874c8e2fcfff1bf0019" title="permuted 5D tensor ">mkldnn_cdeba</a>)   </td></tr>
</table>
<h3>Post-ops and Attributes</h3>
<p>Post-ops and attributes enable you to modify the behavior of the convolution primitive by applying the output scale to the result of the primitive and by chaining certain operations after the primitive. The following attributes and post-ops are supported:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Propagation  </th><th class="markdownTableHeadLeft">Type  </th><th class="markdownTableHeadLeft">Operation  </th><th class="markdownTableHeadLeft">Restrictions  </th><th class="markdownTableHeadLeft">Des   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">forward  </td><td class="markdownTableBodyLeft">attribute  </td><td class="markdownTableBodyLeft"><a class="el" href="structmkldnn_1_1primitive__attr.html#a1c2ba432512cf9950b81febce265e35d">Output scale</a>  </td><td class="markdownTableBodyLeft">int8 convolutions only  </td><td class="markdownTableBodyLeft">Scales the result of convolution by given scale factor(s)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">forward  </td><td class="markdownTableBodyLeft">post-op  </td><td class="markdownTableBodyLeft"><a class="el" href="structmkldnn_1_1post__ops.html#af86d191276edcc3bd16571dd357072ec">eltwise</a>  </td><td class="markdownTableBodyLeft"></td><td class="markdownTableBodyLeft">Applies an <a class="el" href="group__c__api__eltwise.html">Eltwise</a> operation to the result (currently only <a class="el" href="group__c__api__types__generic.html#ggaa27d43cdd1e439cc41a9580d23ce3e97a66ead6848424f0b494e813745efb5548" title="Eltwise: ReLU. ">mkldnn_eltwise_relu</a> algorithm is supported)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">forward  </td><td class="markdownTableBodyLeft">post-op  </td><td class="markdownTableBodyLeft"><a class="el" href="structmkldnn_1_1post__ops.html#a0d8b8d8ebdde1e97afff489c2afcc8be">sum</a>  </td><td class="markdownTableBodyLeft"></td><td class="markdownTableBodyLeft">Adds the operation result to the destination tensor instead of overwriting it   </td></tr>
</table>
<dl class="section note"><dt>Note</dt><dd>The library doesn't prevent using post-ops in training, but note that not all post-ops are feasible for training usage. For instance, using ReLU with non-zero negative slope parameter as a post-op would not produce an additional output <code>workspace</code> that is required to compute backward propagation correctly. Hence, in this particular case one should use separate convolution and eltwise primitives for training.</dd></dl>
<p>The following post-ops chaining is supported by the library:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Type of convolutions  </th><th class="markdownTableHeadLeft">Pos   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">f32 and bf16 convolution  </td><td class="markdownTableBodyLeft">eltwise, sum, sum -&gt; eltwise   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">int8 convolution  </td><td class="markdownTableBodyLeft">eltwise, sum, sum -&gt; eltwise, eltwise -&gt; sum   </td></tr>
</table>
<p>The attributes and post-ops take effect in the following sequence:</p><ul>
<li>Output scale attribute,</li>
<li>Post-ops, in order they were attached.</li>
</ul>
<p>The operations during attributes and post-ops applying are done in single precision floating point data type. The conversion to the actual destination data type happens just before the actual storing.</p>
<h4>Example 1</h4>
<p>Consider the following pseudo code:</p>
<div class="fragment"><div class="line">attribute attr;</div><div class="line">attr.set_output_scale(alpha);</div><div class="line">attr.set_post_ops({</div><div class="line">        { sum={scale=beta} },</div><div class="line">        { eltwise={scale=gamma, type=tanh, alpha=ignore, beta=ignored }</div><div class="line">    });</div><div class="line"></div><div class="line">convolution_forward(src, weights, dst, attr)</div></div><!-- fragment --><p>The would lead to the following:</p>
<p class="formulaDsp">
\[ dst(\overline{x}) = \gamma \cdot \tanh \left( \alpha \cdot conv(src, weights) + \beta \cdot dst(\overline{x}) \right) \]
</p>
<h4>Example 2</h4>
<p>The following pseudo code:</p>
<div class="fragment"><div class="line">attribute attr;</div><div class="line">attr.set_output_scale(alpha);</div><div class="line">attr.set_post_ops({</div><div class="line">        { eltwise={scale=gamma, type=relu, alpha=eta, beta=ignored }</div><div class="line">        { sum={scale=beta} },</div><div class="line">    });</div><div class="line"></div><div class="line">convolution_forward(src, weights, dst, attr)</div></div><!-- fragment --><p>That would lead to the following:</p>
<p class="formulaDsp">
\[ dst(\overline{x}) = \beta \cdot dst(\overline{x}) + \gamma \cdot ReLU \left( \alpha \cdot conv(src, weights), \eta \right) \]
</p>
<h2>Algorithms</h2>
<p>Intel MKL-DNN implements convolution primitives using several different algorithms:</p>
<ul>
<li><em>Direct</em>. The convolution operation is computed directly using SIMD instructions. This is the algorithm used for the most shapes and supports int8, f32 and bf16 data types.</li>
<li><em>Winograd</em>. This algorithm reduces computational complexity of convolution at the expense of accuracy loss and additional memory operations. The implementation is based on the <a href="https://arxiv.org/abs/1509.09308"><b>Fast Algorithms for Convolutional Neural Networks by A. Lavin and S. Gray</b></a>. The Winograd algorithm often results in the best performance, but it is applicable only to particular shapes. Moreover, Winograd only supports int8 and f32 data types.</li>
<li><em>Implicit GEMM</em>. The convolution operation is reinterpreted in terms of matrix-matrix multiplication by rearranging the source data into a <a class="el" href="dev_guide_attributes_scratchpad.html">scratchpad memory</a>. This is a fallback algorithm that is dispatched automatically when other implementations are not available. GEMM convolution supports the int8, f32, and bf16 data types.</li>
</ul>
<h4>Direct Algorithm</h4>
<p>Intel MKL-DNN supports the direct convolution algorithm on all supported platforms for the following conditions:</p>
<ul>
<li>Data and weights memory formats are defined by the convolution primitive (user passes <code>any</code>).</li>
<li>The number of channels per group is a multiple of SIMD width for grouped convolutions.</li>
<li>For each spatial direction padding does not exceed one half of the corresponding dimension of the weights tensor.</li>
<li>Weights tensor width does not exceed 14</li>
</ul>
<p>In case any of these constraints are not met, the implementation will silently fall back to an explicit GEMM algorithm.</p>
<h4>Winograd Convolution</h4>
<p>Intel MKL-DNN supports the Winograd convolution algorithm on systems with Intel AVX-512 support and above under the following conditions:</p>
<ul>
<li>Data and weights memory formats are defined by the convolution primitive (user passes <code>any</code> as the data format).</li>
<li>The spatial domain is two-dimensional.</li>
<li>The weights shape is 3x3, there are no groups, dilation or strides ( \(kh = kw = 3\), and \(sw = sh = 1\), \(dw = dh = 0\)).</li>
<li>The data type is either int8 or f32.</li>
</ul>
<p>In case any of these constraints is not met, the implementation will silently fall back to the direct algorithm.</p>
<p>The Winograd convolution algorithm implementation additionally chooses tile size based on the problem shape and <a class="el" href="group__c__api__types__generic.html#ga5b98c8059c2aff8861157bf070c3f520">propagation kind</a>:</p>
<ul>
<li>For <code>forward_inference</code> Intel MKL-DNN supports \(F(2 \times 2, 3 \times 3)\) or \(F(4 \times 4, 3 \times 3)\)</li>
<li>Intel MKL-DNN supports only \(F(4 \times 4, 3 \times 3)\) Winograd for all the training propagation kinds.</li>
</ul>
<p>The following side effects should be weighed against the (potential) performance boost achieved from using the Winograd algorithm:</p>
<ul>
<li><em>Memory consumption</em>. Winograd implementation in MKL-DNN requires additional scratchpad memory to store intermediate results. As more convolutions using Winograd are added to the topology, the amount of memory required can grow significantly. This growth can be controlled if the scratchpad memory can be reused across multiple primitives. See <a class="el" href="dev_guide_attributes_scratchpad.html">Primitive Attributes: Scratchpad</a> for more details.</li>
<li><em>Accuracy</em>. In some cases Winograd convolution produce results that are significantly less accurate than results from the direct convolution.</li>
</ul>
<p>Create a Winograd convolution by simply creating a convolution descriptor (step 6 in <a class="el" href="cpu_cnn_inference_f32_cpp.html">simple network example</a> specifying the Winograd algorithm. The rest of the steps are exactly the same.</p>
<div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_desc = convolution_forward::desc(</div><div class="line">    prop_kind::forward_inference, algorithm::convolution_winograd,</div><div class="line">    conv1_src_md, conv1_weights_md, conv1_bias_md, conv1_dst_md,</div><div class="line">    conv1_strides, conv1_padding, padding_kind::zero);</div></div><!-- fragment --><h4>Automatic Algorithm Selection</h4>
<p>Intel MKL-DNN supports <code><a class="el" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076acfdececd63a8bc0cfe1021ad614e2ded" title="Convolution algorithm(either direct or Winograd) is chosen just in time. ">mkldnn::algorithm::convolution_auto</a></code> algorithm that instructs the library to automatically select the <em>best</em> algorithm based on the heuristics that take into account tensor shapes and the number of logical processors available. (For automatic selection to work as intended, use the same thread affinity settings when creating the convolution as when executing the convolution.)</p>
<p><a class="anchor" id="dg_conv_impl_limits"></a></p><h2>Implementation Limitations</h2>
<ol type="1">
<li>Refer to <a class="el" href="dev_guide_data_types.html">Data Types</a> for limitations related to data types support.</li>
<li><b>CPU</b><ul>
<li>Winograd are implemented only for Intel(R) AVX-512 or Intel(R) AVX512-DL Boost instruction sets</li>
</ul>
</li>
<li><b>GPU</b><ul>
<li>No support for Winograd algorithm</li>
</ul>
</li>
</ol>
<h2>Performance Tips</h2>
<ul>
<li>Use <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ceda100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">mkldnn::memory::format_tag::any</a> for source, weights, and destinations memory format tags when create a convolution primitive to allow the library to choose the most appropriate memory format. </li>
</ul>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>