<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Getting started</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">1.0.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Getting started </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><b></b></p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="cpu_getting_started_8cpp-example.html">cpu_getting_started.cpp</a></p>
</blockquote>
<p>This C++ API example demonstrates basics of Intel MKL-DNN programming model:</p><ul>
<li>How to create Intel MKL-DNN memory objects.<ul>
<li>How to get data from user's buffer into an Intel MKL-DNN memory object.</li>
<li>How tensor's logical dimensions and memory object formats relate.</li>
</ul>
</li>
<li>How to create Intel MKL-DNN primitives.</li>
<li>How to execute the primitives.</li>
</ul>
<p>The example uses the ReLU operation and consists of the following steps:</p><ol type="1">
<li>Creating <a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_sub1">Engine and stream</a> to execute a primitive.</li>
<li>Performing <a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_sub2">Data preparation (code outside of Intel MKL-DNN)</a>.</li>
<li><a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_sub3">Wrapping data into Intel MKL-DNN memory object</a> (using different flavors).</li>
<li><a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_sub4">Creating a ReLU primitive</a>.</li>
<li><a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_sub5">Executing the ReLU primitive</a>.</li>
<li><a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_sub6">Obtaining the result and validation</a> (checking that the resulting image does not contain negative values).</li>
</ol>
<p>These steps are implemented in the <a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_tutorial">cpu_getting_started_tutorial() function</a> which in turn is called from <a class="el" href="cpu_getting_started_cpp.html#cpu_getting_started_cpp_main">main() function</a> which is also responsible for error handling.</p>
<h1><a class="anchor" id="cpu_getting_started_cpp_headers"></a>
Public headers</h1>
<p>To start using Intel MKL-DNN we should first include <a class="el" href="mkldnn_8hpp.html">mkldnn.hpp</a> header file in the program. We also include <a class="el" href="mkldnn__debug_8h.html">mkldnn_debug.h</a> that contains some debugging facilities like returning a string representation for common Intel MKL-DNN C types.</p>
<p>All C++ API types and functions reside in <code>mkldnn</code> namespace. For simplicity of the example we import this namespace.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="mkldnn_8hpp.html">mkldnn.hpp</a>&quot;</span></div><div class="line"></div><div class="line"><span class="comment">// Optional header to access debug functions like `mkldnn_status2str()`</span></div><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="mkldnn__debug_8h.html">mkldnn_debug.h</a>&quot;</span></div><div class="line"></div><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemkldnn.html">mkldnn</a>;</div></div><!-- fragment --> <h1><a class="anchor" id="cpu_getting_started_cpp_tutorial"></a>
cpu_getting_started_tutorial() function</h1>
<h2><a class="anchor" id="cpu_getting_started_cpp_sub1"></a>
Engine and stream</h2>
<p>All Intel MKL-DNN primitives and memory objects are attached to a particular <a class="el" href="structmkldnn_1_1engine.html">mkldnn::engine</a>, which is an abstraction of an computational device (see also <a class="el" href="dev_guide_basic_concepts.html">Basic Concepts</a>). The primitives are created and optimized for the device they are attached to and the memory objects refer to memory residing on the corresponding device. In particular, that means neither memory objects nor primitives that were created for one engine can be used on another.</p>
<p>To create an engine we should specify the <a class="el" href="structmkldnn_1_1engine.html#a81bcf1ea92d7f98852a2c3e187825de6">mkldnn::engine::kind</a> and the index of the device of the given kind. There is only one CPU engine, so the index must be 0.</p>
<div class="fragment"><div class="line">    <a class="code" href="group__cpp__api__enums.html#gga6d88ff11a07bae09a5c348d314c5d1d9aad1943a9fd6d3d7ee1e6af41a5b0d3e7">engine</a> cpu_engine(engine::kind::cpu, 0);</div></div><!-- fragment --><p> In addition to an engine, all primitives require a <a class="el" href="structmkldnn_1_1stream.html">mkldnn::stream</a> for the execution. The stream encapsulates an execution context and is tied to a particular engine.</p>
<p>The creation is pretty straightforward: </p><div class="fragment"><div class="line">    stream cpu_stream(cpu_engine);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 In the simple cases, when a program works with one device only (e.g. only on CPU), an engine and a stream can be created once and used throughout the program. Some frameworks create singleton objects that hold Intel MKL-DNN engine and stream and are use them throughout the code. <br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 </p>
<h2><a class="anchor" id="cpu_getting_started_cpp_sub2"></a>
Data preparation (code outside of Intel MKL-DNN)</h2>
<p>Now that the preparation work is done, let's create some data to work with. We will create a 4D tensor in NHWC format, which is quite popular in many frameworks.</p>
<p>Note that even though we work with one image only, the image tensor is still 4D. The extra 4th dimension (here N) corresponds to the batch, and, in case of a single image, equals to 1. This is pretty typical to have the batch dimension even when working with a single image.</p>
<p>In Intel MKL-DNN all CNN primitives assume tensors have batch dimension, which is always the first logical dimension (see also <a class="el" href="dev_guide_conventions.html">Naming Conventions</a>).</p>
<div class="fragment"><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> N = 1, H = 13, W = 13, C = 3;</div><div class="line"></div><div class="line">    <span class="comment">// Compute physical strides for each dimension</span></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> stride_N = H * W * C;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> stride_H = W * C;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> stride_W = C;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> stride_C = 1;</div><div class="line"></div><div class="line">    <span class="comment">// An auxiliary function that maps logical index to the physical offset</span></div><div class="line">    <span class="keyword">auto</span> offset = [=](<span class="keywordtype">int</span> n, <span class="keywordtype">int</span> h, <span class="keywordtype">int</span> w, <span class="keywordtype">int</span> c)</div><div class="line">    { <span class="keywordflow">return</span> n * stride_N + h * stride_H + w * stride_W + c * stride_C; };</div><div class="line"></div><div class="line">    <span class="comment">// The image size</span></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> image_size = N * H * W * C;</div><div class="line"></div><div class="line">    <span class="comment">// Allocate a buffer for the image</span></div><div class="line">    std::vector&lt;float&gt; image(image_size);</div><div class="line"></div><div class="line">    <span class="comment">// Initialize the image with some values</span></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> n = 0; n &lt; N; ++n)</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> h = 0; h &lt; H; ++h)</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> w = 0; w &lt; W; ++w)</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> c = 0; c &lt; C; ++c) {</div><div class="line">        <span class="keywordtype">int</span> off = offset(n, h, w, c); <span class="comment">// Get the physical offset of a pixel</span></div><div class="line">        image[off] = -std::cos(off / 10.f);</div><div class="line">    }</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 </p>
<h2><a class="anchor" id="cpu_getting_started_cpp_sub3"></a>
Wrapping data into Intel MKL-DNN memory object</h2>
<p>Now, having the image ready, let's wrap it in an <a class="el" href="structmkldnn_1_1memory.html">mkldnn::memory</a> object to be able to pass the data to Intel MKL-DNN primitives.</p>
<p>Creating <a class="el" href="structmkldnn_1_1memory.html">mkldnn::memory</a> consists of 2 steps:</p><ol type="1">
<li>Initializing the <a class="el" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> struct (also referred as memory descriptor) that only describes the tensor data, but doesn't contain the data itself. Memory descriptors are used to create <a class="el" href="structmkldnn_1_1memory.html">mkldnn::memory</a> objects and to initialize primitive descriptors (shown later in the example);</li>
<li>Creating the <a class="el" href="structmkldnn_1_1memory.html">mkldnn::memory</a> object itself (also referred as a memory object), based on the memory descriptor initialized in step 1, an engine, and, optionally, a handle to a data. The memory object is used when a primitive is executed.</li>
</ol>
<p>Thanks to the <a href="https://en.cppreference.com/w/cpp/language/list_initialization">list initialization</a> introduced in C++11 it is possible to combine these two steps whenever a memory descriptor is not used anywhere else but in creating an <a class="el" href="structmkldnn_1_1memory.html">mkldnn::memory</a> object.</p>
<p>However, for the sake of demonstration, we will show both steps explicitly. <br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 </p>
<h3><a class="anchor" id="cpu_getting_started_cpp_sub31"></a>
Memory descriptor</h3>
<p>To initialize the <a class="el" href="structmkldnn_1_1memory_1_1desc.html">mkldnn::memory::desc</a> we need to pass:</p><ol type="1">
<li>The tensor's dimensions, <b>the semantic order</b> of which is defined by <b>the primitive</b> that will use this memory (descriptor). Which leads to the following: <dl class="section warning"><dt>Warning</dt><dd>Memory descriptors and objects are not aware of any meaning of the data they describe or contain.</dd></dl>
</li>
<li>The data type for the tensor (<a class="el" href="structmkldnn_1_1memory.html#aa8a0d49cc7faaceef9d8f55d66bff57b">mkldnn::memory::data_type</a>).</li>
<li>The memory format tag (<a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ced">mkldnn::memory::format_tag</a>) that describes how the data is going to be laid out in device's memory. The memory format is required for the primitive to correctly handle the data.</li>
</ol>
<p>The code: </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> <a class="code" href="group__cpp__api__enums.html#gga6d88ff11a07bae09a5c348d314c5d1d9a90a729e395453e1d9411ad416c796819">src_md</a> = memory::desc(</div><div class="line">            {N, C, H, W}, <span class="comment">// logical dims, the order is defined by a primitive</span></div><div class="line">            memory::data_type::f32,     <span class="comment">// tensor&#39;s data type</span></div><div class="line">            memory::format_tag::nhwc    <span class="comment">// memory format, NHWC in this case</span></div><div class="line">            );</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 The first thing to notice here is that we pass dimensions as <code>{N, C, H, W}</code> while it might seem more natural to pass <code>{N, H, W, C}</code>, which better corresponds to the user's code. This is because Intel MKL-DNN CNN primitives like ReLU always expect tensors in the following form:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Spatial dim  </th><th class="markdownTableHeadLeft">Ten   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">0D  </td><td class="markdownTableBodyLeft">\(N \times C\)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">1D  </td><td class="markdownTableBodyLeft">\(N \times C \times W\)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">2D  </td><td class="markdownTableBodyLeft">\(N \times C \times H \times W\)   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">3D  </td><td class="markdownTableBodyLeft">\(N \times C \times D \times H \times W\)   </td></tr>
</table>
<p>where:</p><ul>
<li>\(N\) is a batch dimension (discussed above),</li>
<li>\(C\) is channel (aka feature maps) dimension, and</li>
<li>\(D\), \(H\), and \(W\) are spatial dimensions.</li>
</ul>
<p>Now that the logical order of dimension is defined, we need to specify the memory format (the third parameter), which describes how logical indices map to the offset in memory. This is the place where user's format NHWC comes into play. Intel MKL-DNN has different <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ced">mkldnn::memory::format_tag</a> values that covers the most popular memory formats like NCHW, NHWC, CHWN, and some others.</p>
<p>The memory descriptor for the image is called <code>src_md</code>. The <code>src</code> part comes from the fact that the image will be a source for the ReLU primitive (i.e. we formulate memory names from the primitive perspective, hence we will use <code>dst</code> to name the output memory). The <code>md</code> is an acronym for Memory Descriptor. <br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 </p>
<h4><a class="anchor" id="cpu_getting_started_cpp_sub311"></a>
Alternative way to create a memory descriptor</h4>
<p>Before we continue with memory creation, let us show the alternative way to create the same memory descriptor: instead of using the <a class="el" href="structmkldnn_1_1memory.html#a123930e31c5460c2c5f052a59c2a4ced">mkldnn::memory::format_tag</a> we can directly specify the strides of each tensor dimension: </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> alt_src_md = memory::desc(</div><div class="line">            {N, C, H, W}, <span class="comment">// logical dims, the order is defined by a primitive</span></div><div class="line">            memory::data_type::f32,                     <span class="comment">// tensor&#39;s data type</span></div><div class="line">            {stride_N, stride_C, stride_H, stride_W}    <span class="comment">// the strides</span></div><div class="line">            );</div><div class="line"></div><div class="line">    <span class="comment">// Sanity check: the memory descriptors should be the same</span></div><div class="line">    <span class="keywordflow">if</span> (src_md != alt_src_md)</div><div class="line">        <span class="keywordflow">throw</span> std::string(<span class="stringliteral">&quot;memory descriptor initialization mismatch&quot;</span>);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Just as before, the tensor's dimensions come in the <code>N, C, H, W</code> order as required by CNN primitives. To define the physical memory format the strides are passed as the third parameter. Note that the order of the strides corresponds to the order of the tensor's dimensions. </p><dl class="section warning"><dt>Warning</dt><dd>Using the wrong order might lead to incorrect results or even a crash. <br />
<br />
<br />
<br />
<br />
<br />
<br />
 </dd></dl>
<h3><a class="anchor" id="cpu_getting_started_cpp_sub32"></a>
Creating a memory object</h3>
<p>Having a memory descriptor and an engine prepared let's create an input and an output memory objects for ReLU primitive </p><div class="fragment"><div class="line">    <span class="comment">// src_mem refers to a buffer owned by the `image` vector</span></div><div class="line">    <span class="keyword">auto</span> src_mem = memory(src_md, cpu_engine, image.data());</div><div class="line">    <span class="comment">// For dst_mem the library allocates buffer</span></div><div class="line">    <span class="keyword">auto</span> dst_mem = memory(src_md, cpu_engine);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
 We already have a memory buffer for the source memory object. We pass it to the <a class="el" href="structmkldnn_1_1memory.html#a03bd8b5d6d4b395b9fcbb61c6ec1d0fa">mkldnn::memory::memory(const desc &amp;, const engine &amp;, void *)</a> constructor that takes a buffer pointer with its last argument.</p>
<p>Let's use a constructor that instructs the library to allocate a memory buffer for the <code>dst_mem</code> for educational purposes.</p>
<p>The key difference between these two are:</p><ol type="1">
<li>The library will own the memory for <code>dst_mem</code> and will deallocate it when <code>dst_mem</code> is destroyed. That means the memory buffer can only be used while <code>dst_mem</code> is alive.</li>
<li>Library-allocated buffers have good alignment which typically results in better performance.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Memory allocated outside of the library and passed to Intel MKL-DNN should have good alignment for better performance.</dd></dl>
<p>In subsequent section we will show how to get the buffer (pointer) from the <code>dst_mem</code> memory object. <br />
<br />
<br />
<br />
<br />
 </p>
<h2><a class="anchor" id="cpu_getting_started_cpp_sub4"></a>
Creating a ReLU primitive</h2>
<p>Let's now create a ReLU primitive.</p>
<p>The library implements ReLU primitive as a particular algorithm of a more general <a class="el" href="dev_guide_eltwise.html">Eltwise</a> primitive which applies specified function to each and every element of the source tensor.</p>
<p>Just like in case of <a class="el" href="structmkldnn_1_1memory.html">mkldnn::memory</a> a user should always go through (at least) 3 creation steps (which however, can be sometimes combined thanks to C++11):</p><ol type="1">
<li>Initialize operation descriptor (in case of this example, <a class="el" href="structmkldnn_1_1eltwise__forward_1_1desc.html">mkldnn::eltwise_forward::desc</a>), which defines the operation parameters.</li>
<li>Create an operation primitive descriptor (here <a class="el" href="structmkldnn_1_1eltwise__forward_1_1primitive__desc.html">mkldnn::eltwise_forward::primitive_desc</a>), which is a <b>lightweight</b> descriptor of the actual algorithm that <b>implements</b> given operation. User can query different characteristics of the chosen implementation like memory consumptions and some others that will be covered in the next topic (<a class="el" href="cpu_memory_format_propagation_cpp.html">Memory format propagation</a>).</li>
<li>Create a primitive (here <a class="el" href="structmkldnn_1_1eltwise__forward.html">mkldnn::eltwise_forward</a>) that can be executed on memory objects to compute the operation.</li>
</ol>
<p>Intel MKL-DNN separates the steps 2 and 3 to allow user to inspect details of a primitive implementation prior to creating the primitive which may be expensive because, for example, Intel MKL-DNN generates the optimized computational code on the fly.</p>
<dl class="section note"><dt>Note</dt><dd>Primitive creation might be a very expensive operation, so consider creating primitive objects once and executing them multiple times.</dd></dl>
<p>The code: </p><div class="fragment"><div class="line">    <span class="comment">//  ReLU op descriptor (no engine- or implementation-specific information)</span></div><div class="line">    <span class="keyword">auto</span> relu_d = eltwise_forward::desc(</div><div class="line">            prop_kind::forward_inference,</div><div class="line">            algorithm::eltwise_relu,</div><div class="line">            src_md, <span class="comment">// the memory descriptor for an operation to work on</span></div><div class="line">            0.f,    <span class="comment">// alpha parameter means negative slope in case of ReLU</span></div><div class="line">            0.f     <span class="comment">// beta parameter is ignored in case of ReLU</span></div><div class="line">            );</div><div class="line"></div><div class="line">    <span class="comment">// ReLU primitive descriptor, which corresponds to a particular</span></div><div class="line">    <span class="comment">// implementation in the library</span></div><div class="line">    <span class="keyword">auto</span> relu_pd = eltwise_forward::primitive_desc(</div><div class="line">            relu_d,     <span class="comment">// an operation descriptor</span></div><div class="line">            cpu_engine  <span class="comment">// an engine the primitive will be created for</span></div><div class="line">            );</div><div class="line"></div><div class="line">    <span class="comment">// ReLU primitive</span></div><div class="line">    <span class="keyword">auto</span> relu = eltwise_forward(relu_pd); <span class="comment">// !!! this can take quite some time</span></div></div><!-- fragment --><p><br />
<br />
<br />
<br />
 A note about variable names. Similar to the <code>_md</code> suffix used for memory descriptor, we use <code>_d</code> for the operation descriptor names, <code>_pd</code> for the primitive descriptors, and no suffix for primitives themselves.</p>
<p>It is worth mentioning that we specified the exact tensor and its memory format when we were initializing the <code>relu_d</code>. That means <code>relu</code> primitive would perform computations with memory objects that correspond to this description. This is the one and only one way of creating non-compute-intensive primitives like <a class="el" href="dev_guide_eltwise.html">Eltwise</a>, <a class="el" href="dev_guide_batch_normalization.html">Batch Normalization</a>, and others.</p>
<p>Compute-intensive primitives (like <a class="el" href="dev_guide_convolution.html">Convolution</a>) have an ability to define the appropriate memory format on their own. This is one of the key features of the library and will be discussed in detail in the next topic: <a class="el" href="cpu_memory_format_propagation_cpp.html">Memory format propagation</a>. <br />
<br />
<br />
 </p>
<h2><a class="anchor" id="cpu_getting_started_cpp_sub5"></a>
Executing the ReLU primitive</h2>
<p>Finally, let's execute the primitive and wait for its completion.</p>
<p>The input and output memory objects are passed to the <code>execute()</code> method using a &lt;tag, memory&gt; map. Each tag specifies what kind of tensor each memory object represents. All <a class="el" href="dev_guide_eltwise.html">Eltwise</a> primitives require the map to have two elements: a source memory object (input) and a destination memory (output).</p>
<p>A primitive is executed in a stream (the first parameter of the <code>execute()</code> method). Depending on a stream kind an execution might be blocking or non-blocking. This means that we need to call <a class="el" href="structmkldnn_1_1stream.html#a48181ce0f5eb3bcd75778b5aa8866df6">mkldnn::stream::wait</a> before accessing the results.</p>
<div class="fragment"><div class="line">    <span class="comment">// Execute ReLU (out-of-place)</span></div><div class="line">    relu.execute(</div><div class="line">            cpu_stream, <span class="comment">// The execution stream</span></div><div class="line">            {           <span class="comment">// A map with all inputs and outputs</span></div><div class="line">                {MKLDNN_ARG_SRC, src_mem}, <span class="comment">// Source tag and memory obj</span></div><div class="line">                {MKLDNN_ARG_DST, dst_mem}, <span class="comment">// Destination tag and memory obj</span></div><div class="line">            });</div><div class="line"></div><div class="line">    <span class="comment">// Wait the stream to complete the execution</span></div><div class="line">    cpu_stream.wait();</div></div><!-- fragment --><p><br />
<br />
 The <a class="el" href="dev_guide_eltwise.html">Eltwise</a> is one of the primitives that support in-place operations, meaning the source and destination memory can be the same. To perform in-place transformation user needs to pass the same memory object for the both <code>MKLDNN_ARG_SRC</code> and <code>MKLDNN_ARG_DST</code> tags: </p><div class="fragment"><div class="line">    <span class="comment">// Execute ReLU (in-place)</span></div><div class="line">    <span class="comment">// relu.execute(cpu_stream,  {</span></div><div class="line">    <span class="comment">//          {MKLDNN_ARG_SRC, src_mem},</span></div><div class="line">    <span class="comment">//          {MKLDNN_ARG_DST, src_mem},</span></div><div class="line">    <span class="comment">//         });</span></div></div><!-- fragment --><p><br />
</p>
<h2><a class="anchor" id="cpu_getting_started_cpp_sub6"></a>
Obtaining the result and validation</h2>
<p>Now that we have the computed result let's validate that it is actually correct. The result is stored in the <code>dst_mem</code> memory object. So we need to obtain the C++ pointer to a buffer with data via <a class="el" href="structmkldnn_1_1memory.html#af121ae649cfe2fa3f92be40efb22b337">mkldnn::memory::get_data_handle()</a> and cast it to the proper data type as shown below.</p>
<dl class="section warning"><dt>Warning</dt><dd>The <a class="el" href="structmkldnn_1_1memory.html#af121ae649cfe2fa3f92be40efb22b337">mkldnn::memory::get_data_handle()</a> returns a raw handle to the buffer which type is engine specific. For CPU engine the buffer is always a pointer to <code>void</code> which can safely be used. However, for engines other than CPU the handle might be runtime-specific type, such as <code>cl_mem</code> in case of GPU/OpenCL.</dd></dl>
<div class="fragment"><div class="line">    <span class="comment">// Obtain a buffer for the `dst_mem` and cast it to `float *`.</span></div><div class="line">    <span class="comment">// This is safe since we created `dst_mem` as f32 tensor with known</span></div><div class="line">    <span class="comment">// memory format.</span></div><div class="line">    <span class="keywordtype">float</span> *relu_image = <span class="keyword">static_cast&lt;</span><span class="keywordtype">float</span> *<span class="keyword">&gt;</span>(dst_mem.get_data_handle());</div><div class="line"></div><div class="line">    <span class="comment">// Check the results</span></div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> n = 0; n &lt; N; ++n)</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> h = 0; h &lt; H; ++h)</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> w = 0; w &lt; W; ++w)</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> c = 0; c &lt; C; ++c) {</div><div class="line">        <span class="keywordtype">int</span> off = offset(n, h, w, c); <span class="comment">// get the physical offset of a pixel</span></div><div class="line">        <span class="keywordtype">float</span> expected = image[off] &lt; 0 ? 0.f : image[off]; <span class="comment">// expected value</span></div><div class="line">        <span class="keywordflow">if</span> (relu_image[off] != expected) {</div><div class="line">            std::stringstream ss;</div><div class="line">            ss &lt;&lt; <span class="stringliteral">&quot;Unexpected output at index(&quot;</span></div><div class="line">                &lt;&lt; n &lt;&lt; <span class="stringliteral">&quot;, &quot;</span> &lt;&lt; c &lt;&lt; <span class="stringliteral">&quot;, &quot;</span> &lt;&lt; h &lt;&lt; <span class="stringliteral">&quot;, &quot;</span> &lt;&lt; w &lt;&lt; <span class="stringliteral">&quot;): &quot;</span></div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Expect &quot;</span> &lt;&lt; expected &lt;&lt; <span class="stringliteral">&quot; &quot;</span></div><div class="line">                &lt;&lt; <span class="stringliteral">&quot;Got &quot;</span> &lt;&lt; relu_image[off];</div><div class="line">            <span class="keywordflow">throw</span> ss.str();</div><div class="line">        }</div><div class="line">    }</div></div><!-- fragment --> <h1><a class="anchor" id="cpu_getting_started_cpp_main"></a>
main() function</h1>
<p>We now just call everything we prepared earlier.</p>
<p>Since we are using Intel MKL-DNN C++ API we use exception to handle errors (see <a class="el" href="dev_guide_c_and_cpp_apis.html">C and C++ APIs</a>). The Intel MKL-DNN C++ API throws exceptions of type <a class="el" href="structmkldnn_1_1error.html">mkldnn::error</a>, which contains the error status (of type <a class="el" href="group__c__api__types__generic.html#ga31866789b66acfb1c28b2f9bdd7bdfdd">mkldnn_status_t</a>) and a human-readable error message accessible through regular <code>what()</code> method.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> main(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> **argv) {</div><div class="line">    <span class="keywordflow">try</span> {</div><div class="line">        cpu_getting_started_tutorial();</div><div class="line">    } <span class="keywordflow">catch</span> (<a class="code" href="structmkldnn_1_1error.html">mkldnn::error</a> &amp;e) {</div><div class="line">        std::cerr &lt;&lt; <span class="stringliteral">&quot;Intel MKL-DNN error: &quot;</span> &lt;&lt; e.<a class="code" href="structmkldnn_1_1error.html#ac454d6c74b6c5bf2bae982d506b98f64">what</a>() &lt;&lt; std::endl</div><div class="line">            &lt;&lt; <span class="stringliteral">&quot;Error status: &quot;</span> &lt;&lt; mkldnn_status2str(e.status) &lt;&lt; std::endl;</div><div class="line">        <span class="keywordflow">return</span> 1;</div><div class="line">    } <span class="keywordflow">catch</span> (std::string &amp;e) {</div><div class="line">        std::cerr &lt;&lt; <span class="stringliteral">&quot;Error in the example: &quot;</span> &lt;&lt; e &lt;&lt; std::endl;</div><div class="line">        <span class="keywordflow">return</span> 2;</div><div class="line">    }</div><div class="line"></div><div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Example passes&quot;</span> &lt;&lt; std::endl;</div><div class="line">    <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <b></b></p>
<p>Upon compiling and run the example the output should be just:</p>
<div class="fragment"><div class="line">Example passes</div></div><!-- fragment --><p>Users are encouraged to experiment with the code to familiarize themselves with the concepts. In particular, one of the changes that might be of interest is to spoil some of the library calls to check how error handling happens. For instance, if we replace</p>
<div class="fragment"><div class="line">relu.execute(cpu_stream, {</div><div class="line">        {MKLDNN_ARG_SRC, src_mem},</div><div class="line">        {MKLDNN_ARG_DST, dst_mem},</div><div class="line">    });</div></div><!-- fragment --><p>with</p>
<div class="fragment"><div class="line">relu.execute(cpu_stream, {</div><div class="line">        {MKLDNN_ARG_SRC, src_mem},</div><div class="line">        <span class="comment">// {MKLDNN_ARG_DST, dst_mem}, // Oops, forgot about this one</span></div><div class="line">    });</div></div><!-- fragment --><p>we should get the following output:</p>
<div class="fragment"><div class="line">Intel MKL-DNN error: could not execute a primitive</div><div class="line">Error status: invalid_arguments</div></div><!-- fragment --> </div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>