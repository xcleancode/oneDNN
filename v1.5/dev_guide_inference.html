<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>oneDNN: Inference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">oneAPI Deep Neural Network Library (oneDNN)
   &#160;<span id="projectnumber">1.5.0</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>oneDNN includes primitives for operations throughout a deep learning network topology. However, it is important to note the scope of oneDNN is limited to performance critical functionality and the library does not provide all the functions necessary to implement deep learning workloads, for instance data preprocessing or computing loss function. The soft-max classifier is the sole classifier included, but the application of other classifier types will require user's own implementations. The scope of the library is depicted in the following image:</p>
<p> <style>div.image img[src="img_inference_scope.jpg"]{width:80%;}</style><br />
</p><div class="image">
<img src="img_inference_scope.jpg" alt="img_inference_scope.jpg"/>
</div>
 <h2>Best Practices for Inference</h2>
<h2>fp32 Inference</h2>
<p><b>Use Forward Inference Primitives</b></p>
<p>oneDNN provides a forward pass version of each primitive, that avoids storing information required for a backward pass (as in training).</p>
<p>Use the <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa3b9fad4f80d45368f856b5403198ac4c" title="Forward data propagation (inference mode). ">dnnl::prop_kind::forward_inference</a> argument at creation of the <b>operation descriptor</b>, as in this convolution example: </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv_descr = convolution_forward::desc(prop_kind::forward_inference, ...);</div></div><!-- fragment --><p><b>Layout Propagation</b></p>
<p>Compute-intensive oneDNN primitives execute with highest performance on CPU-friendly data formats. Please see description of data formats <a class="el" href="memory_format_propagation_cpp.html">here</a>.</p>
<p>Performance gains are maximized by reordering once, and then propagating the CPU-friendly format through as many layers as possible in your topology. oneDNN provides the <code>format_tag=any</code> for memory descriptors that will be passed to compute-intensive primitives. The compute-intensive primitive types in oneDNN are <a class="el" href="dev_guide_convolution.html">Convolution</a>, <a class="el" href="dev_guide_inner_product.html">Inner Product</a>, and <a class="el" href="dev_guide_rnn.html">RNN</a>.</p>
<p>To accomplish this propagation in a robust manner, its is recommended to follow these steps:</p>
<p>A. On compute-intensive operations:</p><ul>
<li>Pass the <code>format_tag=any</code> when creating oneDNN <b>memory descriptor</b> for source, destination, and weights memory</li>
<li>Use these three <em>memory descriptors</em> with 'format _tag=any` to create <b>operation descriptor</b></li>
<li>Use <em>operation descriptor</em> to create engine-aware <b>primitive descriptor</b></li>
<li>Query the <em>primitive descriptor</em> with <code>.src_desc()</code> method to get recommended format</li>
<li>Write conditional reorder to execute only if user source data or weights don't match the recommended format</li>
<li>Create <b>primitive</b> and add it to stream with <code>primitive.execute(stream, args)</code></li>
</ul>
<p>B. On non-intensive operations:</p><ul>
<li>Query output <b>primitive descriptor</b> with <code>.dst_desc()</code> from previous operation to find current layout</li>
<li>Pass current layout with <code>format_tag=.dst_desc()</code> when creating non-intensive <b>operation descriptor</b></li>
<li>Create <b>primitive</b> and add it to stream with <code>operation.execute(stream, args)</code></li>
</ul>
<p>Now let's take a look at the code syntax to accomplish the compute-intensive steps:</p>
<p>Pass the <code>format_tag=any</code> when creating oneDNN <b>memory descriptor</b> for source, destination, and weights memory </p><div class="fragment"><div class="line">source_mem_descr = memory::desc(args*, memory::format_tag::any);</div><div class="line">dest_mem_descr = memory::desc(args*, memory::format_tag::any);</div><div class="line">weights_mem_descr = memory::desc(args*, memory::format_tag::any);</div></div><!-- fragment --><p>Use these three <em>memory descriptors</em> with 'format _tag=any` to create <b>operation descriptor</b> </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv_descr = convolution_forward::desc(...,</div><div class="line">            source_mem_descr, weights_mem_descr, dest_mem_descr);</div></div><!-- fragment --><p>Use <em>operation descriptor</em> to create engine-aware <b>primitive descriptor</b> </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv_prim_descr = convolution_forward::primitive_desc(conv_descr, engine);</div></div><!-- fragment --><p>Query the <em>primitive descriptor</em> with <code>.src_desc()</code> method to get recommended format Write conditional reorder to execute only if user source data or weights don't match the recommended format (Note: Do this for weight_memory as well) </p><div class="fragment"><div class="line">memory conv_source_memory = user_source_memory;</div><div class="line"><span class="keywordflow">if</span> (conv_prim_descr.src_desc() != user_source_memory.get_desc()) {</div><div class="line">    conv_source_memory = memory(conv_prim_descr.src_desc(), <a class="code" href="group__dnnl__api__primitives__common.html#gga94efdd650364f4d9776cfb9b711cbdc1aad1943a9fd6d3d7ee1e6af41a5b0d3e7">engine</a>);</div><div class="line">    <span class="keyword">auto</span> reorder_prim_descr = reorder::primitive_desc(user_source_memory, conv_source_memory);</div><div class="line">    reorder(reorder_prim_descr).execute(s, user_source_memory, conv_source_memory);</div><div class="line">}</div></div><!-- fragment --><p> Create <b>primitive</b> and add it to stream with <code>primitive.execute(stream, args)</code> </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv = convolution_forward(conv_prim_descr);</div><div class="line">conv.execute(s, {</div><div class="line">            {<a class="code" href="group__dnnl__api__primitives__common.html#gac37ad67b48edeb9e742af0e50b70fe09">DNNL_ARG_SRC</a>, conv_source_memory},</div><div class="line">            {<a class="code" href="group__dnnl__api__primitives__common.html#gaf279f28c59a807e71a70c719db56c5b3">DNNL_ARG_WEIGHTS</a>, conv_weights_memory},</div><div class="line">            {<a class="code" href="group__dnnl__api__primitives__common.html#ga3ca217e4a06d42a0ede3c018383c388f">DNNL_ARG_DST</a>, conv_dest_memory}});</div></div><!-- fragment --><p><b>Cache Weights</b>\ Weights are accessed many times during batched inference. At inference time these weights are essentially constants in the mapping function that the network is applying to the input data. As such, the weights should be reordered (if necessary) once and then used in the reorder form for the duration of the execution. This caching causes the computer to use them in a way similar to how a mathematical function applies a constant, i..e, "Grab-and-go" with no overhead for creation or reorder.</p>
<p><b>Primitive Reuse</b>\ There is JIT compilation overhead associated with primitive creation. It is recommended to reuse any primitive that you can, and only create them once.</p>
<p><b>Fused Primitives</b>\ oneDNN provides fused versions of primitives that attach a non-intensive operation to the end of a compute-intensive operation and then executes both in a single pass, reducing the number of memory accesses needed for the combined operations. The non-intensive operation is added as a <b>post-op</b> attribute to the compute intensive primitive descriptor. Please note that post-ops do not change the number of inputs or outputs of the primitives. Please see the "Post-ops and Attributes" section of the doc for each primitive type in /docs/primitive/ for a list of available fused primitives.</p>
<p>A good example is adding ReLU as a post-op to convolution, which we will use as a demonstration below. The steps are</p>
<ul>
<li>Create a <code>post_op</code> for fused ReLU</li>
<li>Create <b>primitive attribute</b> and add the <code>post_op</code></li>
<li>Create a convolution <b>descriptor</b></li>
<li>Create a convolution <b>primitive descriptor</b>, passing <code>post_op as</code> an arg</li>
</ul>
<p>Create a <code>post_op</code> for fused ReLU </p><div class="fragment"><div class="line">post_ops ops;</div><div class="line">ops.append_eltwise(..., algorithm::eltwise_relu);</div></div><!-- fragment --><p>Create <b>primitive attribute</b> and add the <code>post_op</code> </p><div class="fragment"><div class="line">primitive_attr attr;</div><div class="line">attr.<a class="code" href="structdnnl_1_1primitive__attr.html#ac830fa9f4fcf480b494d73153ad579bf">set_post_ops</a>(ops);</div></div><!-- fragment --><p>Create a convolution <b>descriptor</b> </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv_descr = convolution_forward::desc(...);</div></div><!-- fragment --><p>Create a convolution <b>primitive descriptor</b>, passing the post-op infused <code>attrs</code> as an arg </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv_prim_descr = convolution_forward::primitive_desc(conv_descr, attrs, engine);</div></div><!-- fragment --><h2>int8 Inference</h2>
<p>oneDNN supports low precision int8 for inference execution. Note that not all primitives have int8 versions. Sometimes the speed benefits would be minimal, or the loss in accuracy is not acceptable. Also the soft-max classifier only supports fp32, so int8 inference will require a reorder before executing this primitive.</p>
<p>By default, the oneDNN reorder primitive does not scale upon casting to int8. In order to compress fp32 data to int8 precision while still preserving the entire shape of the distribution, a process called <b>quantization</b> must applied. Quantization will scale the data based on its range to efficiently fill the bits available for int8 type.</p>
<p>To achieve quantization upon casting, the user must provide a few inputs to oneDNN in order to use int8 inference:</p>
<ul>
<li>Specify data type at creation of primitive descriptor (int8 in this case)</li>
<li>Provide a scaling factor for oneDNN reorder primitive</li>
<li>Provide an output scaling factor the operation primitive</li>
</ul>
<p>Please see the dedicated <a class="el" href="dev_guide_inference_int8.html">section</a> on low precision computations in oneDNN for a detailed discussion, including how to calculate the scaling factors. </p>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>