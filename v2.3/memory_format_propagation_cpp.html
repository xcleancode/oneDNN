<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN: Memory Format Propagation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library (oneDNN)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library (oneDNN)</div>
    </a>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
   <div id="projectnumber">2.3.3</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('memory_format_propagation_cpp.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Memory Format Propagation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This example demonstrates memory format propagation, which is critical for deep learning applications performance.</p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="memory_format_propagation_8cpp-example.html">memory_format_propagation.cpp</a> </p>
</blockquote>
<p>Memory format propagation is one of the central notions that needs to be well-understood to use oneDNN correctly.</p>
<p>Convolution and inner product primitives choose the memory format when you create them with the placeholder memory format <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> for input or output. The memory format chosen is based on different circumstances such as hardware and convolutional parameters. Using the placeholder memory format is the recommended practice for convolutions, since they are the most compute-intensive operations in most topologies where they are present.</p>
<p>Other primitives, such as Elementwise, LRN, batch normalization and other, on forward propagation should use the same memory format as the preceding layer thus propagating the memory format through multiple oneDNN primitives. This avoids unnecessary reorders which may be expensive and should be avoided unless a compute-intensive primitive requires a different format. For performance reasons, backward computations of such primitives requires consistent memory format with the corresponding forward computations. Hence, when initializing there primitives for backward computations you should use <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> memory format tag as well.</p>
<p>Below is the short summary when to use and not to use memory format <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> during operation description initialization:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Primitive Kinds  </th><th class="markdownTableHeadLeft">Forward Propagation  </th><th class="markdownTableHeadLeft">Backward Propagation  </th><th class="markdownTableHeadLeft">No Propagation   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">Compute intensive: (De-)convolution, Inner product, RNN  </td><td class="markdownTableBodyLeft">Use <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a>  </td><td class="markdownTableBodyLeft">Use <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a>  </td><td class="markdownTableBodyLeft">N/A   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft">Memory-bandwidth limited: Pooling, Layer and Batch Normalization, Local Response Normalization, Elementwise, Shuffle, Softmax  </td><td class="markdownTableBodyLeft">Use memory format from preceding layer for inputs, and <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> for outputs  </td><td class="markdownTableBodyLeft">Use <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> for gradient tensors, and actual memory formats for data tensors  </td><td class="markdownTableBodyLeft">N/A   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">Memory-bandwidth limited: Reorder, Concat, Sum, Binary  </td><td class="markdownTableBodyLeft">N/A  </td><td class="markdownTableBodyLeft">N/A  </td><td class="markdownTableBodyLeft">Use memory format from preceding layer for inputs, and <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> for outputs   </td></tr>
</table>
<p>Additional format synchronization is required between forward and backward computations when running training workloads. This topic is covered in <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_training">Training-Specific Aspects</a>.</p>
<p>For better understanding of the architecture and design of oneDNN as well as the concepts used in the library, please refer to <a class="el" href="dev_guide_understanding_memory_formats.html">Understanding Memory Formats</a>.</p>
<h1><a class="anchor" id="memory_format_propagation_intro"></a>
Introduction to the tutorial</h1>
<p>This C++ API example demonstrates how to use optimized memory formats supported by oneDNN:</p><ul>
<li>How to configure primitives to use optimized memory formats.</li>
<li>How to determine whether data needs to be reordered from/to optimized memory formats.</li>
</ul>
<p>This tutorial assumes that the reader has already reviewed the <a class="el" href="getting_started_cpp.html">Getting started</a> tutorial.</p>
<p>The example is built around a CNN consisting of a convolution followed by a pooling and consists of the following steps:</p><ol type="1">
<li>Create a pooling primitive descriptor based on the memory format chosen by the convolution primitive.</li>
<li>Create memory descriptors for input and output data in the NCHW memory format.</li>
<li>Determine if input and output data needs to be reordered from/to the optimized memory format.</li>
<li>Create memory objects; and necessary primitives and execute them.</li>
</ol>
<p>These steps are implemented in the <a class="el" href="memory_format_propagation_cpp.html#memory_format_propagation_tutorial">memory_format_propagation() function</a> which in turn is called from <code>main()</code> which is also responsible for error handling.</p>
<h1><a class="anchor" id="memory_format_propagation_tutorial"></a>
memory_format_propagation() function</h1>
<h2><a class="anchor" id="memory_format_propagation_sub1"></a>
Initialization</h2>
<p>We start by creating an engine and a stream that we will use when creating primitive descriptors and executing primitives.</p>
<div class="fragment"><div class="line">    <a class="code" href="group__dnnl__api__primitives__common.html#gga94efdd650364f4d9776cfb9b711cbdc1aad1943a9fd6d3d7ee1e6af41a5b0d3e7">engine</a> eng(engine_kind, 0);</div><div class="line">    stream s(eng);</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub2"></a>
Create convolution and pooling primitives</h2>
<p>To specify that a primitive should pick an optimized format for the specified computation parameters, we create memory descriptors with memory format set to <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec">dnnl::memory::format_tag::any</a>.</p>
<p>This approach works only for a limited set of primitives: convolutions and inner products. Additionally, <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec">dnnl::memory::format_tag::any</a> can be specified for destination memory descriptors which implies that destination will have the same memory format as the source.</p>
<div class="fragment"><div class="line">    <span class="comment">// Tensor and kernel dimensions. We use the same 3x3 kernel with padding=1</span></div><div class="line">    <span class="comment">// for both convolution and pooling primitives, which means that the</span></div><div class="line">    <span class="comment">// activation tensor shapes do not change.</span></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> N = 1, H = 14, W = 14, IC = 128, OC = 256, KH = 3, KW = 3;</div><div class="line">    <span class="keyword">auto</span> conv_src_md = memory::desc({N, IC, H, W}, memory::data_type::f32,</div><div class="line">            memory::format_tag::any <span class="comment">// let convolution choose memory format</span></div><div class="line">    );</div><div class="line">    <span class="keyword">auto</span> conv_weights_md = memory::desc(</div><div class="line">            {OC, IC, KH, KW}, memory::data_type::f32,</div><div class="line">            memory::format_tag::any <span class="comment">// let convolution choose memory format</span></div><div class="line">    );</div><div class="line">    <span class="keyword">auto</span> conv_dst_md = memory::desc({N, OC, H, W}, memory::data_type::f32,</div><div class="line">            memory::format_tag::any <span class="comment">// let convolution choose memory format</span></div><div class="line">    );</div><div class="line">    <span class="keyword">auto</span> pool_dst_md = conv_dst_md; <span class="comment">// shape does not change</span></div></div><!-- fragment --><p> Next, we pass the memory descriptors to primitive descriptors constructors.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_pd = convolution_forward::primitive_desc(</div><div class="line">            {prop_kind::forward_inference, algorithm::convolution_auto,</div><div class="line">                    conv_src_md, conv_weights_md,</div><div class="line">                    conv_dst_md, <span class="comment">// shape information</span></div><div class="line">                    {1, 1}, <span class="comment">// strides</span></div><div class="line">                    {1, 1}, {1, 1}}, <span class="comment">// left and right padding</span></div><div class="line">            eng);</div><div class="line">    <span class="keyword">auto</span> pool_pd = pooling_forward::primitive_desc(</div><div class="line">            {prop_kind::forward_inference, algorithm::pooling_max,</div><div class="line">                    conv_pd.dst_desc(), pool_dst_md, <span class="comment">// shape information</span></div><div class="line">                    {1, 1}, {KH, KW}, <span class="comment">// strides and kernel</span></div><div class="line">                    {1, 1}, {1, 1}}, <span class="comment">// left and right padding</span></div><div class="line">            eng);</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub3"></a>
Create source and destination memory objects</h2>
<p>We assume that the 'user' source and destination memory format is NCHW. Since there is no result validation in this tutorial, we do not bother with filling the data with some values and let oneDNN allocate the memory.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> src_mem = memory(</div><div class="line">            {{N, IC, H, W}, memory::data_type::f32, memory::format_tag::nchw},</div><div class="line">            eng);</div><div class="line">    <span class="keyword">auto</span> weights_mem = memory({{OC, IC, KH, KW}, memory::data_type::f32,</div><div class="line">                                      memory::format_tag::oihw},</div><div class="line">            eng);</div><div class="line">    <span class="keyword">auto</span> dst_mem = memory(</div><div class="line">            {{N, OC, H, W}, memory::data_type::f32, memory::format_tag::nchw},</div><div class="line">            eng);</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub4"></a>
Determine if source and destination need to be reordered</h2>
<p>The idiomatic way to check if a reorder is necessary between the memory format expected a primitive (the convolution in our case) and the available memory format is to compare the corresponding memory descriptors.</p>
<div class="fragment"><div class="line">    <span class="keywordtype">bool</span> need_reorder_src = conv_pd.src_desc() != src_mem.get_desc();</div></div><!-- fragment --> <dl class="section warning"><dt>Warning</dt><dd>It is by design that it is not possible to just compare memory tags. The reason behind this is that a memory format tags only provide a partial description of how data is laid out in memory and do not, for example, describe memory objects obtained via sub-memory constructor.</dd></dl>
<p>We repeat the process for the weights and destination memory format descriptors as well.</p>
<div class="fragment"><div class="line">    <span class="keywordtype">bool</span> need_reorder_weights</div><div class="line">            = conv_pd.weights_desc() != weights_mem.get_desc();</div><div class="line">    <span class="keywordtype">bool</span> need_reorder_dst = conv_pd.dst_desc() != dst_mem.get_desc();</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub45"></a>
Allocate intermediate buffers if necessary</h2>
<p>Based on the flags computed before, we can now decide if we need extra intermediate buffers to hold the source and weights data for the convolution and the output of the pooling.</p>
<p>Memory objects for the intermediate buffers are created based on the memory descriptors obtained from the primitive descriptors to ensure consistency.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_src_mem</div><div class="line">            = need_reorder_src ? memory(conv_pd.src_desc(), eng) : src_mem;</div><div class="line">    <span class="keyword">auto</span> conv_weights_mem = need_reorder_weights</div><div class="line">            ? memory(conv_pd.weights_desc(), eng)</div><div class="line">            : weights_mem;</div><div class="line">    <span class="keyword">auto</span> conv_dst_mem = memory(conv_pd.dst_desc(), eng);</div><div class="line">    <span class="keyword">auto</span> pool_dst_mem</div><div class="line">            = need_reorder_dst ? memory(pool_pd.dst_desc(), eng) : dst_mem;</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub5"></a>
Perform reorders for source data if necessary</h2>
<p>Now we get to the part where we actually start executing things. We check if reorders are necessary based on the flags computed before and create and execute them immediately.</p>
<dl class="section note"><dt>Note</dt><dd>We call <a class="el" href="structdnnl_1_1stream.html#a59985fa8746436057cf51a820ef8929c">dnnl::stream::wait()</a> before reorder primitives get out of scope and destroyed to accommodate for potentially asynchronous execution.</dd></dl>
<div class="fragment"><div class="line">    <span class="keywordflow">if</span> (need_reorder_src) {</div><div class="line">        <span class="keyword">auto</span> reorder_src = reorder(src_mem, conv_src_mem);</div><div class="line">        reorder_src.execute(</div><div class="line">                s, {{<a class="code" href="group__dnnl__api__primitives__common.html#ga953b34f004a8222b04e21851487c611a">DNNL_ARG_FROM</a>, src_mem}, {<a class="code" href="group__dnnl__api__primitives__common.html#gaf700c3396987b450413c8df5d78bafd9">DNNL_ARG_TO</a>, conv_src_mem}});</div><div class="line">        s.wait(); <span class="comment">// wait for the reorder to complete</span></div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keywordflow">if</span> (need_reorder_weights) {</div><div class="line">        <span class="keyword">auto</span> reorder_weights = reorder(weights_mem, conv_weights_mem);</div><div class="line">        reorder_weights.execute(s,</div><div class="line">                {{<a class="code" href="group__dnnl__api__primitives__common.html#ga953b34f004a8222b04e21851487c611a">DNNL_ARG_FROM</a>, weights_mem},</div><div class="line">                        {<a class="code" href="group__dnnl__api__primitives__common.html#gaf700c3396987b450413c8df5d78bafd9">DNNL_ARG_TO</a>, conv_weights_mem}});</div><div class="line">        s.wait(); <span class="comment">// wait for the reorder to complete</span></div><div class="line">    }</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub6"></a>
Create and execute convolution and pooling primitives</h2>
<p>After the reorders, we are now ready to compute convolution and pooling.</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_scratchpad_mem = memory(conv_pd.scratchpad_desc(), eng);</div><div class="line">    <span class="keyword">auto</span> conv = convolution_forward(conv_pd);</div><div class="line">    conv.execute(s,</div><div class="line">            {{<a class="code" href="group__dnnl__api__primitives__common.html#gac37ad67b48edeb9e742af0e50b70fe09">DNNL_ARG_SRC</a>, conv_src_mem}, {<a class="code" href="group__dnnl__api__primitives__common.html#gaf279f28c59a807e71a70c719db56c5b3">DNNL_ARG_WEIGHTS</a>, conv_weights_mem},</div><div class="line">                    {<a class="code" href="group__dnnl__api__primitives__common.html#ga3ca217e4a06d42a0ede3c018383c388f">DNNL_ARG_DST</a>, conv_dst_mem}});</div><div class="line">    <span class="keyword">auto</span> pool_scratchpad_mem = memory(pool_pd.scratchpad_desc(), eng);</div><div class="line">    <span class="keyword">auto</span> pool = pooling_forward(pool_pd);</div><div class="line">    pool.execute(</div><div class="line">            s, {{<a class="code" href="group__dnnl__api__primitives__common.html#gac37ad67b48edeb9e742af0e50b70fe09">DNNL_ARG_SRC</a>, conv_dst_mem}, {<a class="code" href="group__dnnl__api__primitives__common.html#ga3ca217e4a06d42a0ede3c018383c388f">DNNL_ARG_DST</a>, pool_dst_mem}});</div><div class="line">    s.wait();</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_sub7"></a>
Reorder destination data if necessary</h2>
<p>The only potentially remaining operation is a reorder from the pooling destination memory object to the user's one. Similarly to the reorders for the source and weights memory objects, it is performed depending on the value of the previously computed flag.</p>
<div class="fragment"><div class="line">    <span class="keywordflow">if</span> (need_reorder_dst) {</div><div class="line">        <span class="keyword">auto</span> reorder_dst = reorder(pool_dst_mem, dst_mem);</div><div class="line">        reorder_dst.execute(</div><div class="line">                s, {{<a class="code" href="group__dnnl__api__primitives__common.html#ga953b34f004a8222b04e21851487c611a">DNNL_ARG_FROM</a>, pool_dst_mem}, {<a class="code" href="group__dnnl__api__primitives__common.html#gaf700c3396987b450413c8df5d78bafd9">DNNL_ARG_TO</a>, dst_mem}});</div><div class="line">        s.wait();</div><div class="line">    }</div></div><!-- fragment --> <h2><a class="anchor" id="memory_format_propagation_results"></a>
Results</h2>
<p>Upon compiling and run the example the output should be just:</p>
<div class="fragment"><div class="line">Example passed.</div></div><!-- fragment --><p>It may be interesting to check what really happens during the run. We can use <code>DNNL_VERBOSE</code> environment variable for that (see also <a class="el" href="dev_guide_verbose.html">Verbose Mode</a>). Here's an example output:</p>
<div class="fragment"><div class="line">$ DNNL_VERBOSE=1 ./memory-format-propagation-cpp</div><div class="line">dnnl_verbose,info,oneDNN &lt;ver&gt; (Git Hash &lt;hash&gt;)</div><div class="line">dnnl_verbose,info,cpu,runtime:OpenMP</div><div class="line">dnnl_verbose,info,cpu,isa:Intel AVX2</div><div class="line">dnnl_verbose,info,gpu,runtime:none</div><div class="line">dnnl_verbose,exec,cpu,reorder,jit:uni,undef,</div><div class="line">    src_f32::blocked:abcd:f0 dst_f32::blocked:aBcd8b:f0,,,1x128x14x14,0.326904</div><div class="line">dnnl_verbose,exec,cpu,reorder,jit:uni,undef,</div><div class="line">    src_f32::blocked:abcd:f0 dst_f32::blocked:ABcd8b8a:f0,,,256x128x3x3,0.244141</div><div class="line">dnnl_verbose,exec,cpu,convolution,jit:avx2,forward_inference,</div><div class="line">    src_f32::blocked:aBcd8b:f0 wei_f32::blocked:ABcd8b8a:f0 bia_undef::undef::f0 dst_f32::blocked:aBcd8b:f0,,</div><div class="line">    alg:convolution_direct,mb1_ic128oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.20312</div><div class="line">dnnl_verbose,exec,cpu,pooling,jit:avx,forward_inference,</div><div class="line">    src_f32::blocked:aBcd8b:f0 dst_f32::blocked:aBcd8b:f0 ws_undef::undef::f0,,</div><div class="line">    alg:pooling_max,mb1ic256_ih14oh14kh3sh1ph1_iw14ow14kw3sw1pw1,0.187012</div><div class="line">dnnl_verbose,exec,cpu,reorder,jit:uni,undef,</div><div class="line">    src_f32::blocked:aBcd8b:f0 dst_f32::blocked:abcd:f0,,,1x256x14x14,0.0419922</div><div class="line">Example passed on CPU.</div></div><!-- fragment --><p>From this output we can deduce that:</p><ul>
<li>The convolution primitive picked up <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa448a7fc9219294ce172b0edf9498b5c4">dnnl::memory::format_tag::aBcd8b</a> optimized memory format for activations. In this format the channels dimension (denoted by letter B since it is the second dimension; see also <a class="el" href="dev_guide_conventions.html">Naming Conventions</a>) is blocked by a factor of 8. Because of this memory format is different from the NCHW format the tutorial uses, the source and destination had to be reordered to and from this optimized memory layout.</li>
<li>The convolution primitive picked up <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fabcbce50e9c241458767871fa053e1ba0">dnnl::memory::format_tag::ABcd8b8a</a> optimized memory format (output (A) and input (B) channel dimensions blocked by 8) which we also had to reorder the initial weights to since they are in the OIHW memory format. </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>