<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>DNNL: CNN f32 inference example</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Deep Neural Network Library (DNNL)
   &#160;<span id="projectnumber">1.92.0</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">CNN f32 inference example </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This C++ API example demonstrates how to build an AlexNet neural network topology for forward-pass inference.</p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="cnn_inference_f32_8cpp-example.html">cnn_inference_f32.cpp</a> </p>
</blockquote>
<p>Some key take-aways include:</p>
<ul>
<li>How tensors are implemented and submitted to primitives.</li>
<li>How primitives are created.</li>
<li>How primitives are sequentially submitted to the network, where the output from primitives is passed as input to the next primitive. The latter specifies a dependency between the primitive input and output data.</li>
<li>Specific 'inference-only' configurations.</li>
<li>Limiting the number of reorders performed that are detrimental to performance.</li>
</ul>
<p>The example implements the AlexNet layers as numbered primitives (for example, conv1, pool1, conv2).</p>
<p>Initialize an engine and stream. The last parameter in the call represents the index of the engine. </p><div class="fragment"><div class="line">    <a class="code" href="group__dnnl__api__primitives__common.html#gga94efdd650364f4d9776cfb9b711cbdc1aad1943a9fd6d3d7ee1e6af41a5b0d3e7">engine</a> eng(engine_kind, 0);</div><div class="line">    stream s(eng);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create a vector for the primitives and a vector to hold memory that will be used as arguments. </p><div class="fragment"><div class="line">    std::vector&lt;primitive&gt; net;</div><div class="line">    std::vector&lt;std::unordered_map&lt;int, memory&gt;&gt; net_args;</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Allocate buffers for input and output data, weights, and bias. </p><div class="fragment"><div class="line">    std::vector&lt;float&gt; user_src(batch * 3 * 227 * 227);</div><div class="line">    std::vector&lt;float&gt; user_dst(batch * 1000);</div><div class="line">    std::vector&lt;float&gt; conv1_weights(product(conv1_weights_tz));</div><div class="line">    std::vector&lt;float&gt; conv1_bias(product(conv1_bias_tz));</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create memory that describes data layout in the buffers. This example uses tag::nchw (batch-channels-height-width) for input data and tag::oihw for weights. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> user_src_memory = memory({{conv1_src_tz}, dt::f32, tag::nchw}, eng);</div><div class="line">    write_to_dnnl_memory(user_src.data(), user_src_memory);</div><div class="line">    <span class="keyword">auto</span> user_weights_memory</div><div class="line">            = memory({{conv1_weights_tz}, dt::f32, tag::oihw}, eng);</div><div class="line">    write_to_dnnl_memory(conv1_weights.data(), user_weights_memory);</div><div class="line">    <span class="keyword">auto</span> conv1_user_bias_memory</div><div class="line">            = memory({{conv1_bias_tz}, dt::f32, tag::x}, eng);</div><div class="line">    write_to_dnnl_memory(conv1_bias.data(), conv1_user_bias_memory);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create memory descriptors with layout tag::any. The <code>any</code> format enables the convolution primitive to choose the data format that will result in best performance based on its input parameters (convolution kernel sizes, strides, padding, and so on). If the resulting format is different from <code>nchw</code>, the user data must be transformed to the format required for the convolution (as explained below). </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv1_src_md = memory::desc({conv1_src_tz}, dt::f32, tag::any);</div><div class="line">    <span class="keyword">auto</span> conv1_bias_md = memory::desc({conv1_bias_tz}, dt::f32, tag::any);</div><div class="line">    <span class="keyword">auto</span> conv1_weights_md = memory::desc({conv1_weights_tz}, dt::f32, tag::any);</div><div class="line">    <span class="keyword">auto</span> conv1_dst_md = memory::desc({conv1_dst_tz}, dt::f32, tag::any);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create a convolution descriptor by specifying propagation kind, <a class="el" href="dev_guide_convolution.html">convolution algorithm</a>, shapes of input, weights, bias, output, convolution strides, padding, and kind of padding. Propagation kind is set to prop_kind::forward_inference to optimize for inference execution and omit computations that are necessary only for backward propagation. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv1_desc = convolution_forward::desc(prop_kind::forward_inference,</div><div class="line">            algorithm::convolution_direct, conv1_src_md, conv1_weights_md,</div><div class="line">            conv1_bias_md, conv1_dst_md, conv1_strides, conv1_padding,</div><div class="line">            conv1_padding);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create a convolution primitive descriptor. Once created, this descriptor has specific formats instead of the <code>any</code> format specified in the convolution descriptor. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv1_prim_desc = convolution_forward::primitive_desc(conv1_desc, eng);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
 Check whether data and weights formats required by convolution is different from the user format. In case it is different change the layout using reorder primitive. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv1_src_memory = user_src_memory;</div><div class="line">    <span class="keywordflow">if</span> (conv1_prim_desc.src_desc() != user_src_memory.get_desc()) {</div><div class="line">        conv1_src_memory = memory(conv1_prim_desc.src_desc(), eng);</div><div class="line">        net.push_back(reorder(user_src_memory, conv1_src_memory));</div><div class="line">        net_args.push_back({{<a class="code" href="group__dnnl__api__primitives__common.html#ga953b34f004a8222b04e21851487c611a">DNNL_ARG_FROM</a>, user_src_memory},</div><div class="line">                {<a class="code" href="group__dnnl__api__primitives__common.html#gaf700c3396987b450413c8df5d78bafd9">DNNL_ARG_TO</a>, conv1_src_memory}});</div><div class="line">    }</div><div class="line"></div><div class="line">    <span class="keyword">auto</span> conv1_weights_memory = user_weights_memory;</div><div class="line">    <span class="keywordflow">if</span> (conv1_prim_desc.weights_desc() != user_weights_memory.get_desc()) {</div><div class="line">        conv1_weights_memory = memory(conv1_prim_desc.weights_desc(), eng);</div><div class="line">        reorder(user_weights_memory, conv1_weights_memory)</div><div class="line">                .execute(s, user_weights_memory, conv1_weights_memory);</div><div class="line">    }</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
 Create a memory primitive for output. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv1_dst_memory = memory(conv1_prim_desc.dst_desc(), eng);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
 Create a convolution primitive and add it to the net. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv1_dst_memory = memory(conv1_prim_desc.dst_desc(), eng);</div></div><!-- fragment --><p><br />
<br />
<br />
 Create the relu primitive. For better performance, keep the input data format for ReLU (as well as for other operation primitives until another convolution or inner product is encountered) the same as the one chosen for convolution. Also note that ReLU is done in-place by using conv1 memory. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> relu1_desc = eltwise_forward::desc(prop_kind::forward_inference,</div><div class="line">            algorithm::eltwise_relu, conv1_dst_memory.get_desc(),</div><div class="line">            negative1_slope);</div><div class="line">    <span class="keyword">auto</span> relu1_prim_desc = eltwise_forward::primitive_desc(relu1_desc, eng);</div><div class="line"></div><div class="line">    net.push_back(eltwise_forward(relu1_prim_desc));</div><div class="line">    net_args.push_back({{<a class="code" href="group__dnnl__api__primitives__common.html#gac37ad67b48edeb9e742af0e50b70fe09">DNNL_ARG_SRC</a>, conv1_dst_memory},</div><div class="line">            {<a class="code" href="group__dnnl__api__primitives__common.html#ga3ca217e4a06d42a0ede3c018383c388f">DNNL_ARG_DST</a>, conv1_dst_memory}});</div></div><!-- fragment --><p><br />
<br />
 For training execution, pooling requires a private workspace memory to perform the backward pass. However, pooling should not use 'workspace' for inference, because this is detrimental to performance. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> pool1_desc = pooling_forward::desc(prop_kind::forward_inference,</div><div class="line">            algorithm::pooling_max, lrn1_dst_memory.get_desc(), pool1_dst_md,</div><div class="line">            pool1_strides, pool1_kernel, pool_padding, pool_padding);</div><div class="line">    <span class="keyword">auto</span> pool1_pd = pooling_forward::primitive_desc(pool1_desc, eng);</div><div class="line">    <span class="keyword">auto</span> pool1_dst_memory = memory(pool1_pd.dst_desc(), eng);</div><div class="line"></div><div class="line">    net.push_back(pooling_forward(pool1_pd));</div><div class="line">    net_args.push_back({{<a class="code" href="group__dnnl__api__primitives__common.html#gac37ad67b48edeb9e742af0e50b70fe09">DNNL_ARG_SRC</a>, lrn1_dst_memory},</div><div class="line">            {<a class="code" href="group__dnnl__api__primitives__common.html#ga3ca217e4a06d42a0ede3c018383c388f">DNNL_ARG_DST</a>, pool1_dst_memory}});</div></div><!-- fragment --><p> The example continues to create more layers according to the AlexNet topology. <br />
</p>
<p>Finally, execute the primitives. For this example, the net is executed multiple times and each execution is timed individually. </p><div class="fragment"><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> j = 0; j &lt; times; ++j) {</div><div class="line">        assert(net.size() == net_args.size() &amp;&amp; <span class="stringliteral">&quot;something is missing&quot;</span>);</div><div class="line">        <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; net.size(); ++i)</div><div class="line">            net.at(i).execute(s, net_args.at(i));</div><div class="line">    }</div></div><!-- fragment --></div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>