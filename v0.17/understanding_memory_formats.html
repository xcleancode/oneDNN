<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Understanding Memory Formats</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">0.17.4</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Understanding Memory Formats </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>Most of the computations are about data: analyzing data, adjusting data, reading and storing data, generating data... DNN domain is no exception. Images, weights/filters, sound, and text require efficient representation in computer memory to perform operations fast and in the most convenient way.</p>
<p>This article is devoted to <b>data format</b> &ndash; one form of data representation that describes how multidimensional arrays (nD) are stored in linear (1D) memory address space and why this is important for <a href="https://github.com/intel/mkl-dnn/">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)</a>.</p>
<blockquote class="doxtable">
<p>Note: for the purpose of this article data <em>format</em> and <em>layout</em> are used interchangeably. </p>
</blockquote>
<h3>Nomenclature used</h3>
<ul>
<li>Channels mean the same as feature maps</li>
<li>Upper-case letters denote the dimensions (e.g. <code>N</code>)</li>
<li>Lower-case letters denote the index (e.g. <code>n</code>, where <code>0 &lt;= n &lt; N</code>)</li>
<li><p class="startli">The notation for the activations:</p>
<p class="startli"><em>batch</em> <b>N</b>, <em>channels</em> <b>C</b>, <em>depth</em> <b>D</b>, <em>height</em> <b>H</b>, <em>width</em> <b>W</b></p>
</li>
<li><p class="startli">The notation for the weights:</p>
<p class="startli"><em>groups</em> <b>G</b>, <em>output channels</em> <b>O</b>, <em>input channels</em> <b>I</b>, <em>depth</em> <b>D</b>, <em>height</em> <b>H</b>, <em>width</em> <b>W</b></p>
</li>
</ul>
<h2>Data formats</h2>
<p>Let's first focus on data formats for activations (images).</p>
<p>Activations consist of channels (aka feature maps) and a spatial domain, 1D, 2D or 3D. Spatial domain together with channels form an image. During the training phase images are typically grouped together in batches. Even if there is only one image, we would still assume there is a batch with batch size equal to 1. Hence, the overall dimensionality of activations is 4D (<b>N</b>, <b>C</b>, <b>H</b>, and <b>W</b>) or 5D (<b>N</b>, <b>C</b>, <b>D</b>, <b>H</b>, and <b>W</b>).</p>
<p>In this article for the sake of simplicity we will use 2D spatial only.</p>
<h3>Plain data formats</h3>
<p>It would be simpler to start with an example.</p>
<p>Consider 4D activations with batch equals 2, 16 channels, and 5 x 4 spatial domain. Logical representation is given on the picture below. </p><div class="image">
<img src="mem_fmt_img1.png" alt="mem_fmt_img1.png"/>
<div class="caption">
Activations</div></div>
<p> The value at the position (n, c, h, w) is generated with the following formula: </p><div class="fragment"><div class="line">value(n, c, h, w) = n * CHW + c * HW + h * W + w</div></div><!-- fragment --><p>In order to define how data in this 4D-tensor is laid out in memory we need to define how to map it to a 1D tensor via an <b>offset</b> function that takes logical index (n, c, h, w) as an input and returns an address displacement to where the value is located: </p><div class="fragment"><div class="line">offset : (int, int, int, int) --&gt; int</div></div><!-- fragment --><h4>NCHW</h4>
<p>Let's describe the order in which the tensor values are laid out in memory for one the very popular format <b>NCHW</b>. The <code>[a:?]</code> marks refer to the jumps shown in the picture below that shows the 1D representation of an NCHW tensor in memory.</p>
<ul>
<li><code>[a:0]</code> First within a line, from left to right</li>
<li><code>[a:1]</code> Then line by line from top to bottom</li>
<li><code>[a:2]</code> Then go from one plane to another (in depth)</li>
<li><code>[a:3]</code> And finally switch from one image in a batch (<b>n</b> = 0) to another (<b>n</b> = 1)</li>
</ul>
<p>Then the offset function is: </p><div class="fragment"><div class="line">offset_nchw(n, c, h, w) = n * CHW + c * HW + h * W + w</div></div><!-- fragment --><p>We use <code>nchw</code> here to denote that <code>w</code> is the inner-most dimension, meaning that two elements adjacent in memory would share the same indices of <code>n</code>, <code>c</code>, and <code>h</code>, and their index of <code>w</code> would be different by <code>1</code>. This is of course true only for non-border elements. On the contrary <code>n</code> is the outermost dimension here, meaning that if you need to take the same pixel <code>(c, h, w)</code> but on the next image you have to jump over the whole image size <code>C*H*W</code>.</p>
<p>This data format is called <b>NCHW</b> and used by default in BVLC* Caffe. TensorFlow* also supports this data format.</p>
<blockquote class="doxtable">
<p>Note: It is just a coincidence that <code>offset_nchw()</code> is the same as <code>value()</code> in this example. </p>
</blockquote>
<p>One can create memory with <b>NCHW</b> data layout using <a class="el" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74aed4a51a8db128a648b942882369a06d6" title="4D data tensor with the physical layout nchw, used in Caffe. ">mkldnn_nchw</a> of the enum type <a class="el" href="group__c__api__types__generic.html#ga30a390f87ef74bf03dfa48a7b3f2fb74" title="Memory format specification. ">mkldnn_memory_format_t</a> defined in <a href="https://github.com/intel/mkl-dnn/blob/master/include/mkldnn_types.h">mkldnn_types.h</a> for C API and <a class="el" href="structmkldnn_1_1memory.html#a563b90355ae4fbfed12f6db8ab25a87eafc4fd623cc6ff8ed099be6d52743200a">mkldnn::memory::nchw</a> defined in <a href="https://github.com/intel/mkl-dnn/blob/master/include/mkldnn.hpp">mkldnn.hpp</a> for C++ API.</p>
<h4>NHWC</h4>
<p>Another quite popular data format is <b>NHWC</b> and it uses the following offset function: </p><div class="fragment"><div class="line">offset_nhwc(n, c, h, w) = n * HWC + h * WC + w * C + c</div></div><!-- fragment --><p>In this case the inner-most dimension is channels (<code>[b:0]</code>) that is followed by width (<code>[b:1]</code>), height (<code>[b:2]</code>), and finally batch (<code>[b:3]</code>).</p>
<p>For a single image (<b>N</b> = 1), this format is very similar to how <a href="https://en.wikipedia.org/wiki/BMP_file_format">BMP-file format</a> works, where the image is kept pixel by pixel and every pixel contains all required information about colors (for instance 3 channels for 24bit BMP).</p>
<p>NHWC data format is the default one for <a href="https://www.tensorflow.org/performance/performance_guide#data_formats">TensorFlow</a>.</p>
<p>This layout corresponds to <a class="el" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74adf1fb06f73cf8dd23ae620eea6516526" title="4D data tensor with the physical layout nhwc, used in TensorFlow. ">mkldnn_nhwc</a> or <a class="el" href="structmkldnn_1_1memory.html#a563b90355ae4fbfed12f6db8ab25a87ea5ec50071376483f7364de8b1b85d685d">mkldnn::memory::nhwc</a>.</p>
<h4>CHWN</h4>
<p>The last example here for the plain data layout is <b>CHWN</b> which is used by <a href="https://neon.nervanasys.com/index.html/design.html#data-layout">Neon</a>. This layout might be very interesting from a vectorization perspective if an appropriate batch size is used, but on the other hand users cannot always have <em>good</em> batch size (e.g. in case of real-time inference batch is typically 1).</p>
<p>The dimensions order is (from inner-most to outer-most): batch (<code>[c:0]</code>), width (<code>[c:1]</code>), height (<code>[c:2]</code>), channels (<code>[c:3]</code>).</p>
<p>The offset function for <b>CHWN</b> format is defined as: </p><div class="fragment"><div class="line">offset_chwn(n, c, h, w) = c * HWN + h * WN + w * N + n</div></div><!-- fragment --><p>This layout corresponds to <a class="el" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74a2d1cd3beca16be268720538840d68ec5" title="4D data tensor with the physical layout chwn, used in Neon. ">mkldnn_chwn</a> or <a class="el" href="structmkldnn_1_1memory.html#a563b90355ae4fbfed12f6db8ab25a87eaeab9b02510aacc27bd0043473caa6a8e">mkldnn::memory::chwn</a>.</p>
<div class="image">
<img src="mem_fmt_img2.png" alt="mem_fmt_img2.png"/>
<div class="caption">
Different plain layouts</div></div>
 <h4>Relevant reading</h4>
<p><a href="https://www.tensorflow.org/performance/xla/shapes">TensorFlow Doc. Shapes and Layout</a></p>
<h3>Generalization of the plain data layout</h3>
<h4>Strides</h4>
<p>In the previous examples the data was kept packed or in dense form, meaning pixels follow one another. Sometimes it might be necessary to not keep data contiguous in memory. For instance some might need to work with sub-tensor within a bigger tensor. Sometimes it might be beneficial to artificially make the data disjoint, like in case of GEMM with non-trivial leading dimension to get better performance (<a href="https://software.intel.com/en-us/articles/a-simple-example-to-measure-the-performance-of-an-intel-mkl-function">see Tips 6</a>).</p>
<p>The following picture shows simplified case for 2D matrix of size <code>rows x columns</code> kept in row-major format where rows have some non-trivial (i.e. not equal to the number of columns) stride.</p>
<div class="image">
<img src="strides.png" alt="strides.png"/>
<div class="caption">
Strides</div></div>
<p> In this case the general offset function looks like: </p><div class="fragment"><div class="line">offset(n, c, h, w) = n * stride_n</div><div class="line">                   + c * stride_c</div><div class="line">                   + h * stride_h</div><div class="line">                   + w * stride_w</div></div><!-- fragment --><p>Note, then <b>NCHW</b>, <b>NHWC</b>, and <b>CHWN</b> formats are just special cases of the format with strides. For example for <b>NCHW</b> we have: </p><div class="fragment"><div class="line">stride_n = CHW, stride_c = HW, stride_h = W, stride_w = 1</div></div><!-- fragment --><p>Intel MKL-DNN supports strides via blocking structure. The pseudo code is: </p><div class="fragment"><div class="line">memory_desc_t md; <span class="comment">// memory descriptor object</span></div><div class="line"></div><div class="line"><span class="comment">// logical description, layout independent</span></div><div class="line">md.ndims = 4;           <span class="comment">// # dimensions</span></div><div class="line">md.dims = {N, C, H, W}; <span class="comment">// dimensions themselves</span></div><div class="line"></div><div class="line"><span class="comment">// physical description</span></div><div class="line">md.memory_format = <a class="code" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74ae7e569e4a1924f15d37971d2f785646b">mkldnn_blocked</a>; <span class="comment">// generic blocked format</span></div><div class="line">md.layout_desc.blocking.strides[0] = {</div><div class="line">    stride_n, stride_c, stride_h, stride_w</div><div class="line">};</div></div><!-- fragment --><p> In particular whenever a user creates memory with <a class="el" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74aed4a51a8db128a648b942882369a06d6" title="4D data tensor with the physical layout nchw, used in Caffe. ">mkldnn_nchw</a> format MKL-DNN computes the strides and fills the structure on behalf of the user. That can be done manually though.</p>
<h2>Blocked layout</h2>
<p>Plain layouts give great flexibility and are very convenient for use. That's why most of the frameworks and applications use either <b>NCHW</b> or <b>NHWC</b> layouts. However depending on the operation that is performed on data it might turn out that those layouts are sub-optimal from performance perspective.</p>
<p>In order to achieve better vectorization and cache re-usage Intel MKL-DNN introduces blocked layout that splits one or several dimensions into the blocks of fixed size. The most popular Intel MKL-DNN data format is <b>nChw16c</b> on AVX512+ systems and <b>nChw8c</b> on SSE4.2+ systems. As one might guess from the name the only dimension that is blocked is channels and the block size is either 16 in the former case or 8 in the later case.</p>
<p>Precisely, the offset function for <b>nChw8c</b> is: </p><div class="fragment"><div class="line">offset_nChw8c(n, c, h, w) = n * CHW</div><div class="line">                          + (c / 8) * HW*8</div><div class="line">                          + h * W*8</div><div class="line">                          + w * 8</div><div class="line">                          + (c % 8)</div></div><!-- fragment --><p>Note that blocks of 8 channels are kept contiguously in memory. Pixel by pixel the spatial domain is covered. Then next slice covers subsequent 8 channels (i.e. moving from <code>c=0..7</code> to <code>c=8..15</code>). Once all channel blocks are covered the next image in the batch appears.</p>
<div class="image">
<img src="mem_fmt_blk.png" alt="mem_fmt_blk.png"/>
<div class="caption">
nChw8c format</div></div>
 <blockquote class="doxtable">
<p>Note: we use lower- and uppercase letters in the formats to distinguish between the blocks (e.g. 8c) and the remaining co-dimension (<b>C</b> = channels / 8). </p>
</blockquote>
<p>The reason behind the format choice can be found in <a href="https://arxiv.org/pdf/1602.06709v1.pdf">this paper</a>.</p>
<p>Intel MKL-DNN describes this type of memory via blocking structure as well. The pseudo code is: </p><div class="fragment"><div class="line">memory_desc_t md;</div><div class="line"><span class="comment">// logical description, layout independent</span></div><div class="line">md.ndims = 4;           <span class="comment">// # dimensions</span></div><div class="line">md.dims = {N, C, H, W}; <span class="comment">// dimensions themselves</span></div><div class="line"></div><div class="line"><span class="comment">// physical description</span></div><div class="line">md.memory_format = <a class="code" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74acf26dda829c10d1a318c955e1d185248">mkldnn_nChw8c</a>; <span class="comment">// blocked layout with</span></div><div class="line">                                  <span class="comment">// channels blocked by 8</span></div><div class="line">md.layout_desc.blocking.block_dims = {</div><div class="line">    1, <span class="comment">// no blocking by n, hence 1</span></div><div class="line">    8, <span class="comment">//    blocking by c, hence 8</span></div><div class="line">    1, <span class="comment">// no blocking by h, hence 1</span></div><div class="line">    1, <span class="comment">// no blocking by w, hence 1</span></div><div class="line">};</div><div class="line"></div><div class="line">ptrdiff_t stride_n = C*H*W;</div><div class="line">ptrdiff_t stride_C = H*W*8;</div><div class="line">ptrdiff_t stride_h =   W*8;</div><div class="line">ptrdiff_t stride_w =     8;</div><div class="line">ptrdiff_t stride_8c =    1;</div><div class="line"></div><div class="line">md.layout_desc.blocking.strides[0] = { <span class="comment">// strides between blocks</span></div><div class="line">    stride_n, stride_C, stride_h, stride_w</div><div class="line">};</div><div class="line">md.layout_desc.blocking.strides[1] = { <span class="comment">// strides within blocks</span></div><div class="line">    1,         <span class="comment">// ignored since no blocking by n</span></div><div class="line">    stride_8c, <span class="comment">// blocks of channels are contiguous</span></div><div class="line">    1,         <span class="comment">// ignored since no blocking by h</span></div><div class="line">    1,         <span class="comment">// ignored since no blocking by w</span></div><div class="line">};</div></div><!-- fragment --><h3>What if channels are not multiple of 8 (or 16)?</h3>
<p>Blocking data layout gives significant performance improvement for the convolutions, but what to do when the number of channels is not multiple of the block size, say 17 channels for <b>nChw8c</b> format?</p>
<p>Well one of the possible ways to handle that would be to use blocked layout for as many channels as possible by rounding them down to a number that is a multiple of the block size (in this case <code>16 = 17 / 8 * 8</code>) and process the tail somehow. However that would lead to introduction of very special tail processing code into many Intel MKL-DNN kernels.</p>
<p>So we came up with another solution using zero-padding. The idea is to round the channels up to make them multiples of the block size and pad created tail with zeros (in the example above <code>24 = div_up(17, 8) * 8</code>). Then primitives like convolutions might work with rounded-up number of channels instead of the original ones and compute the correct result (adding zeros doesn't change the result).</p>
<p>That allows supporting arbitrary number of channels with almost no changes to the kernels. The price would be some extra computations on those zeros, but this is either negligible or the performance with overheads is still higher than the performance with plain data layout.</p>
<p>The picture below depicts the idea. Note that some extra computations happen while computing <code>d0</code>, but that does not affect the result.</p>
<div class="image">
<img src="mem_fmt_padded_blk.png" alt="mem_fmt_padded_blk.png"/>
<div class="caption">
Padded format</div></div>
<p> The feature is supported starting with Intel MKL-DNN <a href="https://github.com/intel/mkl-dnn/releases/tag/v0.15">v0.15</a>. So far the support is limited by <code>f32</code> data type and on the AVX512+ systems. We plan to extend the implementations for other cases as well.</p>
<p>Some pitfalls of the given approach:</p>
<ul>
<li>To keep <em>padded data are zeros</em> invariant <a class="el" href="group__c__api__memory.html#ga6a67314dcf389e42757bf94f01086e9c" title="For a memory primitive, sets the data handle. ">mkldnn_memory_set_data_handle()</a> and <a class="el" href="structmkldnn_1_1memory.html#a9e2e6e828a867debc2be1508b31bce8d">mkldnn::memory::set_data_handle()</a> physically put zeros whenever user attaches a pointer to a memory that uses zero padding. That might affect performance if too many unnecessary calls to these functions are made. We might consider extending our API in future to allow attaching pointers without subsequent initialization with zeros, if user can guarantee the padding is already filled correctly</li>
<li>Memory size required to keep the data cannot be computed by the formula <code>sizeof(data_type) * N * C * H * W</code> anymore. Actual size should always be queried via <a class="el" href="group__c__api__memory.html#gaa024664d4ddd7a9ca295c692d2651648" title="Returns the size (in bytes) that is required for given memory_primitive_desc. ">mkldnn_memory_primitive_desc_get_size()</a> in C and <a class="el" href="structmkldnn_1_1memory_1_1primitive__desc.html#ae2d72838806aabc3dd865157631f7000" title="Returns the number of bytes required to allocate the memory described including the padding area...">mkldnn::memory::primitive_desc::get_size()</a> in C++</li>
<li>Element-wise operations that are implemented in user's code and directly operate on Intel MKL-DNN blocked layout like that: <div class="fragment"><div class="line">for (int e = 0; e &lt; phys_size; ++e)</div><div class="line">    x[e] = eltwise_op(x[e])</div></div><!-- fragment --> are not safe if the data is padded with zeros and the <code>eltwise_op(0) != 0</code></li>
</ul>
<p>Relevant Intel MKL-DNN code: </p><div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> C = 17;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> C_padded = div_up(17, 8) * 8; <span class="comment">// 24</span></div><div class="line"></div><div class="line"><span class="comment">// logical description, layout independent</span></div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> ndims    = 4;            <span class="comment">// # of dimensions</span></div><div class="line"><a class="code" href="group__c__api__types__memory.html#ga9acc437c5b1f7ca760c048dc2de1c44d">mkldnn_dims_t</a> dims = {N, C, H, W}; <span class="comment">// dimensions themselves</span></div><div class="line"></div><div class="line">memory_desc_t md;</div><div class="line"><span class="comment">// initialize memory descriptor</span></div><div class="line"><a class="code" href="group__c__api__memory.html#ga02cbee3c2c0142a1f18eaae5f2f3d890">mkldnn_memory_desc_init</a>(&amp;md, ndims,</div><div class="line">                             dims,</div><div class="line">                             <a class="code" href="group__c__api__types__generic.html#gga826b2a9be4d94ac17f99bacac6d0cb29a6d9faa3e054a4f7b7e5ab5b3d420684e">mkldnn_f32</a>,   <span class="comment">// single precision data type</span></div><div class="line">                             <a class="code" href="group__c__api__types__generic.html#gga30a390f87ef74bf03dfa48a7b3f2fb74acf26dda829c10d1a318c955e1d185248">mkldnn_nChw8c</a> <span class="comment">// blocked layout</span></div><div class="line">                             );</div><div class="line"></div><div class="line">ptrdiff_t expect_stride_n = C_padded*H*W;   <span class="comment">// note C_padded here, not C</span></div><div class="line">ptrdiff_t expect_stride_C =          H*W*8;</div><div class="line">ptrdiff_t expect_stride_h =            W*8;</div><div class="line">ptrdiff_t expect_stride_w =              8;</div><div class="line">ptrdiff_t expect_stride_8c =             1;</div><div class="line"></div><div class="line"><span class="keywordtype">bool</span> expect_true = <span class="keyword">true</span></div><div class="line">    &amp;&amp; <span class="keyword">true</span> <span class="comment">// logical dims stay as is</span></div><div class="line">    &amp;&amp; md.dims[0] == N</div><div class="line">    &amp;&amp; md.dims[1] == C</div><div class="line">    &amp;&amp; md.dims[2] == H</div><div class="line">    &amp;&amp; md.dims[3] == W</div><div class="line">    &amp;&amp; <span class="keyword">true</span> <span class="comment">// padded dims are rounded accordingly</span></div><div class="line">    &amp;&amp; md.layout_desc.blocking.padding_dims[0] == N</div><div class="line">    &amp;&amp; md.layout_desc.blocking.padding_dims[1] == C_padded</div><div class="line">    &amp;&amp; md.layout_desc.blocking.padding_dims[2] == H</div><div class="line">    &amp;&amp; md.layout_desc.blocking.padding_dims[3] == W</div><div class="line">    &amp;&amp; <span class="keyword">true</span> <span class="comment">// strides correspond to the physcal layout</span></div><div class="line">    &amp;&amp; md.layout_desc.blocking.strides[0][0] == expect_stride_n</div><div class="line">    &amp;&amp; md.layout_desc.blocking.strides[0][1] == expect_stride_C</div><div class="line">    &amp;&amp; md.layout_desc.blocking.strides[0][2] == expect_stride_h</div><div class="line">    &amp;&amp; md.layout_desc.blocking.strides[0][3] == expect_stride_w</div><div class="line">    &amp;&amp; md.layout_desc.blocking.strides[1][1] == expect_stride_8c;</div><div class="line"></div><div class="line">assert(expect_true);</div></div><!-- fragment --> <hr/>
<p><a class="el" href="legal_information.html">Legal information</a> </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
