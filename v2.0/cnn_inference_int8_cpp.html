<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN: CNN int8 inference example</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library (oneDNN)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library (oneDNN)</div>
    </a>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
   <div id="projectnumber">2.0.0</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('cnn_inference_int8_cpp.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">CNN int8 inference example </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This C++ API example demonstrates how to run AlexNet's conv3 and relu3 with int8 data type.</p>
<blockquote class="doxtable">
<p>Example code: <a class="el" href="cnn_inference_int8_8cpp-example.html">cnn_inference_int8.cpp</a> </p>
</blockquote>
<p>Configure tensor shapes </p><div class="fragment"><div class="line">    <span class="comment">// AlexNet: conv3</span></div><div class="line">    <span class="comment">// {batch, 256, 13, 13} (x)  {384, 256, 3, 3}; -&gt; {batch, 384, 13, 13}</span></div><div class="line">    <span class="comment">// strides: {1, 1}</span></div><div class="line">    memory::dims conv_src_tz = {batch, 256, 13, 13};</div><div class="line">    memory::dims conv_weights_tz = {384, 256, 3, 3};</div><div class="line">    memory::dims conv_bias_tz = {384};</div><div class="line">    memory::dims conv_dst_tz = {batch, 384, 13, 13};</div><div class="line">    memory::dims conv_strides = {1, 1};</div><div class="line">    memory::dims conv_padding = {1, 1};</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Next, the example configures the scales used to quantize f32 data into int8. For this example, the scaling value is chosen as an arbitrary number, although in a realistic scenario, it should be calculated from a set of precomputed values as previously mentioned. </p><div class="fragment"><div class="line">    <span class="comment">// Choose scaling factors for input, weight, output and bias quantization</span></div><div class="line">    <span class="keyword">const</span> std::vector&lt;float&gt; src_scales = {1.8f};</div><div class="line">    <span class="keyword">const</span> std::vector&lt;float&gt; weight_scales = {2.0f};</div><div class="line">    <span class="keyword">const</span> std::vector&lt;float&gt; bias_scales = {1.0f};</div><div class="line">    <span class="keyword">const</span> std::vector&lt;float&gt; dst_scales = {0.55f};</div><div class="line"></div><div class="line">    <span class="comment">// Choose channel-wise scaling factors for convolution</span></div><div class="line">    std::vector&lt;float&gt; conv_scales(384);</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> scales_half = 384 / 2;</div><div class="line">    std::fill(conv_scales.begin(), conv_scales.begin() + scales_half, 0.3f);</div><div class="line">    std::fill(conv_scales.begin() + scales_half + 1, conv_scales.end(), 0.8f);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 The <em>source, weights, bias</em> and <em>destination</em> datasets use the single-scale format with mask set to '0', while the <em>output</em> from the convolution (conv_scales) will use the array format where mask = 2 corresponding to the output dimension. </p><div class="fragment"><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> src_mask = 0;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> weight_mask = 0;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> bias_mask = 0;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> dst_mask = 0;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">int</span> conv_mask = 2; <span class="comment">// 1 &lt;&lt; output_channel_dim</span></div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create the memory primitives for user data (source, weights, and bias). The user data will be in its original 32-bit floating point format. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> user_src_memory = memory({{conv_src_tz}, dt::f32, tag::nchw}, eng);</div><div class="line">    write_to_dnnl_memory(user_src.data(), user_src_memory);</div><div class="line">    <span class="keyword">auto</span> user_weights_memory</div><div class="line">            = memory({{conv_weights_tz}, dt::f32, tag::oihw}, eng);</div><div class="line">    write_to_dnnl_memory(conv_weights.data(), user_weights_memory);</div><div class="line">    <span class="keyword">auto</span> user_bias_memory = memory({{conv_bias_tz}, dt::f32, tag::x}, eng);</div><div class="line">    write_to_dnnl_memory(conv_bias.data(), user_bias_memory);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create a memory descriptor for each convolution parameter. The convolution data uses 8-bit integer values, so the memory descriptors are configured as:</p>
<ul>
<li>8-bit unsigned (u8) for source and destination.</li>
<li>8-bit signed (s8) for bias and weights.</li>
</ul>
<blockquote class="doxtable">
<p><b>Note</b> The destination type is chosen as <em>unsigned</em> because the convolution applies a ReLU operation where data results \(\geq 0\). </p>
</blockquote>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_src_md = memory::desc({conv_src_tz}, dt::u8, tag::any);</div><div class="line">    <span class="keyword">auto</span> conv_bias_md = memory::desc({conv_bias_tz}, dt::s8, tag::any);</div><div class="line">    <span class="keyword">auto</span> conv_weights_md = memory::desc({conv_weights_tz}, dt::s8, tag::any);</div><div class="line">    <span class="keyword">auto</span> conv_dst_md = memory::desc({conv_dst_tz}, dt::u8, tag::any);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
<br />
 Create a convolution descriptor passing the int8 memory descriptors as parameters. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_desc = convolution_forward::desc(prop_kind::forward,</div><div class="line">            algorithm::convolution_direct, conv_src_md, conv_weights_md,</div><div class="line">            conv_bias_md, conv_dst_md, conv_strides, conv_padding,</div><div class="line">            conv_padding);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
<br />
 Configuring int8-specific parameters in an int8 primitive is done via the Attributes Primitive. Create an attributes object for the convolution and configure it accordingly. </p><div class="fragment"><div class="line">    primitive_attr conv_attr;</div><div class="line">    conv_attr.set_output_scales(conv_mask, conv_scales);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
<br />
 The ReLU layer from Alexnet is executed through the PostOps feature. Create a PostOps object and configure it to execute an <em>eltwise relu</em> operation. </p><div class="fragment"><div class="line">    <span class="keyword">const</span> <span class="keywordtype">float</span> ops_scale = 1.f;</div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">float</span> ops_alpha = 0.f; <span class="comment">// relu negative slope</span></div><div class="line">    <span class="keyword">const</span> <span class="keywordtype">float</span> ops_beta = 0.f;</div><div class="line">    post_ops ops;</div><div class="line">    ops.append_eltwise(ops_scale, algorithm::eltwise_relu, ops_alpha, ops_beta);</div><div class="line">    conv_attr.set_post_ops(ops);</div></div><!-- fragment --><p><br />
<br />
<br />
<br />
 Create a primitive descriptor using the convolution descriptor and passing along the int8 attributes in the constructor. The primitive descriptor for the convolution will contain the specific memory formats for the computation. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_prim_desc</div><div class="line">            = convolution_forward::primitive_desc(conv_desc, conv_attr, eng);</div></div><!-- fragment --><p><br />
<br />
<br />
 Create a memory for each of the convolution's data input parameters (source, bias, weights, and destination). Using the convolution primitive descriptor as the creation parameter enables oneDNN to configure the memory formats for the convolution.</p>
<p>Scaling parameters are passed to the reorder primitive via the attributes primitive.</p>
<p>User memory must be transformed into convolution-friendly memory (for int8 and memory format). A reorder layer performs the data transformation from f32 (the original user data) into int8 format (the data used for the convolution). In addition, the reorder transforms the user data into the required memory format (as explained in the simple_net example).</p>
<div class="fragment"><div class="line">    <span class="keyword">auto</span> conv_src_memory = memory(conv_prim_desc.src_desc(), eng);</div><div class="line">    primitive_attr src_attr;</div><div class="line">    src_attr.set_output_scales(src_mask, src_scales);</div><div class="line">    <span class="keyword">auto</span> src_reorder_pd</div><div class="line">            = reorder::primitive_desc(eng, user_src_memory.get_desc(), eng,</div><div class="line">                    conv_src_memory.get_desc(), src_attr);</div><div class="line">    <span class="keyword">auto</span> src_reorder = reorder(src_reorder_pd);</div><div class="line">    src_reorder.execute(s, user_src_memory, conv_src_memory);</div><div class="line"></div><div class="line">    <span class="keyword">auto</span> conv_weights_memory = memory(conv_prim_desc.weights_desc(), eng);</div><div class="line">    primitive_attr weight_attr;</div><div class="line">    weight_attr.set_output_scales(weight_mask, weight_scales);</div><div class="line">    <span class="keyword">auto</span> weight_reorder_pd</div><div class="line">            = reorder::primitive_desc(eng, user_weights_memory.get_desc(), eng,</div><div class="line">                    conv_weights_memory.get_desc(), weight_attr);</div><div class="line">    <span class="keyword">auto</span> weight_reorder = reorder(weight_reorder_pd);</div><div class="line">    weight_reorder.execute(s, user_weights_memory, conv_weights_memory);</div><div class="line"></div><div class="line">    <span class="keyword">auto</span> conv_bias_memory = memory(conv_prim_desc.bias_desc(), eng);</div><div class="line">    primitive_attr bias_attr;</div><div class="line">    bias_attr.set_output_scales(bias_mask, bias_scales);</div><div class="line">    <span class="keyword">auto</span> bias_reorder_pd</div><div class="line">            = reorder::primitive_desc(eng, user_bias_memory.get_desc(), eng,</div><div class="line">                    conv_bias_memory.get_desc(), bias_attr);</div><div class="line">    <span class="keyword">auto</span> bias_reorder = reorder(bias_reorder_pd);</div><div class="line">    bias_reorder.execute(s, user_bias_memory, conv_bias_memory);</div></div><!-- fragment --><p><br />
<br />
 Create the convolution primitive and add it to the net. The int8 example computes the same Convolution +ReLU layers from AlexNet simple-net.cpp using the int8 and PostOps approach. Although performance is not measured here, in practice it would require less computation time to achieve similar results. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> conv = convolution_forward(conv_prim_desc);</div><div class="line">    conv.execute(s,</div><div class="line">            {{<a class="code" href="group__dnnl__api__primitives__common.html#gac37ad67b48edeb9e742af0e50b70fe09">DNNL_ARG_SRC</a>, conv_src_memory},</div><div class="line">                    {<a class="code" href="group__dnnl__api__primitives__common.html#gaf279f28c59a807e71a70c719db56c5b3">DNNL_ARG_WEIGHTS</a>, conv_weights_memory},</div><div class="line">                    {<a class="code" href="group__dnnl__api__primitives__common.html#gad0cbc09942aba93fbe3c0c2e09166f0d">DNNL_ARG_BIAS</a>, conv_bias_memory},</div><div class="line">                    {<a class="code" href="group__dnnl__api__primitives__common.html#ga3ca217e4a06d42a0ede3c018383c388f">DNNL_ARG_DST</a>, conv_dst_memory}});</div></div><!-- fragment --><p><br />
</p>
<p>Finally, <em>dst memory</em> may be dequantized from int8 into the original f32 format. Create a memory primitive for the user data in the original 32-bit floating point format and then apply a reorder to transform the computation output data. </p><div class="fragment"><div class="line">    <span class="keyword">auto</span> user_dst_memory = memory({{conv_dst_tz}, dt::f32, tag::nchw}, eng);</div><div class="line">    write_to_dnnl_memory(user_dst.data(), user_dst_memory);</div><div class="line">    primitive_attr dst_attr;</div><div class="line">    dst_attr.set_output_scales(dst_mask, dst_scales);</div><div class="line">    <span class="keyword">auto</span> dst_reorder_pd</div><div class="line">            = reorder::primitive_desc(eng, conv_dst_memory.get_desc(), eng,</div><div class="line">                    user_dst_memory.get_desc(), dst_attr);</div><div class="line">    <span class="keyword">auto</span> dst_reorder = reorder(dst_reorder_pd);</div><div class="line">    dst_reorder.execute(s, conv_dst_memory, user_dst_memory);</div></div><!-- fragment --><p>[Dequantize the result] </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>