<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>oneDNN: Nuances of int8 Computations</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div class="mobile-nav"><i id="nav-btn"></i><a href="index.html">oneAPI Deep Neural Network Library (oneDNN)</a></div>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">
     <a href="index.html">
      <div id="full-name">oneAPI Deep Neural Network Library (oneDNN)</div>
    </a>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
   <div id="projectnumber">1.8.0</div>
  <div>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('dev_guide_int8_computations.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Nuances of int8 Computations </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p>This document uses <b>int8</b> to denote 8-bit integer no matter whether it is signed or unsigned. To emphasize the signedness of the data type <b>u8</b> (<code>uint8_t</code>) or <b>s8</b> (<code>int8_t</code>) are used. In particular, if a primitive has two inputs the types would be written using "/". For instance:</p><ul>
<li>int8 GEMM denotes any integer GEMM with 8-bit integer inputs, while</li>
<li>u8/s8 GEMM denotes <a class="el" href="group__dnnl__api__blas.html#gaef24848fd198d8a178d3ad95a78c1767" title="Performs integer matrix-matrix multiply on 8-bit unsigned matrix A, 8-bit signed matrix B...">dnnl_gemm_u8s8s32()</a> only. </li>
</ul>
</blockquote>
<p>The operation primitives that work with the int8 data type (<a class="el" href="structdnnl_1_1memory.html#a8e83474ec3a50e08e37af76c8c075dcea3e8d88fdd85d7153525e0647cdd97686" title="8-bit signed integer. ">dnnl::memory::data_type::s8</a> and <a class="el" href="structdnnl_1_1memory.html#a8e83474ec3a50e08e37af76c8c075dcea077393852be20e37026d6281827662f2" title="8-bit unsigned integer. ">dnnl::memory::data_type::u8</a>) typically use s32 (<code>int32_t</code>) as an intermediate data type (<a class="el" href="structdnnl_1_1memory.html#a8e83474ec3a50e08e37af76c8c075dceaa860868d23f3a68323a2e3f6563d7f31" title="32-bit signed integer. ">dnnl::memory::data_type::s32</a>) to avoid integer overflows.</p>
<p>For instance, the int8 average <a class="el" href="dev_guide_pooling.html">pooling</a> primitive accumulates the int8 input values in a window to an s32 accumulator, then divides the result by the window size, and then stores the result back to the int8 destination:</p>
<ul>
<li>\( \dst_{s8}(...) = (s8) \Biggl( \Biggl( \sum\limits_{kh,kw} (s32)\src_{s8}(...) \Biggr) \div (kh \cdot kw) \Biggr) \)</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>The max pooling primitive can directly work with int8 data types.</dd></dl>
<p>Using an s32 accumulator is especially important for matrix-multiply such as operation primitives that have chains of multiplication and accumulation of int8 values. These primitives are:</p><ul>
<li><a class="el" href="dev_guide_convolution.html">Convolution</a></li>
<li>Int8 GEMMs: <a class="el" href="group__dnnl__api__blas.html#ga2b763b7629846913507d88fba875cc26" title="Performs integer matrix-matrix multiply on 8-bit signed matrix A, 8-bit signed matrix B...">dnnl_gemm_s8s8s32()</a> and <a class="el" href="group__dnnl__api__blas.html#gaef24848fd198d8a178d3ad95a78c1767" title="Performs integer matrix-matrix multiply on 8-bit unsigned matrix A, 8-bit signed matrix B...">dnnl_gemm_u8s8s32()</a></li>
<li><a class="el" href="dev_guide_inner_product.html">Inner Product</a></li>
<li><a class="el" href="dev_guide_rnn.html">RNN</a> with LSTM or GRU cell functions</li>
</ul>
<p>Ideally, the semantics of these operations should be as follows:</p><ol type="1">
<li><b>Convert all inputs to s32 data type</b>.</li>
<li>Perform the operation using s32 data type.</li>
<li>(Optionally) <a class="el" href="dev_guide_attributes_post_ops.html">Post process</a> the data. This typically happens with additional data conversion to the f32 data type.</li>
<li>(Optionally) Down-convert the result to the destination data type.</li>
</ol>
<p>Depending on the hardware, the first step might vary slightly.</p>
<p>This document focuses on the first two steps (since the last two steps are independent from the hardware used), and describes the behaviors on different systems and the reasons behind them.</p>
<h2>CPU</h2>
<p><a class="anchor" id="dg_i8_comp_s11"></a></p><h3>1. Inputs of mixed type: u8 and s8</h3>
<p>Instruction Set Architecture (ISA) has special instructions that enable multiplying and adding the vectors of u8 and s8 very efficiently. oneDNN enables int8 support using these particular instructions.</p>
<p>Unfortunately, these instructions do not have the counterparts that work with vectors of the same type (either s8/s8 or u8/u8). The details for the s8/s8 case are covered in the <a class="el" href="dev_guide_int8_computations.html#dg_i8_comp_s12">2. Inputs of the same type: s8</a> section below.</p>
<h4>1.1. Processors with the Intel AVX2 or Intel AVX-512 Support</h4>
<p><em>System examples: Intel Xeon processor E7 v3 Family (formerly Haswell), Intel Xeon Scalable processor x1xx series (formerly Skylake).</em></p>
<p>oneDNN implements matrix multiplication such as operations with u8 and s8 operands on the Intel AVX2 and Intel AVX512 Instruction Set by using a sequence of <code>VPMADDUBSW, VPMADDWD, VPADDD</code> instructions <a class="el" href="dev_guide_int8_computations.html#dg_i8_ref_sdm">[1]</a>:</p>
<ol type="1">
<li><code>VPMADDUBSW</code> multiplies two pairs of u8/s8 values and accumulates the result into s16 (<code>int16_t</code>) with potential saturation.</li>
<li><code>VPMADDWD</code> sums the pairs of s16 values obtained above into s32. Computed sum is exact.</li>
<li><code>VPADDD</code> accumulates obtained s32 value to the accumulator.</li>
</ol>
<p>The pseudo-code for the sequence is shown below: </p><div class="fragment"><div class="line"><span class="comment">// Want to compute:</span></div><div class="line"><span class="comment">// c_s32 += sum{i=0..3}(a_u8[i] * b_s8[i])</span></div><div class="line">int32_t u8s8s32_compute_avx512(</div><div class="line">        uint8_t a_u8[4], int8_t b_s8[4], int32_t c_s32) {</div><div class="line"></div><div class="line">    <span class="comment">// Compute using VPMADDUBSW, VPMADDWD, VPADDD:</span></div><div class="line">    int16_t ab_s16[4];</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; 4; ++i)</div><div class="line">        ab_s16[i] = (int16_t)a_u8[i] * (int16_t)b_s8[i]; <span class="comment">// Exact computations</span></div><div class="line"></div><div class="line">    int16_t VPMADDUBSW_res[2];</div><div class="line">    VPMADDUBSW_res[0] = saturate_to_s16(ab_s16[0] + ab_s16[1]);  <span class="comment">// CAUTION: POTENTIAL OVERFLOW / SATURATION</span></div><div class="line">    VPMADDUBSW_res[1] = saturate_to_s16(ab_s16[2] + ab_s16[3]);  <span class="comment">// CAUTION: POTENTIAL OVERFLOW / SATURATION</span></div><div class="line"></div><div class="line">    c_s32 +=</div><div class="line">        (int32_t)VPMADDUBSW_res[0] +</div><div class="line">        (int32_t)VPMADDUBSW_res[1];</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> c_s32;</div><div class="line">}</div></div><!-- fragment --><p>Note the potential saturation happening at the first step (or the snippet that is marked with <code>CAUTION</code> in the pseudo-code). Consider the following example:</p>
<div class="fragment"><div class="line">uint8_t a_u8[4] = {255, 255, 0, 0};</div><div class="line">int8_t  b_s8[4] = {127, 127, 0, 0};</div><div class="line">int32_t c_s32 = 0;</div><div class="line"></div><div class="line">c_s32 = u8s8s32_compute(a_u8, b_s8, c_s32);</div><div class="line"><span class="comment">// c_s32 = 32767</span></div><div class="line"><span class="comment">//       = 0</span></div><div class="line"><span class="comment">//       + max(INT16_MIN, min(INT16_MAX, 255 * 127 + 255 * 127))</span></div><div class="line"><span class="comment">//       + max(INT16_MIN, min(INT16_MAX,   0 *   0 +   0 *   0));</span></div><div class="line"><span class="comment">// While one might expect 64770 = 255 * 127 + 255 * 127 + 0 * 0 + 0 * 0;</span></div></div><!-- fragment --><p>This is the major pitfall of using this sequence of instructions. As far as the precise result is concerned, one of the possible instruction sequences would be <code>VPMOVSXBW/VPMOVZXBW, VPMADDWD, VPADDD</code> <a class="el" href="dev_guide_int8_computations.html#dg_i8_ref_sdm">[1]</a>, where the first ones casts the s8/u8 values to s16. Unfortunately, using them would lead to 2x lower performance.</p>
<p>When one input is of type u8 and the other one is of type s8, oneDNN assumes that it is the user's responsibility to choose the quantization parameters so that no overflow/saturation occurs. For instance, a user can use u7 <code>[0, 127]</code> instead of u8 for the unsigned input, or s7 <code>[-64, 63]</code> instead of the s8 one. It is worth mentioning that this is required only when the Intel AVX2 or Intel AVX512 Instruction Set is used.</p>
<p>The <b>RNN</b> primitive behaves slightly differently than the convolution and inner product primitives, or u8/s8 GEMM. Even though its hidden state is represented by the u8 data type, the non-symmetric quantization is assumed. Namely, the formula is:</p><ul>
<li>\(data_{f32}[:] = \frac{1}{scale}(data_{u8}[:] - shift)\).</li>
</ul>
<p>But similarly to the other primitives, the RNN primitive does not handle potential overflows automatically. It is up to the user to specify the appropriate quantization parameters (see <a class="el" href="structdnnl_1_1primitive__attr.html#a39ce5aa8b06ed331d8e2158108cc8324" title="Sets quantization scale and shift parameters for RNN data tensors. ">dnnl::primitive_attr::set_rnn_data_qparams()</a> and <a class="el" href="structdnnl_1_1primitive__attr.html#a61bd70f97baa628fd49b2c8b334b913e" title="Sets quantization scaling factors for RNN weights tensors. ">dnnl::primitive_attr::set_rnn_weights_qparams()</a>). The recommended ones are:</p><ul>
<li>Data (hidden states) use <code>scale = 127</code>, and <code>shift = 128</code>.</li>
<li>Weights use <code>scale = 63 / W_max</code>, where <code>W_max</code> is \(\max | W_{f32}[:]| {}_{} \).</li>
</ul>
<h4>1.2. Processors with the Intel DL Boost Support</h4>
<p><em>System examples: Intel Xeon Scalable processor x2xx series (formerly Cascade Lake).</em></p>
<p>Intel DL Boost brings the <code>VPDPBUSD</code> instruction <a class="el" href="dev_guide_int8_computations.html#dg_i8_ref_isa_ext">[2]</a>, which enables computing the sum of four products of s8 and u8 values. This instruction performs same computations as the sequence of <code>VPMADDUBSW, VPMADDWD, VPADDD</code> instructions shown above, but with the major difference that the intermediate overflow and saturation cannot occur.</p>
<p>In other words, <code>VPDPBUSD</code> enables you to exactly compute: </p><div class="fragment"><div class="line"><span class="comment">// Want to compute:</span></div><div class="line"><span class="comment">// c_s32 += sum{i=0..3}(a_u8[i] * b_s8[i])</span></div><div class="line">int32_t u8s8s32_compute_avx512_dl_boost(</div><div class="line">        uint8_t a_u8[4], int8_t b_s8[4], int32_t c_s32) {</div><div class="line"></div><div class="line">    <span class="comment">// Compute using VPDPBUSD:</span></div><div class="line">    c_s32 +=</div><div class="line">        (int32_t)a_u8[0] * (int32_t)b_s8[0] +</div><div class="line">        (int32_t)a_u8[1] * (int32_t)b_s8[1] +</div><div class="line">        (int32_t)a_u8[2] * (int32_t)b_s8[2] +</div><div class="line">        (int32_t)a_u8[3] * (int32_t)b_s8[3];</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> c_s32;</div><div class="line">}</div></div><!-- fragment --><p>Since the instruction always computes the result accurately, no special tricks are required, and operations follow the semantics shown above.</p>
<p><a class="anchor" id="dg_i8_comp_s12"></a></p><h3>2. Inputs of the same type: s8</h3>
<p>As mentioned above, with the current instruction set it is impossible to multiply and add two vectors of the s8 data type as efficiently as it is for the mixed case. However, in real-world applications the inputs are typically signed.</p>
<p>To overcome this issue, oneDNN employs a trick: at run-time, it adds 128 to one of the s8 input to make it of type u8 instead. Once the result is computed, oneDNN subtracts the extra value it added by replacing the s8 with u8. This subtracted value sometimes referred as a <b>compensation</b>.</p>
<p>Conceptually the formula is: </p><p class="formulaDsp">
\[ Y_{s32} = X_{s8} \cdot W_{s8} = X&#39;_{u8} \cdot W_{s8} - 128 \cdot W_{s8}, \]
</p>
<p>where:</p><ul>
<li>\( X&#39;_{u8} = X_{s8} + 128 \), and</li>
<li>\( 128 \cdot W_{s8} {}_{} \) is a compensation.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>Since s8/s8 implementations are based on u8/s8 ones, the performance of the former might be slightly lower than the latter. The difference might vary depending on the problem sizes, hardware, and environment, but is expected to be in a range from 0% to 15% in most cases.</dd></dl>
<p>Since s8/s8 implementations are based on u8/s8 ones, they have the same potential issue with overflow/saturation when the Intel AVX2 or Intel AVX512 Instruction Set is used. The difference between the expected and actual results might be much greater though in this case. Consider the following example:</p>
<div class="fragment"><div class="line">int8_t  a_s8[4] = {127, 127, 0, 0};</div><div class="line">int8_t  b_s8[4] = {127, 127, 0, 0};</div><div class="line">int32_t c_s32 = 0;</div><div class="line"></div><div class="line"><span class="comment">// s8s8 the uses u8s8</span></div><div class="line"><span class="keyword">auto</span> s8s8s32_compute = [](int8_t a_s8[4], int8_t b_s8[4], int32_t c_s32) {</div><div class="line">    uint8_t a_u8[4] = { 128 + a_s8[0], ...};</div><div class="line">    c_s32 = u8s8s32_compute(a_u8, b_s8, c_s32);</div><div class="line"></div><div class="line">    <span class="comment">// apply the compensation</span></div><div class="line">    c_s32 +=</div><div class="line">        - 128 * b_s8[0]</div><div class="line">        - 128 * b_s8[1]</div><div class="line">        - 128 * b_s8[2]</div><div class="line">        - 128 * b_s8[3];</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> c_s32;</div><div class="line">};</div><div class="line"></div><div class="line">c_s32 = s8s8s32_compute(a_s8, b_s8, c_s32);</div><div class="line"><span class="comment">// c_s32 = 255</span></div><div class="line"><span class="comment">//       = 32767 - 128 * (127 + 127 + 0 + 0);</span></div><div class="line"><span class="comment">// While one might expect 32258 !!!</span></div></div><!-- fragment --><p>Note that processors with no support of the Intel AVX2 and Intel AVX512 Instruction Set or with support of the Intel DL Boost Instruction Set are not affected by these issues due to the reasons described in <a class="el" href="dev_guide_int8_computations.html#dg_i8_comp_s11">1. Inputs of mixed type: u8 and s8</a> section above.</p>
<p>Different primitives solve the potential overflow differently. The overview of the implementations are given below:</p>
<ol type="1">
<li><b>Convolution</b> primitive. The source is treated as <code>X_s8</code>, which would be shifted during the execution. The compensation is precomputed by a reorder during quantization of the weights, and embedded into them. Finally, when the Intel AVX2 or Intel AVX512 Instruction Set is used the reorder additionally scales the weights by 0.5 to overcome the potential overflow issue. During the convolution execution, the result would be re-scaled back. This rescaling introduces an error that might insignificantly affect the inference accuracy (compared to a platform with the Intel DL Boost Instruction Set).</li>
<li><p class="startli"><b>s8/s8 GEMM</b> (<a class="el" href="group__dnnl__api__blas.html#ga2b763b7629846913507d88fba875cc26" title="Performs integer matrix-matrix multiply on 8-bit signed matrix A, 8-bit signed matrix B...">dnnl_gemm_s8s8s32()</a>) does nothing to handle the overflow issue. It is up to the user to prepare the data so that the overflow/saturation does not occur. For instance, the user can specify s7 <code>[-64, 63]</code> instead of s8 for the second input.</p>
<dl class="section warning"><dt>Warning</dt><dd>It would not be enough to use s7 <code>[-64, 63]</code> for the first input. The only possible way to avoid overflow by shrinking the range of the first input would be to use the range <code>[-128, -1]</code>, which is most likely meaningless.</dd></dl>
</li>
<li>The <b>inner product</b> primitive directly calls s8/s8 GEMM, so it inherits the behavior of the latter. The user should consider using the appropriate scaling factors to avoid potential issues.</li>
<li>The <b>RNN</b> primitive does not support s8/s8 inputs.</li>
</ol>
<h2>GPU</h2>
<p>int8 data types are not supported on GPU (see <a class="el" href="dev_guide_data_types.html">Data Types</a>).</p>
<h2>References</h2>
<p><a class="anchor" id="dg_i8_ref_sdm"></a>[1] <a href="https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html">Intel(R) 64 and IA-32 Architectures Software Developer's Manual Combined Volumes 2A, 2B, 2C, and 2D: Instruction Set Reference, A-Z</a>. 325383-070US May 2019.</p>
<p><a class="anchor" id="dg_i8_ref_isa_ext"></a>[2] <a href="https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-and-future-features-programming-reference.html">Intel(R) Architecture Instruction Set Extensions and Future Features Programming Reference</a>. 319433-037 May 2019. <em>Chapter 2.1. VPDPBUSD — Multiply and Add Unsigned and Signed Bytes</em>.</p>
<p><a class="anchor" id="dg_i8_ref_wp"></a>[3] Rodriguez, Andres, et al. <a href="https://software.intel.com/content/www/us/en/develop/articles/lower-numerical-precision-deep-learning-inference-and-training">"Lower numerical precision deep learning inference and training."</a> Intel White Paper (2018). </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<div class="footer">
    <script>
        $('#top').prependTo($('#side-nav'));
    </script>
    <div class="footer-wrapper">
        <hr>
        <ul class="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>