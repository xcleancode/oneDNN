<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>DNNL: RNN</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Deep Neural Network Library (DNNL)
   &#160;<span id="projectnumber">1.1.3</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">RNN </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p></p>
<p>API reference: <a class="el" href="group__c__api__rnn.html">C</a>, <a class="el" href="group__cpp__api__rnn.html">C++</a></p>
<p></p>
</blockquote>
<p>The RNN primitive computes a stack of unrolled recurrent cells, as depicted in Figure 1. <code>bias</code>, <code>src_iter</code> and <code>dst_iter</code> are optional parameters. If not provided, <code>bias</code> and <code>src_iter</code> will default to 0.</p>
<p> <style>div.image img[src="unrolled_stack_rnn.jpg"]{width:80%;}</style><br />
</p><div class="image">
<img src="unrolled_stack_rnn.jpg" alt="unrolled_stack_rnn.jpg"/>
<div class="caption">
Figure 1: Example of stacked recurrent cells unrolled over the time dimension and executed with the left2right direction. Dashed lines represent optional parameters.</div></div>
<p> The RNN primitive supports four modes for evaluation direction:</p><ul>
<li>left2right will process the input data timestamps by increasing order</li>
<li>right2left will process the input data timestamps by decreasing order</li>
<li>bidirectional_concat will process all the stacked layers from left2right and from right2left independently, and will concatenate the output in dst_layer over the channel dimension.</li>
<li>bidirectional_sum will process all the stacked layers from left2right and from right2left independently, and will sum the two outputs to dst_layer.</li>
</ul>
<p>Even though the RNN primitive supports passing a different number of channels for <code>src_layer</code>, <code>src_iter</code>, <code>dst_layer</code>, and <code>dst_iter</code>, we always require the following conditions in order for the dimension to be consistent:</p><ul>
<li>\(channels(dst\_layer) = channels(dst\_iter)\),</li>
<li>when \(T &gt; 1\), \(channels(src\_iter) = channels(dst\_iter)\),</li>
<li>when \(L &gt; 1\), \(channels(src\_layer) = channels(dst\_layer)\),</li>
<li>when using the <code>bidirectional_concat</code> direction, \(channels(dst\_layer) = 2 * channels(dst\_iter)\).</li>
</ul>
<p>The general formula for the execution of a stack of unrolled recurrent cells depends on the current iteration of the previous layer ( \(h_{t,l-1}\) and \(c_{t,l-1}\)) and the previous iteration of the current layer ( \(h_{t-1, l}\)). Here is the exact equation for non-LSTM cells:</p>
<p class="formulaDsp">
\[ \begin{align} h_{t, l} = Cell(h_{t, l-1}, h_{t-1, l}) \end{align} \]
</p>
<p> where \(t,l\) are the indices of the timestamp and the layer of the cell being executed.</p>
<p>And here is the equation for LSTM cells:</p>
<p class="formulaDsp">
\[ \begin{equation*} (h_{t, l},c_{t,l}) = Cell(h_{t, l-1}, h_{t-1, l}, c_{t-1,l}) \end{equation*} \]
</p>
<p> where \(t,l\) are the indices of the timestamp and the layer of the cell being executed.</p>
<h1>Cell Functions</h1>
<p>The RNN API provides five cell functions:</p>
<ul>
<li><a href="#Vanilla-RNN">Vanilla RNN</a>, a single-gate recurrent cell,</li>
<li><a href="#LSTM">LSTM</a>, a four-gate long short-term memory cell,</li>
<li><a href="#GRU">GRU</a>, a three-gate gated recurrent unit cell,</li>
<li><a href="#Linear-before-reset-GRU">Linear-before-reset GRU</a>, a three-gate recurrent unit cell with the linear layer before the reset gate.</li>
</ul>
<h2>Vanilla RNN</h2>
<p>A single-gate recurrent cell initialized with <code>vanilla_rnn_forward::desc</code> or <code>vanilla_rnn_forward::desc</code> as in the following example. </p><div class="fragment"><div class="line"><span class="keyword">auto</span> vanilla_rnn_desc = vanilla_rnn_forward::desc(</div><div class="line">    aprop, activation, direction, src_layer_desc, src_iter_desc,</div><div class="line">    weights_layer_desc, weights_iter_desc, bias_desc,</div><div class="line">    dst_layer_desc, dst_iter_desc);</div></div><!-- fragment --><p>The Vanilla RNN cell supports the ReLU, Tanh and Sigmoid activation functions. The following equations defines the mathematical operation performed by the Vanilla RNN cell for the forward pass:</p>
<p class="formulaDsp">
\[ \begin{align} a_t &amp;= W \cdot h_{t,l-1} + U \cdot h_{t-1, l} + B \\ h_t &amp;= activation(a_t) \end{align} \]
</p>
<h2>LSTM</h2>
<p>A four-gate long short-term memory recurrent cell initialized with <code>lstm_forward::desc</code> or <code>lstm_backward::desc</code> as in the following example.</p>
<div class="fragment"><div class="line"><span class="keyword">auto</span> lstm_desc = lstm_forward::desc(</div><div class="line">    aprop, direction, src_layer_desc, src_iter_h_desc, src_iter_c_desc,</div><div class="line">    weights_layer_desc, weights_iter_desc, bias_desc, dst_layer_desc,</div><div class="line">    dst_iter_h_desc, dst_iter_c_desc);</div></div><!-- fragment --><p>Note that for all tensors with a dimension depending on the gates number, we implicitly require the order of these gates to be <code>i</code>, <code>f</code>, \(\tilde c\), and <code>o</code>. The following equation gives the mathematical description of these gates and output for the forward pass:</p>
<p class="formulaDsp">
\[ \begin{align} i_t &amp;= \sigma(W_i \cdot h_{t,l-1} + U_i \cdot h_{t-1, l} + B_i) \\ f_t &amp;= \sigma(W_f \cdot h_{t,l-1} + U_f \cdot h_{t-1, l} + B_f) \\ \tilde c_t &amp;= tanh(W_{\tilde c} \cdot h_{t,l-1} + U_{\tilde c} \cdot h_{t-1, l} + B_{\tilde c}) \\ o_t &amp;= \sigma(W_o \cdot h_{t,l-1} + U_o \cdot h_{t-1, l} + B_o) \\ \\ c_t &amp;= f_t * c_{t-1} + i_t * \tilde c_t \\ h_t &amp;= tanh(c_t) * o_t \end{align} \]
</p>
<p>where \(W_*\) are stored in <code>weights_layer</code>, \(U_*\) are stored in <code>weights_iter</code> and \(B_*\) are stored in <code>bias</code>.</p>
<dl class="section note"><dt>Note</dt><dd>In order for the dimensions to be consistent, we require \(channels(src\_iter\_c) = channels(dst\_iter\_c) = channels(dst\_iter)\).</dd></dl>
<h2>GRU</h2>
<p>A three-gate gated recurrent unit cell, initialized with <code>gru_forward::desc</code> or <code>gru_backward::desc</code> as in the following example. </p><div class="fragment"><div class="line"><span class="keyword">auto</span> gru_desc = gru_forward::desc(</div><div class="line">    aprop, direction, src_layer_desc, src_iter_desc,</div><div class="line">    weights_layer_desc, weights_iter_desc, bias_desc,</div><div class="line">    dst_layer_desc, dst_iter_desc);</div></div><!-- fragment --><p>Note that for all tensors with a dimension depending on the gates number, we implicitly require the order of these gates to be <code>u</code>, <code>r</code> and \(\tilde{c}\). The following equation gives the mathematical definition of these gates.</p>
<p class="formulaDsp">
\[ \begin{align} u_t &amp;= \sigma(W_u \cdot h_{t,l-1} + U_u \cdot h_{t-1, l} + B_u) \\ r_t &amp;= \sigma(W_r \cdot h_{t,l-1} + U_r \cdot h_{t-1, l} + B_r) \\ \tilde c_t &amp;= tanh(W_o \cdot h_{t,l-1} + U_o \cdot (r * h_{t-1, l}) + B_o) \\ h_t &amp;= u_t * h_{t-1, l} + (1 - u_t) * \tilde c_t \end{align} \]
</p>
<p>where \(W_*\) are in <code>weights_layer</code>, \(U_*\) are in <code>weights_iter</code>, and \(B_*\) are stored in <code>bias</code>.</p>
<dl class="section note"><dt>Note</dt><dd>If you need to replace u_t by (1-u_t) when computing h_t, you can achieve this by multiplying \(W_u\), \(U_u\) and \(B_u\) by \(-1\). This is possible as \(u_t = \sigma(W_u \cdot h_{t,l-1} + U_u \cdot h_{t-1, l} + B_u)\), and \(1 – \sigma(a) = \sigma(-a)\).</dd></dl>
<h2>Linear-Before-Reset GRU</h2>
<p>A three-gate gated recurrent unit cell with linear layer applied before the reset gate, initialized with or as in the following example. </p><div class="fragment"><div class="line"><span class="keyword">auto</span> lbr_gru_desc = lbr_gru_forward::desc(</div><div class="line">    aprop, direction, src_layer_desc, src_iter_desc,</div><div class="line">    weights_layer_desc, weights_iter_desc, bias_desc,</div><div class="line">    dst_layer_desc, dst_iter_desc);</div></div><!-- fragment --><p>The following equation describes the mathematical behavior of the Linear-Before-Reset GRU cell.</p>
<p class="formulaDsp">
\[ \begin{align} u &amp;= \sigma(W_u \cdot h_{t,l-1} + U_u \cdot h_{t-1, l} + B_u) \\ r &amp;= \sigma(W_r \cdot h_{t,l-1} + U_r \cdot h_{t-1, l} + B_r) \\ \tilde c &amp;= tanh(W_o \cdot h_{t,l-1} + r *(U_o \cdot h_{t-1, l} + B_{u&#39;}) + B_o) \\ h_t &amp;= u_t * h_{t-1, l} + (1 - u_t) * \tilde c \end{align} \]
</p>
<p>Note that for all tensors with a dimension depending on the gates number, except the bias, we implicitly require the order of these gates to be <code>u</code>, <code>r</code> and \(\tilde{c}\). For the <code>bias</code> tensor, we implicitly require the order of the gates to be <code>u</code>, <code>r</code>, \(\tilde{c}\) and &lsquo;u&rsquo;`.</p>
<dl class="section note"><dt>Note</dt><dd>If you need to replace u_t by (1-u_t) when computing h_t, you can achieve this by multiplying \(W_u\), \(U_u\) and \(B_u\) by \(-1\). This is possible as \(u_t = \sigma(W_u \cdot h_{t,l-1} + U_u \cdot h_{t-1, l} + B_u)\), and \(1 – \sigma(a) = \sigma(-a)\).</dd></dl>
<h1>Data Types</h1>
<p>The following table lists the combination of data types supported by the RNN primitive for each input and output memory object.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Propagation  </th><th class="markdownTableHeadNone">Input data  </th><th class="markdownTableHeadNone">Recurrent data  </th><th class="markdownTableHeadNone">Weights  </th><th class="markdownTableHeadNone">Bias  </th><th class="markdownTableHeadNone">Output Data   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Forward / Backward  </td><td class="markdownTableBodyNone">f32  </td><td class="markdownTableBodyNone">f32  </td><td class="markdownTableBodyNone">f32  </td><td class="markdownTableBodyNone">f32  </td><td class="markdownTableBodyNone">f32   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">Forward  </td><td class="markdownTableBodyNone">f16  </td><td class="markdownTableBodyNone">f16  </td><td class="markdownTableBodyNone">f16  </td><td class="markdownTableBodyNone">f16  </td><td class="markdownTableBodyNone">f16   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Forward inference  </td><td class="markdownTableBodyNone">u8  </td><td class="markdownTableBodyNone">u8  </td><td class="markdownTableBodyNone">s8  </td><td class="markdownTableBodyNone">f32  </td><td class="markdownTableBodyNone">u8, f32   </td></tr>
</table>
<dl class="section warning"><dt>Warning</dt><dd>There might be hardware and/or implementation specific restrictions. Check <a class="el" href="dev_guide_rnn.html#dg_rnn_impl_limits">Implementation Limitations</a> section below.</dd></dl>
<h1>Data and Weights Formats</h1>
<p>In the DNNL programming model, the RNN primitive is one of a few that support the placeholder memory format memory::format::any (shortened to <code>any</code> from now on) and can define data and weight memory objects format based on the primitive parameters.</p>
<p>The following table summarizes the data layouts supported by the RNN primitive.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Input/Output Data  </th><th class="markdownTableHeadNone">Recurrent Data  </th><th class="markdownTableHeadNone">Weights  </th><th class="markdownTableHeadNone">Bias   </th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyNone">any  </td><td class="markdownTableBodyNone">any  </td><td class="markdownTableBodyNone">any  </td><td class="markdownTableBodyNone">ldgo   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyNone">ntc, tnc  </td><td class="markdownTableBodyNone">ldnc  </td><td class="markdownTableBodyNone">ldigo, ldgoi  </td><td class="markdownTableBodyNone">ldgo   </td></tr>
</table>
<p>While an RNN primitive can be created with memory formats specified explicitly, the performance is likely to be sub-optimal. When using <code>any</code> it is necessary to first create an RNN primitive descriptor and then query it for the actual data and weight memory objects formats.</p>
<dl class="section note"><dt>Note</dt><dd>The RNN primitive supports padded tensors and views. So even if two memory descriptors share the same data layout, they might still be different.</dd></dl>
<h1>Considerations for Training</h1>
<p>When using the RNN API for training, the forward pass should use the <code>forward_training</code> propagation kind, and a workspace should be passed to both the forward pass and the backward pass. Note that after executing the backward pass, the workspace is no more valid and should be populated once again by another forward pass.</p>
<p><a class="anchor" id="dg_rnn_impl_limits"></a></p><h1>Implementation Limitations</h1>
<ol type="1">
<li>Refer to <a class="el" href="dev_guide_data_types.html">Data Types</a> for limitations related to data types support.</li>
<li><b>GPU</b><ul>
<li>No support for GRU </li>
</ul>
</li>
</ol>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>