<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>DNNL: Int8 Inference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Deep Neural Network Library (DNNL)
   &#160;<span id="projectnumber">1.1.3</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Int8 Inference </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Introduction</h2>
<p>To push higher performance during inference computations, recent work has focused on computing at a lower precision (that is, shrinking the size of data for activations and weights) to achieve higher throughput. Eight-bit computations (referred to as int8) offer improved performance over higher-precision types because they enable packing more data into a single instruction, at the cost of reduced (but acceptable) accuracy.</p>
<h2>Int8 Workflow</h2>
<p>There are different ways to use lower precision to perform inference. Please go through the <a class="el" href="dev_guide_attributes_quantization.html">Primitive Attributes: Quantization</a> page to get the initial understanding of what kind of quantization model DNNL supports.</p>
<h3>Quantization Process</h3>
<p>To operate with int8 data types from a higher-precision format (for example, 32-bit floating point), data must first be <em>quantized</em>. The quantization process converts a given input into a lower-precision format. The precision and accuracy factors are determined by the scaling factors.</p>
<h3>Scale</h3>
<p>The scale is usually obtained from sampling the dataset of previous executions in the original format (for example, the activations and weights from training in fp32) and is formulated as:</p>
<ul>
<li>\( R_{\{\alpha,w\}} = max(abs(T_{\{\alpha,w\}}))\)</li>
</ul>
<p>where \(T_{\{\alpha,w\}} {}_{}\) is a tensor corresponding to either the weights \(w\) or the activations \(\alpha\).</p>
<p>The purpose is to establish the range of values used in the computation, where selecting a proper scaling factor prevents over- or underflows during computation of the lower-precision results.</p>
<h3>Quantization Factor</h3>
<p>The next step is to calculate the <b>quantization factor</b> for converting the values into the corresponding int8 range. This is also known as the <b>scale</b> or <b>scaling factor</b> applied to the original high-precision values and is calculated as:</p>
<ul>
<li>\( Q_{\alpha} = \frac{255}{R_{\alpha}}\) is the quantization factor for activations with non-negative values.</li>
<li>\( Q_{w} = \frac{127}{R_{w}}\) is the quantization factor for weights.</li>
</ul>
<p>The low-precision values, known as the <b>quantized</b> activation, weights, and bias values, are calculated as:</p>
<ul>
<li>\(\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil \in [0,255]\)</li>
<li>\(W_{s8} = \lceil Q_{w} W_{f32} \rceil \in [-127,127]\)</li>
<li>\(b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil \in [-2^{31},2^{31}-1]\)</li>
</ul>
<p>where the function \( \lceil \rceil \) rounds to the selected rounding mode (typically determined by the MXCSR register; the default value is RoundNearestEven).</p>
<p>When the destination value (for example, from a convolution) is stored as a signed 32-bit integer, the result is bound to the same quantization <em>scaling</em> factors:</p>
<ul>
<li>\(X_{s32} = W_{s8} \times \alpha{u8} + b_{s32} \approx Q_{\alpha} Q_{\omega} X_{f32}\)</li>
<li>where \(X_{f32} = W_{f32} \times \alpha_{f32} + b_{f32}\)</li>
</ul>
<p>where the approximated value is due to the rounded values.</p>
<p>Inversely, the dequantized value is calculated as:</p>
<ul>
<li>\(X_{f32} \approx \frac{1}{Q_{\alpha} Q_{\omega}} X_{s32} \)</li>
</ul>
<h3>Quantization Example</h3>
<p>To show how the int8 parameters are obtained, suppose we first start off with a set of arbitrary high-precision input and output values. These values come from sampling a previously executed training run and are in their original 32-bit floating point format as:</p>
<ul>
<li>activations: \( T_{\alpha} = [15, 14, 15 ... 8, 11 ]\) where \( max(abs(T_{\alpha})) = 15\)</li>
<li>weights: \( T_{\omega} = [-5.1 , 6.8, ... -1.2, 9.8 ]\) where \( max(abs(T_{\omega})) = 9.8\)</li>
<li>bias: \( T_{\alpha} = [ 2.4, -5.2 ... -8 ]\) where \( max(abs(T_{\alpha})) = 8\)</li>
</ul>
<p>The scaling factors are:</p>
<ul>
<li>\( Q_{\alpha} = \frac{255}{R_{\alpha}} = \frac{255}{15} = 17 \)</li>
<li>\( Q_{w} = \frac{127}{R_{w}} = \frac{127}{9.8} = 12.96\)</li>
</ul>
<p>Finally, the quantized input values for the 8-bit operation are calculated as:</p>
<ul>
<li>\(\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil\) \( = \lceil 17 \times [15, 14, ... 11 ] \rceil = [255, 238, ... 187] \)</li>
<li>\(W_{s8} = \lceil Q_{w} W_{f32} \rceil = \lceil 12.96 \times [-5.1 , 6.8, ... -1.2, 9.8 ] \rceil = [-66, 88, ... -15, 127] \)</li>
<li>\(b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil = \lceil 17 \times 12.96 \times [ 2.4, -5.2 ... -8 ] \rceil = [528, -1145, ... -1762] \)</li>
</ul>
<p>These arrays are the new inputs for the int8 net.</p>
<h2>DNNL Support for Low-Precision int8 Primitives</h2>
<p>DNNL supports low-precision computations for inference through the int8 primitives. int8 primitives are ordinary DNNL primitives that have their input and output parameters configured to 8-bit types. int8 primitives are optimized for high performance on the compatible hardware (see <a class="el" href="dev_guide_data_types.html">Data Types</a>).</p>
<h3>DNNL Attributes</h3>
<p>DNNL primitive behavior may be extended for additional functionalities involving output data transformation. These additional features are configured via <b>primitive attributes</b>. The primitive attributes definition is an opaque structure for passing extra parameters to a primitive descriptor. These parameters include a scaling factor and fused post-ops. All operation primitives support the attributes structure; however, some configurations are not implemented and result in <em>failed primitive creation</em>.</p>
<p>The <b>scaling factor</b>, as previously described, is known prior to the inference operation where the values are calculated from a set of formulas. In DNNL, the scaling factor is applied to the output of a primitive. Moreover, to perform input transformations (for example, source, bias, and weights), DNNL performs quantizing and dequantizing of data for int8 through the <b>Reorder Primitive</b>.</p>
<p>DNNL has two formats for defining the output scaling factor. Depending on the configuration set by the scaling mask, either the output is scaled uniformly across all the dimensions (<em>mask = 0</em>) or a set of scaling values is applied to specific dimensions, as explained below:</p>
<ul>
<li>A <em>single floating point value</em> shared across the tensor <div class="image">
<img src="img_singlescalar.png" alt="img_singlescalar.png"/>
<div class="caption">
Single-value scaling format</div></div>
</li>
<li>An array of floating point values each corresponding to a specific output channel <div class="image">
<img src="img_multiscalar.png" alt="img_multiscalar.png"/>
<div class="caption">
Multi-value scaling format</div></div>
 The <b>mask</b> parameter determines the dimension to which the scales array is applied, where the i<sup>th</sup>-bit(s) of mask selects the dimension or dimensions d<sub>i</sub> (where <em>d</em> is an <em>n</em>-dimensional output tensor with logical dimensions as [<em>d0, d1, ..., dn-1</em>]). For example:</li>
<li>The single-scale format always has mask = 0.</li>
<li>For a 5-dimensional tensor T[g0, o1,i2,h3,w4] where the numbering indicates the bit-index:<ul>
<li>A mask = 2 = 2<sup>1</sup> selects the output channel for scaling.</li>
<li>A mask = 3 = 2<sup>0</sup> | 2<sup>1</sup> selects the group and output channels.</li>
</ul>
</li>
</ul>
<p>Mask is always applied to the logical dimension; this is independent of the dimension format that the primitive might select. The dimensions in DNNL are defined as follows:</p><ul>
<li>2D dimensional data the order of dimensions is always: (n, c)</li>
<li>4D dimensional data the order is always: (n, c, h, w)</li>
<li>5D dimensional weights the order is always: (g, oc, ic, kh, kw)</li>
</ul>
<p>Fused <b>post-ops</b> allow chaining operations during the primitive computation. Note that the resulting output value from post-ops is always affected by the scaling factor. The supported operations are:</p>
<ul>
<li>Accumulation where the primitive sums the resulting values from previously computed activations as:<ul>
<li>\(dst[ ] \leftarrow scale * dst[] + op(...)\), instead of</li>
<li>\(dst[ ] \leftarrow op(...)\)</li>
</ul>
</li>
<li>Element-wise (eltwise) operation with kind, alpha and beta parameters as:<ul>
<li>\(dst[ ] \leftarrow scale * eltwise\_op ( op(...) )\), instead of</li>
<li>\(dst[ ] \leftarrow op(...)\)</li>
</ul>
</li>
</ul>
<p>The list of supported eltwise operations for int8 is currently limited to ReLU. For instance, post-ops may only configure a convolution with accumulation followed by eltwise (relu).</p>
<h2>Example</h2>
<p><a class="el" href="cnn_inference_int8_cpp.html">CNN int8 inference example</a> example walks through the steps of int8 inference. </p>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>