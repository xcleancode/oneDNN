<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>DNNL: Primitive Attributes: Post-ops</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">Deep Neural Network Library (DNNL)
   &#160;<span id="projectnumber">1.1.3</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Primitive Attributes: Post-ops </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>DNNL implements some basic capabilities of operation fusion using the <b>post-ops attributes</b> API. The operation fusion typically reduces the memory bandwidth pressure hence leading to higher performance.</p>
<p>The post-ops change the default behavior of a primitive and hence are implemented through the <a class="el" href="dev_guide_attributes.html">Primitive Attributes</a> mechanism.</p>
<p>Currently the following post-ops are supported by the library:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Post-ops \ Primitive  </th><th class="markdownTableHeadLeft"><a class="el" href="dev_guide_convolution.html">Convolution</a>  </th><th class="markdownTableHeadLeft"><a class="el" href="dev_guide_inner_product.html">Inner Product</a>  </th><th class="markdownTableHeadLeft"></th></tr>
<tr class="markdownTableBody" class="markdownTableRowOdd">
<td class="markdownTableBodyLeft"><a class="el" href="dev_guide_attributes_post_ops.html#dev_guide_attributes_post_ops_eltwise">Eltwise</a>  </td><td class="markdownTableBodyLeft">Partial  </td><td class="markdownTableBodyLeft">Partial  </td><td class="markdownTableBodyLeft">Partial   </td></tr>
<tr class="markdownTableBody" class="markdownTableRowEven">
<td class="markdownTableBodyLeft"><a class="el" href="dev_guide_attributes_post_ops.html#dev_guide_attributes_post_ops_sum">Sum</a>  </td><td class="markdownTableBodyLeft">Partial  </td><td class="markdownTableBodyLeft">N/A  </td><td class="markdownTableBodyLeft">N/A   </td></tr>
</table>
<p>Just like <a class="el" href="dev_guide_attributes.html">Primitive Attributes</a>, the post-ops are represented by an opaque structure (<a class="el" href="group__c__api__primitive__attr.html#ga7d715ce1a81606584df9dcf045976401">dnnl_post_ops_t</a> in C API and <a class="el" href="structdnnl_1_1post__ops.html">dnnl::post_ops</a> in C++ API) which is copied once it is attached to the attributes using C++ <a class="el" href="structdnnl_1_1primitive__attr.html#a7bf5358e35bf0e99019db45ae8fad0b5">dnnl::primitive_attr::set_post_ops</a> or C <a class="el" href="group__c__api__attributes.html#ga7045d42606599f156bfca69820c21ea2">dnnl_primitive_attr_set_post_ops</a> functions. These attributes then are passed to a primitive descriptor creation function to take effect. Below is a simple skeleton for C++ API:</p>
<div class="fragment"><div class="line"><a class="code" href="structdnnl_1_1post__ops.html">dnnl::post_ops</a> po; <span class="comment">// default empty post-ops</span></div><div class="line">assert(po.<a class="code" href="structdnnl_1_1post__ops.html#a84653b68d83c2d84d3ac432a8dc1f5fd">len</a>() == 0); <span class="comment">// no post-ops attached</span></div><div class="line"></div><div class="line">po.append_SOMETHING(params); <span class="comment">// append some particular post-op</span></div><div class="line">po.append_SOMETHING_ELSE(other_params); <span class="comment">// append one more post-op</span></div><div class="line"></div><div class="line"><span class="comment">// (!) Note that the order of appending matters!</span></div><div class="line">assert(po.<a class="code" href="structdnnl_1_1post__ops.html#a84653b68d83c2d84d3ac432a8dc1f5fd">len</a>() == 2);</div><div class="line"></div><div class="line"><a class="code" href="structdnnl_1_1primitive__attr.html">dnnl::primitive_attr</a> attr; <span class="comment">// default attributes</span></div><div class="line">attr.<a class="code" href="structdnnl_1_1primitive__attr.html#a7bf5358e35bf0e99019db45ae8fad0b5">set_post_ops</a>(po); <span class="comment">// attach the post-ops to the attr</span></div><div class="line"></div><div class="line"><span class="comment">// further po changes would not affect attr</span></div><div class="line"></div><div class="line">primitive::primitive_desc op_pd(params, attr); <span class="comment">// create a pd with the attr</span></div></div><!-- fragment --><dl class="section note"><dt>Note</dt><dd>Different post-ops can be chained together by appending one after another. Note that the appending order matters: the sequence of the post-ops is executed in the order of appearance.</dd></dl>
<dl class="section warning"><dt>Warning</dt><dd>Different primitives have different capabilities on supporting post-ops. Moreover, the support might also depend on the actual implementation of a primitive. For instance, the library generally doesn't support post-ops for reference primitives (which are typically very slow, so there is no point in doing the actual fusion). So the robust integration should handle errors accordingly. See the <a class="el" href="dev_guide_attributes.html#dev_guide_attributes_error_handling">section on attributes error handling</a>.</dd></dl>
<p>The post-op object can be inspected by <a class="el" href="structdnnl_1_1post__ops.html#a454acad1a18f2763f07b42912778c0f8">dnnl::post_ops::kind()</a> function that takes an index of the post-op (that must be less than the value returned by <a class="el" href="structdnnl_1_1post__ops.html#a84653b68d83c2d84d3ac432a8dc1f5fd">dnnl::post_ops::len()</a>) and returns it's kind.</p>
<h2>Supported Post-ops</h2>
<p><a class="anchor" id="dev_guide_attributes_post_ops_eltwise"></a></p><h3>Eltwise Post-op</h3>
<p>The eltwise post-op enables fusing a primitive with a <a class="el" href="dev_guide_eltwise.html">Eltwise</a> primitive. This is probably one of the most popular kinds of fusion: an eltwise (typically an activation function) with preceding convolution or inner product.</p>
<p>The <a class="el" href="classdnnl_1_1primitive.html#ad1ec93215a0cf3aa0a32bae0c2cd9169">dnnl::primitive::kind</a> of this post-op is <a class="el" href="classdnnl_1_1primitive.html#ad1ec93215a0cf3aa0a32bae0c2cd9169a98b908c7d0339bb6a4832db44fc2c8da" title="An element-wise primitive. ">dnnl::primitive::kind::eltwise</a>.</p>
<p>API:</p><ul>
<li>C: <a class="el" href="group__c__api__attributes__post__ops.html#ga3f4e1e6923df257def1b8b38113bced5">dnnl_post_ops_append_eltwise</a></li>
<li>C++: <a class="el" href="structdnnl_1_1post__ops.html#a9cc93664cc11d2cd4c900f172be5703f">dnnl::post_ops::append_eltwise</a></li>
</ul>
<p>The parameters (C++ API for simplicity): </p><div class="fragment"><div class="line"><span class="keywordtype">void</span> <a class="code" href="structdnnl_1_1post__ops.html#a9cc93664cc11d2cd4c900f172be5703f">dnnl::post_ops::append_eltwise</a>(</div><div class="line">        <span class="keywordtype">float</span> scale, <span class="comment">// scaling factor (described below)</span></div><div class="line">        <a class="code" href="group__cpp__api__enums.html#ga00377dd4982333e42e8ae1d09a309640">algorithm</a> alg, <span class="keywordtype">float</span> alpha, <span class="keywordtype">float</span> beta <span class="comment">// same as in Eltwise primitive</span></div><div class="line">        );</div></div><!-- fragment --><p>The <code>alg</code>, <code>alpha</code>, and <code>beta</code> parameters are the same as in <a class="el" href="dev_guide_eltwise.html">Eltwise</a>.</p>
<p>The Eltwise post-op replaces: </p><p class="formulaDsp">
\[ dst(:) = Op(...) \]
</p>
<p>with</p>
<p class="formulaDsp">
\[ dst(:) = scale \cdot Eltwise( Op(...) ) \]
</p>
<p>The intermediate result of \(Op(...)\) is not stored. Hence in most of the case this kind of fusion cannot be used with the training.</p>
<p>The \(scale\) factor is supported in <a class="el" href="dev_guide_attributes_quantization.html">INT8</a> inference only. For other cases the scale must be equal to <code>1.0</code>.</p>
<p><a class="anchor" id="dev_guide_attributes_post_ops_sum"></a></p><h3>Sum Post-op</h3>
<p>Appends an accumulation (sum) post-op. Prior to accumulating the result, the previous value would be multiplied by scale.</p>
<p>The kind of this post-op is <a class="el" href="classdnnl_1_1primitive.html#ad1ec93215a0cf3aa0a32bae0c2cd9169a1d623b89683f9ce4e074de1676d12416" title="A sum primitive. ">dnnl::primitive::kind::sum</a>.</p>
<p>This feature might improve performance for cases like residual learning blocks, where the result of a convolution is accumulated to the previously computed activations. The scale parameter can be used in <a class="el" href="dev_guide_attributes_quantization.html">INT8</a> inference only when the result and previous activations have different logical scaling factors.</p>
<p>The sum post-op replaces </p><p class="formulaDsp">
\[ dst(:) = Op(...) \]
</p>
<p>with</p>
<p class="formulaDsp">
\[ dst(:) = scale \cdot dst(:) + Op(...) \]
</p>
<dl class="section warning"><dt>Warning</dt><dd>This post-op (as well as all the others) disregards the original layout of the destination; that is, the layout of the original destination is expected to be the same as the layout of the output destination.</dd></dl>
<h2>Examples of Chained Post-ops</h2>
<p>Different post-ops can be chained together by appending one after another. Note that the order matters: the post-ops are executed in the order they have been appended.</p>
<p>Let's consider some examples.</p>
<h3>Sum -&gt; ReLU</h3>
<p>This pattern is pretty common for the CNN topologies from the ResNet family.</p>
<div class="fragment"><div class="line"><a class="code" href="structdnnl_1_1post__ops.html">dnnl::post_ops</a> po;</div><div class="line">po.<a class="code" href="structdnnl_1_1post__ops.html#a078ab8ec15423d2b3d26f3619a78ca38">append_sum</a>(</div><div class="line">        <span class="comment">/* scale = */</span> 1.f);</div><div class="line">po.<a class="code" href="structdnnl_1_1post__ops.html#a9cc93664cc11d2cd4c900f172be5703f">append_eltwise</a>(</div><div class="line">        <span class="comment">/* scale     = */</span> 1.f</div><div class="line">        <span class="comment">/* alg kind  = */</span> <a class="code" href="group__cpp__api__enums.html#gga00377dd4982333e42e8ae1d09a309640aba09bebb742494255b90b43871c01c69">dnnl::algorithm::eltwise_relu</a>,</div><div class="line">        <span class="comment">/* neg slope = */</span> 0.f,</div><div class="line">        <span class="comment">/* unused for relu */</span> 0.f);</div><div class="line"></div><div class="line"><a class="code" href="structdnnl_1_1primitive__attr.html">dnnl::primitive_attr</a> attr;</div><div class="line">attr.<a class="code" href="structdnnl_1_1primitive__attr.html#a7bf5358e35bf0e99019db45ae8fad0b5">set_post_ops</a>(po);</div><div class="line"></div><div class="line">convolution_forward::primitive_desc(conv_d, attr, engine);</div></div><!-- fragment --><p>This will lead to the following primitive behavior:</p>
<p class="formulaDsp">
\[ dst(:) = ReLU(dst(:) + conv(src(:), weights(:)) \]
</p>
<p><a class="anchor" id="dev_guide_attributes_post_ops_with_scales"></a></p><h3>Tanh -&gt; Sum -&gt; ScaleShift</h3>
<p>The hypothetical example to illustrate the sequence of operations applied. We also set all the scales to non-one to as well as use <a class="el" href="structdnnl_1_1primitive__attr.html#a4b81acc8e48886313154f75c1708ae02">dnnl::primitive_attr::set_output_scales</a> which will be covered in <a class="el" href="dev_guide_attributes_quantization.html">Primitive Attributes: Quantization</a>. Unfortunately (or fortunately) the sequence is not supported by the library and is merely used to illustrate the semantics of post-ops.</p>
<div class="fragment"><div class="line"><a class="code" href="structdnnl_1_1post__ops.html">dnnl::post_ops</a> po;</div><div class="line">po.<a class="code" href="structdnnl_1_1post__ops.html#a9cc93664cc11d2cd4c900f172be5703f">append_eltwise</a>(</div><div class="line">        <span class="comment">/* scale     = */</span> s_tanh,</div><div class="line">        <span class="comment">/* alg kind  = */</span> <a class="code" href="group__cpp__api__enums.html#gga00377dd4982333e42e8ae1d09a309640a38dd7159307eab45742c78e72f06abb0">dnnl::algorithm::eltwise_tanh</a>,</div><div class="line">        <span class="comment">/* unused for tanh */</span> 0.f,</div><div class="line">        <span class="comment">/* unused for tanh */</span> 0.f);</div><div class="line">po.<a class="code" href="structdnnl_1_1post__ops.html#a078ab8ec15423d2b3d26f3619a78ca38">append_sum</a>(</div><div class="line">        <span class="comment">/* scale     = */</span> s_sum);</div><div class="line">po.<a class="code" href="structdnnl_1_1post__ops.html#a9cc93664cc11d2cd4c900f172be5703f">append_eltwise</a>(</div><div class="line">        <span class="comment">/* scale     = */</span> s_linear,</div><div class="line">        <span class="comment">/* alg kind  = */</span> <a class="code" href="group__cpp__api__enums.html#gga00377dd4982333e42e8ae1d09a309640a21aba6844d2de47b92ab1d110f561945">dnnl::algorithm::eltwise_linear</a>,</div><div class="line">        <span class="comment">/* scale     = */</span> alpha,</div><div class="line">        <span class="comment">/* shift     = */</span> beta);</div><div class="line"></div><div class="line"><a class="code" href="structdnnl_1_1primitive__attr.html">dnnl::primitive_attr</a> attr;</div><div class="line">attr.<a class="code" href="structdnnl_1_1primitive__attr.html#a4b81acc8e48886313154f75c1708ae02">set_output_scales</a>(0, {s_conv});</div><div class="line">attr.<a class="code" href="structdnnl_1_1primitive__attr.html#a7bf5358e35bf0e99019db45ae8fad0b5">set_post_ops</a>(po);</div><div class="line"></div><div class="line">convolution_forward::primitive_desc(conv_d, attr, engine);</div></div><!-- fragment --><p>This will lead to the following primitive behavior (for better readability the tensors are designated by their names only; i.e., <code>(:)</code> is omitted):</p>
<p class="formulaDsp">
\[ dst = s_{linear} \cdot ( \alpha \cdot ( s_{sum} \cdot dst + s_{tanh} \cdot \tanh ( s_{conv} \cdot conv(src, weights) ) ) + \beta ) \]
</p>
 </div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>