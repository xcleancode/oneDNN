<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Introduction to Low-Precision 8-bit Integer Computations</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">0.21.0</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Introduction to Low-Precision 8-bit Integer Computations </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><blockquote class="doxtable">
<p><img class="formulaInl" alt="$\dagger$" src="form_22.png"/> Disclaimer: MKLDNN Int8 primitives are a work in progress and not all definitions and configurations have been implemented or included in the documentation. Moreover, the example included in this documentation relies on int8 primitives which use the <b>MKL binary dependency</b> and is limited to MKLDNN built with the MKL binary. </p>
</blockquote>
<h2>Introduction</h2>
<p>To push higher performance during inference computations, recent work has focused on computing at a lower precision (i.e. shrinking the size of data for activations and weights) to achieve higher throughput. Eight-bit computations (referred to as int8) offer improved performance over higher precision types -because it allows packing more data into a single instruction, at the cost of reduced but acceptable accuracy.</p>
<h2>Int8 Workflow</h2>
<h3>Quantization Process</h3>
<p>To operate with int8 data types from a higher precision format (e.g. 32-bit floating point), data must first be <em>quantized</em>. The quantization process converts a given input into a lower-precision format. The precision and accuracy factors are determined by the scale and rounding-mode respectively.</p>
<h3>Scale</h3>
<p>The scale is usually obtained from sampling the dataset of previous executions in the original format (e.g. the activations and weights from training in fp32) and is formulated as:</p>
<ul>
<li><img class="formulaInl" alt="$ R_{\{\alpha,w\}} = max(abs(T_{\{\alpha,w\}}))$" src="form_23.png"/><br />
where <img class="formulaInl" alt="$ T_{\{\alpha,w\}}$" src="form_24.png"/> is a tensor corresponding to either the weights <img class="formulaInl" alt="$w$" src="form_25.png"/> or the activations <img class="formulaInl" alt="$\alpha$" src="form_26.png"/>.</li>
</ul>
<p>The purpose is to establish the range of values used in the computation where selecting a proper scaling factor prevents over or underflows when computing the lower precision results.</p>
<h3>Quantization Factor</h3>
<p>The next step is to calculate the <b>quantization factor</b> for converting the values into the corresponding int8 range. This is also known as the <b>scale</b> or <b>scaling factor</b> applied to the original high-precision values and is calculated as:</p>
<ul>
<li><img class="formulaInl" alt="$ Q_{\alpha} = \frac{255}{R_{\alpha}}$" src="form_27.png"/> is the quantization factor for activations with non-negative values.</li>
<li><img class="formulaInl" alt="$ Q_{w} = \frac{127}{R_{w}}$" src="form_28.png"/> is the quantization factor for weights.</li>
</ul>
<p>The low-precision values, known as the <b>quantized</b> activation, weights, and bias values are calculated as:</p><ul>
<li><img class="formulaInl" alt="$\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil \in [0,255]$" src="form_29.png"/></li>
<li><img class="formulaInl" alt="$W_{s8} = \lceil Q_{w} W_{f32} \rceil \in [-127,127]$" src="form_30.png"/></li>
<li><img class="formulaInl" alt="$b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil \in [-2^{31},2^{31}-1]$" src="form_31.png"/><br />
where the function <img class="formulaInl" alt="$ \lceil \rceil $" src="form_32.png"/> rounds to the selected rounding mode.</li>
</ul>
<p>When the destination value (e.g. from a convolution) is stored as a signed 32-bit integer, the result is bound to the same quantization <em>scaling</em> factors:</p><ul>
<li><img class="formulaInl" alt="$ X_{s32} = W_{s8} \times \alpha{u8} + b_{s32} \approx Q_{\alpha} Q_{\omega} X_{f32}$" src="form_33.png"/></li>
<li>where <img class="formulaInl" alt="$X_{f32} = W_{f32} \times \alpha_{f32} + b_{f32}$" src="form_34.png"/></li>
</ul>
<p>Where the approximated value is due to the rounded values.</p>
<p>Inversely, the dequantized value is calculated as:</p><ul>
<li><img class="formulaInl" alt="$ X_{f32} \approx \frac{1}{Q_{\alpha} Q_{\omega}} X_{s32} $" src="form_35.png"/></li>
</ul>
<h3>Quantization Example</h3>
<p>To show how the int8 parameters are obtained, suppose we first start off with a set of arbitrary high-precision input and output values. These values come from sampling a previously executed training run and are in their original 32-bit floating point format as:</p><ul>
<li>activations: <img class="formulaInl" alt="$ T_{\alpha} = [15, 14, 15 ... 8, 11 ]$" src="form_36.png"/> where <img class="formulaInl" alt="$ max(abs(T_{\alpha})) = 15$" src="form_37.png"/></li>
<li>weights: <img class="formulaInl" alt="$ T_{\omega} = [-5.1 , 6.8, ... -1.2, 9.8 ]$" src="form_38.png"/> where <img class="formulaInl" alt="$ max(abs(T_{\omega})) = 9.8$" src="form_39.png"/></li>
<li>bias: <img class="formulaInl" alt="$ T_{\alpha} = [ 2.4, -5.2 ... -8 ]$" src="form_40.png"/> where <img class="formulaInl" alt="$ max(abs(T_{\alpha})) = 8$" src="form_41.png"/></li>
</ul>
<p>The scaling factors are:</p>
<ul>
<li><img class="formulaInl" alt="$ Q_{\alpha} = \frac{255}{R_{\alpha}} = \frac{255}{15} = 17 $" src="form_42.png"/></li>
<li><img class="formulaInl" alt="$ Q_{w} = \frac{127}{R_{w}} = \frac{127}{9.8} = 12.96$" src="form_43.png"/></li>
</ul>
<p>Finally, the quantized input values for the 8-bit operation are calculated as:</p>
<ul>
<li><img class="formulaInl" alt="$\alpha_{u8} = \lceil Q_{\alpha} \alpha_{f32} \rceil$" src="form_44.png"/> <img class="formulaInl" alt="$ = \lceil 17 \times [15, 14, ... 11 ] \rceil = [255, 238, ... 187] $" src="form_45.png"/></li>
<li><img class="formulaInl" alt="$W_{s8} = \lceil Q_{w} W_{f32} \rceil = \lceil 12.96 \times [-5.1 , 6.8, ... -1.2, 9.8 ] \rceil = [-66, 88, ... -15, 127] $" src="form_46.png"/></li>
<li><img class="formulaInl" alt="$b_{s32} = \lceil Q_{\alpha} Q_{w} b_{f32} \rceil = \lceil 17 \times 12.96 \times [ 2.4, -5.2 ... -8 ] \rceil = [528, -1145, ... -1762] $" src="form_47.png"/></li>
</ul>
<p>These arrays are the new inputs for the int8 net.</p>
<h2>MKLDNN Support for low-precision int8 Primitives</h2>
<p>MKLDNN supports low-precision computations for inference through the int8 primitives. Int8 primitives are ordinary MKLDNN primitives which have their input and output parameters configured to 8-bit types. Int8 primitives are optimized for high performance, one example is the use of specialized 512-bit wide low-precision instructions available through the Advanced Vector Extensions 512 (AVX512) for Intel Skylake Server Systems. Currently, the <img class="formulaInl" alt="$\dagger$" src="form_22.png"/>supported primitives are:</p><ul>
<li>convolution</li>
<li>pooling</li>
<li>eltwise</li>
<li>sum</li>
<li>concat</li>
<li>reorder</li>
</ul>
<h3>MKLDNN Attributes</h3>
<p>MKLDNN primitive behaviour may be extended for additional functionalities involving output data transformation. These additional features are configured via <b>primitive attributes</b>. The primitive attributes definition is an opaque structure for passing extra parameters to a primitive descriptor. These parameters include Scaling Factor, Round Mode and Fused Post-Operations (<b>PostOps</b>). All operation primitives support the attributes structure, however, not all configurations are implemented and result in <em>failed primitive creation</em>.</p>
<p>The <b>scaling factor</b>, as previously described, is known prior to the inference operation where the values are calculated from a set of formulas. In MKLDNN, the scaling factor is applied to the output of a primitive. Moreover, to perform input transformations (e.g. source, bias and weights), MKLDNN performs quantizing and dequantizing of data for int8 through the <b>Reorder Primitive</b>.</p>
<p>MKLDNN has 2 formats for defining the output scaling factor, depending on the configuration set by the scaling mask, the output is either scaled uniformly across all the dimensions (<em>mask = 0</em>) or a set of scaling values are applied to specific dimension(s), as explanation below:</p>
<ul>
<li>A <em>single float-value</em> shared across the tensor. <div class="image">
<img src="img_singlescalar.png" alt="img_singlescalar.png"/>
<div class="caption">
Single-value scaling format</div></div>
</li>
<li>An array of float values each corresponding to a specific output channel. <div class="image">
<img src="img_multiscalar.png" alt="img_multiscalar.png"/>
<div class="caption">
Multi-value scaling format</div></div>
 The <b>mask</b> parameter determines the dimension to which the scales array is applied to where the i<sup>th</sup>-bit(s) of mask selects the dimension(s) d<sub>i</sub> (where <em>d</em> is an <em>n</em>-dimensional output tensor with logical dimensions as [<em>d0, d1, ..., dn-1</em>]), for example:</li>
<li>The single-scale format always has mask = 0.</li>
<li>For a 5-dimensional tensor T[g0, o1,i2,h3,w4] where the numbering indicates the bit-index:<ul>
<li>A mask = 2 = 2<sup>1</sup> selects the output channel for scaling.</li>
<li>A mask = 3 = 2<sup>0</sup> | 2<sup>1</sup> selects the group and output channels.</li>
</ul>
</li>
</ul>
<p><b>Note</b>: Mask is always applied to the logical dimension; this is independent of the dimension format that the primitive might select. The dimensions in MKLDNN are defined as follows:</p><ul>
<li>2D dimensional data the order of dimensions is always: (n, c)</li>
<li>4D dimensional data the order is always: (n, c, h, w)</li>
<li>5D dimensional weights the order is always: (g, oc, ic, kh, kw)</li>
</ul>
<p>The <b>Round Mode</b> in attributes specifies the form of rounding for the resulting output value. Round mode will be applied whenever the output data type is not 32-bit floating point. The two options are:</p><ul>
<li>Nearest even integer value (default and recommended)</li>
<li>Down (or floor) to the previous largest integer value.</li>
</ul>
<p>Fused <b>Post-Operations</b> (PostOps) allow chaining operations during the primitive computation. Note that the resulting output value from PostOps is always affected by the scaling factor. The supported operations are:</p><ul>
<li>Accumulation where the primitive sums the resulting values from previously computed activations as:<br />
 <img class="formulaInl" alt="$ dst[ ] \leftarrow scale * dst[] + op(...); instead of dst[ ] \leftarrow op(...) $" src="form_48.png"/></li>
<li>Element-wise (eltwise) operation with kind, alpha and beta parameters as:<br />
 <img class="formulaInl" alt="$ dst[ ] \leftarrow scale * eltwise_op ( op(...) ); instead of dst[ ] \leftarrow op(...)$" src="form_49.png"/></li>
</ul>
<p>The list of supported eltwise operations for int8 is currently limited to ReLU. For instance, PostOps may only configure a convolution with accumulation followed by eltwise (relu).</p>
<h2>Simple_Net.cpp Example using 8-bit and PostOps Computations</h2>
<p>The MKLDNN repository contains an example called <em>simple_int8_net.cpp</em> that executes a Convolution with ReLU from the AlexNet topology using int8 computations. This example extends the simple_net.cpp source focusing on the creation and execution of int8 primitives using PostOps and Scaling Factors to obtain similar results.</p>
<ol type="1">
<li>Initially, the example configures the tensors according to the dimensions in Conv3 of AlexNet. <div class="fragment"><div class="line">memory::dims conv_src_tz = { batch, 256, 13, 13 };</div><div class="line">memory::dims conv_weights_tz = { 384, 256, 3, 3 };</div><div class="line">memory::dims conv_bias_tz = { 384 };</div><div class="line">memory::dims conv_dst_tz = { batch, 384, 13, 13 };</div><div class="line">memory::dims conv_strides = { 1, 1 };</div><div class="line"><span class="keyword">auto</span> conv_padding = { 1, 1 };</div></div><!-- fragment --></li>
<li>Next, the example configures the scales used to quantize fp32 data into int8. For this example, the scaling value is chosen as an arbitrary number, although in a realistic scenario, it should be calculated from a set of precomputed values as previously mentioned. <div class="fragment"><div class="line"><span class="comment">/* Set Scaling mode for int8 quantizing */</span></div><div class="line"><span class="keyword">const</span> std::vector&lt;float&gt; src_scales = { 1.8 };</div><div class="line"><span class="keyword">const</span> std::vector&lt;float&gt; weight_scales = { 2 };</div><div class="line"><span class="keyword">const</span> std::vector&lt;float&gt; bias_scales = { 1 };</div><div class="line"><span class="keyword">const</span> std::vector&lt;float&gt; dst_scales = { 0.55 };</div><div class="line"></div><div class="line"><span class="comment">/* assign halves of vector with arbitrary values */</span></div><div class="line">std::vector&lt;float&gt; conv_scales(384);</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> scales_half = 384 / 2;</div><div class="line">std::fill(conv_scales.begin(), conv_scales.begin() + scales_half, 0.3);</div><div class="line">std::fill(conv_scales.begin() + scales_half, conv_scales.end(), 0.8);</div></div><!-- fragment --> The <em>source, weights, bias</em> and <em>destination</em> datasets use the single-scale format with mask set to ‘0’, while the <em>output</em> from the convolution (conv_scales) will use the array format where mask = 2 corresponding to the output dimension. <div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> src_mask = 0;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> weight_mask = 0;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> bias_mask = 0;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> dst_mask = 0;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> conv_mask = 2; <span class="comment">// 1 &lt;&lt; output_channel_dim</span></div></div><!-- fragment --></li>
<li>Create the memory primitives for user data (source, weights and bias). The user data will be in its original 32-bit floating point format. <div class="fragment"><div class="line"><span class="keyword">auto</span> user_src_memory = memory(</div><div class="line">        { { { conv_src_tz }, memory::data_type::f32, memory::format::nchw },</div><div class="line">                cpu_engine },</div><div class="line">        user_src.data());</div><div class="line"><span class="comment">/* ... */</span></div></div><!-- fragment --></li>
<li><p class="startli">Create a memory descriptor for each convolution parameter. The convolution data uses 8-bit integer values, so the memory descriptors are configured as:</p><ul>
<li>8-bit unsigned (u8) for source and destination.</li>
<li>8-bit signed (s8) for bias and weights.</li>
</ul>
<p class="startli">Note: the destination type is chosen as <em>unsigned</em> because the convolution applies a ReLU operation where data results ≥ 0. </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv_src_md = memory::desc(</div><div class="line">        { conv_src_tz }, memory::data_type::u8, memory::format::any);</div><div class="line"><span class="comment">/* ... */</span></div><div class="line"><span class="keyword">auto</span> conv_dst_md = memory::desc(</div><div class="line">        { conv_dst_tz }, memory::data_type::u8, memory::format::any);</div></div><!-- fragment --></li>
<li>Create a convolution descriptor passing the int8 memory descriptors as parameters. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv_desc = convolution_forward::desc(<a class="code" href="group__cpp__api__enums.html#ggaeb087eae78f70a4d249a90aefa165cf8a064cbe5d8f2575433bfaaaef8937f369">prop_kind::forward</a>,</div><div class="line">        <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a7c32719b6310ede805768dfbe7c909a8">convolution_direct</a>, conv_src_md, conv_weights_md, conv_bias_md,</div><div class="line">        conv_dst_md, conv_strides, conv_padding, conv_padding,</div><div class="line">        <a class="code" href="group__cpp__api__enums.html#ggaa1ff931b28e0a90473ad5bddd21c60fcafbabef80df01bc0a5636d0ca9189355d">padding_kind::zero</a>);</div></div><!-- fragment --></li>
<li><p class="startli">Configuring int8-specific parameters in an int8 primitive is done via the Attributes Primitive. Create an attributes object for the convolution and configure it accordingly. </p><div class="fragment"><div class="line">primitive_attr conv_attr;</div><div class="line"></div><div class="line"><span class="comment">/* Specify the rounding mode */</span></div><div class="line">conv_attr.set_int_output_round_mode(<a class="code" href="group__cpp__api__enums.html#gga9d7c927196d96da9d2dcb0926f50845dadfa185bc1c3ce87a0905475a288c58fd">round_mode::round_nearest</a>);</div><div class="line"></div><div class="line"><span class="comment">/* Specify the scales array and corresponding mask */</span></div><div class="line">conv_attr.set_output_scales(conv_mask, conv_scales);</div></div><!-- fragment --><p class="startli">The ReLU layer from Alexnet is executed through the PostOps feature. Create a PostOps object and configure it to execute an <em>eltwise relu</em> operation. </p><div class="fragment"><div class="line"><span class="keyword">const</span> <span class="keywordtype">float</span> ops_scale = 1.;</div><div class="line"><span class="keyword">const</span> <span class="keywordtype">float</span> negative_slope = 0.;</div><div class="line">post_ops ops;</div><div class="line">ops.append_eltwise(ops_scale, <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a7949269c3f40dbf756f63a20e5dae2e0">algorithm::eltwise_relu</a>, negative_slope, 0.);</div><div class="line">conv_attr.set_post_ops(ops);</div></div><!-- fragment --></li>
<li>Create a primitive descriptor using the convolution descriptor and passing along the int8 attributes in the constructor. The primitive descriptor for the convolution will contain the specific memory formats for the computation. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv_prim_desc = convolution_forward::primitive_desc(</div><div class="line">        conv_desc, conv_attr, cpu_engine);</div></div><!-- fragment --></li>
<li>Create a memory primitive for each of the convolution’s data input parameters (source, bias, weights and destination). Using the convolution primitive descriptor as the creation parameter allows MKLDNN to configure the memory formats for the convolution. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv_src_memory = memory(conv_prim_desc.src_primitive_desc());</div></div><!-- fragment --> Scaling parameters are passed to the reorder primitive via the attributes primitive. <div class="fragment"><div class="line">primitive_attr src_attr;</div><div class="line">src_attr.set_int_output_round_mode(<a class="code" href="group__cpp__api__enums.html#gga9d7c927196d96da9d2dcb0926f50845dadfa185bc1c3ce87a0905475a288c58fd">round_mode::round_nearest</a>);</div><div class="line">src_attr.set_output_scales(src_mask, src_scales);</div></div><!-- fragment --></li>
<li>User memory must be transformed into convolution-friendly memory (for int8 and memory format). A reorder layer performs the data transformation from fp32 (the original user data) into int8 format (the data used for the convolution). In addition, the reorder will transform the user data into the required memory format (as explained in the simple_net example). <div class="fragment"><div class="line"><span class="keyword">auto</span> src_reorder_pd</div><div class="line">        = reorder::primitive_desc(user_src_memory.get_primitive_desc(),</div><div class="line">                conv_src_memory.get_primitive_desc(), src_attr);</div><div class="line">net.push_back(reorder(src_reorder_pd, user_src_memory, conv_src_memory));</div><div class="line"><span class="comment">/* ... */</span></div></div><!-- fragment --></li>
<li>Create the convolution primitive and add it to the net. The int8 example computes the same Convolution +ReLU layers from AlexNet simple-net.cpp using the int8 and PostOps approach. Although performance is not measured here, if it were, in practice, it would require less computation time to achieve similar results. <div class="fragment"><div class="line">net.push_back(convolution_forward(conv_prim_desc, conv_src_memory,</div><div class="line">        conv_weights_memory, conv_bias_memory, conv_dst_memory));</div></div><!-- fragment --></li>
<li>Finally, <em>dst memory</em> may be dequantized from int8 into the original fp32 format. Create a memory primitive for the user data in the original 32-bit floating point format and apply a reorder to transform the computation output data. <div class="fragment"><div class="line"><span class="keyword">auto</span> user_dst_memory = memory(</div><div class="line">        { { { conv_dst_tz }, memory::data_type::f32, memory::format::nchw },</div><div class="line">                cpu_engine },</div><div class="line">        user_dst.data());</div><div class="line">primitive_attr dst_attr;</div><div class="line">dst_attr.set_int_output_round_mode(<a class="code" href="group__cpp__api__enums.html#gga9d7c927196d96da9d2dcb0926f50845dadfa185bc1c3ce87a0905475a288c58fd">round_mode::round_nearest</a>);</div><div class="line">dst_attr.set_output_scales(dst_mask, dst_scales);</div><div class="line"><span class="keyword">auto</span> dst_reorder_pd</div><div class="line">        = reorder::primitive_desc(conv_dst_memory.get_primitive_desc(),</div><div class="line">                user_dst_memory.get_primitive_desc(), dst_attr);</div></div><!-- fragment --></li>
</ol>
<p>The diagram to summarize this example is as follows:</p>
<div class="image">
<img src="img_diagram.png" alt="img_diagram.png"/>
<div class="caption">
Diagram depicting simple_net_int8 data flow</div></div>
<hr/>
<p> <a class="el" href="legal_information.html">Legal information</a> </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
