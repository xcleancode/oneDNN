<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: Winograd Convolution</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">0.18.1</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Winograd Convolution </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Why use a different convolution algorithm?</h2>
<p>Executing convolution using the <b>Winograd algorithm</b> often gives a significant performance boost compared with using the <b>Direct algorithm</b>. Details about the algorithm can be found in <a href="https://arxiv.org/abs/1509.09308"><b>Fast Algorithms for Convolutional Neural Networks by A. Lavin and S. Gray</b></a>.</p>
<h2>Winograd in Intel(R) MKL-DNN</h2>
<p>Intel(R) MKL-DNN supports the <b>Winograd algorithm</b> for convolutions with the following sizes:</p><ul>
<li>2D convolution (i.e. spatial depth <code>d=1</code>)</li>
<li>kernel sizes <code>kh=3,kw=3</code>.</li>
<li>strides <code>sh=sw=1</code>.</li>
<li><b>Inference</b> - Based on convolution sizes, MKLDNN chooses between two different tile sizes F(2x2, 3x3) or F(4x4, 3x3)(refer to <a href="https://arxiv.org/abs/1509.09308">Winograd paper</a> for more informartion on tile sizes).</li>
<li><b>Training</b> - Uses F(4x4, 3x3) winograd.</li>
</ul>
<p>Create a Winograd convolution by simply creating a convolution descriptor (step 6 in <a class="el" href="ex_simplenet.html">SimpleNet Example</a>) with right algorithm. The rest of the steps for creating convolution are exactly the same as shown in the example. </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_desc = convolution_forward::desc(</div><div class="line">    <a class="code" href="group__cpp__api__enums.html#ggaeb087eae78f70a4d249a90aefa165cf8ad3fbf2689723f727189276e535b0058e">prop_kind::forward_inference</a>, <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a3fcadb2934e7f8a0a4b9dacbcf9e8e36">algorithm::convolution_winograd</a>,</div><div class="line">    conv1_src_md, conv1_weights_md, conv1_bias_md, conv1_dst_md,</div><div class="line">    conv1_strides, conv1_padding, <a class="code" href="group__cpp__api__enums.html#ggaa1ff931b28e0a90473ad5bddd21c60fcafbabef80df01bc0a5636d0ca9189355d">padding_kind::zero</a>);</div></div><!-- fragment --><h2>Auto dispatching of convolution algorithm</h2>
<p>Instead of choosing a convolution algorithm for each and every convolution in a topology, a user could simply ask MKLDNN to make the choice.</p>
<p>Creating a convolution by using <code>convolution_auto</code> allows MKLDNN to dispatch the <em>best</em> algorithm. </p><div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_desc = convolution_forward::desc(</div><div class="line">    <a class="code" href="group__cpp__api__enums.html#ggaeb087eae78f70a4d249a90aefa165cf8ad3fbf2689723f727189276e535b0058e">prop_kind::forward_inference</a>, <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a7f2458b493b72f9e9099082c5be09e58">algorithm::convolution_auto</a>,</div><div class="line">    conv1_src_md, conv1_weights_md, conv1_bias_md, conv1_dst_md,</div><div class="line">    conv1_strides, conv1_padding, <a class="code" href="group__cpp__api__enums.html#ggaa1ff931b28e0a90473ad5bddd21c60fcafbabef80df01bc0a5636d0ca9189355d">padding_kind::zero</a>);</div></div><!-- fragment --><p>MKLDNN would choose the algorithm which will potentially give <em>best performance</em> based on</p><ul>
<li>convolution dimensions</li>
<li>number of logical processors available. (For auto-dispatching to work as intended, use the same thread affinity settings when creating the convolution as when executing the convolution.) <em>The relationship between convolution sizes and the best performing algorithm is empirically based on performance observations</em></li>
</ul>
<h3>Example using benchdnn</h3>
<p>The following examples use <a href="https://github.com/intel/mkl-dnn/tree/master/tests/benchdnn"><b>benchdnn</b></a> to illustrate the performance benefits of using <code>convolution_auto</code>.</p>
<p>On a 2 Socket Intel Xeon 8180 processor with 28 cores/socket and HT off: </p><div class="fragment"><div class="line">OMP_NUM_THREADS=56 KMP_AFFINITY=granularity=fine,compact numactl -l tests/benchdnn/benchdnn --mode=p --conv -v5 --alg=auto --dir=BWD_WB mb112ic64ih300oc64oh300kh3ph1n&quot;ssd_300_voc0712:conv1_2&quot;</div><div class="line"></div><div class="line">mkldnn implementation: jit_wino_4x3:avx512_core</div><div class="line">...</div><div class="line">mkldnn_verbose,exec,convolution,jit_wino_4x3:avx512_core,backward_weights,fsrc:nChw16c fwei:gOIhw16i16o fbia:x fdst:nChw16c,alg:convolution_winograd,mb112_g1ic64oc64_ih300oh300kh3sh1dh0ph1_iw300ow300kw3sw1dw0pw1,61.32</div><div class="line">...</div><div class="line">perf,ssd_300_voc0712:conv1_2,--dir=BWD_WB --alg=auto mb112ic64ih300oc64oh300kh3ph1nssd_300_voc0712:conv1_2,739.879,0,61.332,12063.5,62.503,11837.5</div></div><!-- fragment --><p>In the above test-case <code>convolution_auto</code> choses winograd convolution (using a heuristic based on the convolution sizes and number of threads), as winograd convolution is faster than direct in this case. </p><div class="fragment"><div class="line">OMP_NUM_THREADS=56 KMP_AFFINITY=granularity=fine,compact numactl -l tests/benchdnn/benchdnn --mode=p --conv -v5 --alg=direct --dir=BWD_WB mb112ic64ih300oc64oh300kh3ph1n&quot;ssd_300_voc0712:conv1_2&quot;</div><div class="line"></div><div class="line">mkldnn implementation: jit:avx512_common</div><div class="line">...</div><div class="line">mkldnn_verbose,exec,convolution,jit:avx512_common,backward_weights,fsrc:nchw fwei:gOhwi16o fbia:x fdst:nChw16c,alg:convolution_direct,mb112_g1ic64oc64_ih300oh300kh3sh1dh0ph1_iw300ow300kw3sw1dw0pw1,176.10</div><div class="line">...</div><div class="line">perf,ssd_300_voc0712:conv1_2,--dir=BWD_WB mb112ic64ih300oc64oh300kh3ph1nssd_300_voc0712:conv1_2,739.879,0,175.422,4217.7,180.315,4103.26</div></div><!-- fragment --><p><br />
</p>
<p>In the following example, <code>convolution_auto</code> chooses direct convolution because the winograd implementation is slower than direct in this case. </p><div class="fragment"><div class="line">OMP_NUM_THREADS=56 KMP_AFFINITY=granularity=fine,compact tests/benchdnn/benchdnn --mode=p --conv -v5 --alg=auto --dir=BWD_WB mb112ic64ih28oc64oh28kh3ph1n&quot;googlenet_v2:inception_3a/3x3&quot;</div><div class="line"></div><div class="line">mkldnn implementation: jit:avx512_common</div><div class="line">...</div><div class="line">mkldnn_verbose,exec,convolution,jit:avx512_common,backward_weights,fsrc:nChw16c fwei:gOIhw16i16o fbia:x fdst:nChw16c,alg:convolution_direct,mb112_g1ic64oc64_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,1.13</div><div class="line">perf,googlenet_v2:inception_3a/3x3,--dir=BWD_WB --alg=auto mb112ic64ih28oc64oh28kh3ph1ngooglenet_v2:inception_3a/3x3,6.1693,0,1.04272,5916.52,1.13284,5445.88</div></div><!-- fragment --> <div class="fragment"><div class="line">OMP_NUM_THREADS=56 KMP_AFFINITY=granularity=fine,compact tests/benchdnn/benchdnn --mode=p --conv -v5 --alg=wino --dir=BWD_WB mb112ic64ih28oc64oh28kh3ph1n&quot;googlenet_v2:inception_3a/3x3&quot;</div><div class="line"></div><div class="line">mkldnn implementation: jit_wino_4x3:avx512_core</div><div class="line">...</div><div class="line">mkldnn_verbose,exec,convolution,jit_wino_4x3:avx512_core,backward_weights,fsrc:nChw16c fwei:gOIhw16i16o fbia:x fdst:nChw16c,alg:convolution_winograd,mb112_g1ic64oc64_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.15</div><div class="line">...</div><div class="line">perf,googlenet_v2:inception_3a/3x3,--dir=BWD_WB --alg=wino mb112ic64ih28oc64oh28kh3ph1ngooglenet_v2:inception_3a/3x3,6.1693,0,2.14404,2877.41,2.20445,2798.56</div></div><!-- fragment --><h2>Other considerations when using Winograd</h2>
<p>The following side-effects should be weighed against the performance boost achieved when using Winograd:</p><ul>
<li><b>Memory</b> - Transforms are intermmediate results in winograd, which often require significant memory. Currently this memory is allocated internally by MKLDNN as scratchpad memory. As more convolutions using winograd are added to the topology, this memory could grow significantly. This growth is mitigated when several convolutions using Winograd are created by the same instance and executed sequentially, because then this scratchpad can be shared between convolutions.</li>
<li><b>Accuracy</b> - In some cases Winograd can be signficantly less accurate than direct as demontrated in <a href="https://arxiv.org/abs/1509.09308">Winograd paper</a>. </li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
