<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>Intel(R) MKL-DNN: SimpleNet Example</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Intel(R) Math Kernel Library for Deep Neural Networks (Intel(R) MKL-DNN)
   &#160;<span id="projectnumber">0.18.1</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">SimpleNet Example </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This C++ API example demonstrates how to build an AlexNet neural network topology for forward-pass inference. Some key take-aways include:</p>
<ul>
<li>How tensors implemented and submitted to primitives.</li>
<li>How primitives are created.</li>
<li>How primitives are sequentially submitted to the network, where the output from primitives is passed as input to the next primitive. The later specifies dependency between primitive input &lt;-&gt; output data.</li>
<li>Specific 'inference-only' configurations.</li>
<li>Limit the number of reorders performed which are decremental to performance.</li>
</ul>
<p>The simple_net.cpp example implements the AlexNet layers as numbered primitives (e.g. conv1, pool1, conv2).</p>
<h2>Highlights for implementing the simple_net.cpp Example:</h2>
<ol type="1">
<li>Initialize a CPU engine. The last parameter in the engine() call represents the index of the engine. <div class="fragment"><div class="line"><span class="keyword">using namespace </span><a class="code" href="namespacemkldnn.html">mkldnn</a>;</div><div class="line"><span class="keyword">auto</span> cpu_engine = <a class="code" href="structmkldnn_1_1engine.html">engine</a>(<a class="code" href="structmkldnn_1_1engine.html#a81bcf1ea92d7f98852a2c3e187825de6a7e33e884a6d7eb40a73125557b14a7a7">engine::cpu</a>, 0);</div></div><!-- fragment --></li>
<li>Create a primitives vector that represents the net. <div class="fragment"><div class="line">std::vector&lt;primitive&gt; net;</div></div><!-- fragment --></li>
<li>Additionally, create a separate vector holding the weights. This will allow executing transformations only once and outside the topology stream. <div class="fragment"><div class="line">std::vector&lt;primitive&gt; net_weights;</div></div><!-- fragment --></li>
<li>Allocate a vector for input data and create the tensor to configure the dimensions. <div class="fragment"><div class="line">memory::dims conv1_src_tz = { batch, 3, 227, 227 };</div><div class="line">std::vector&lt;float&gt; user_src(batch * 3 * 227 * 227);</div><div class="line"><span class="comment">/* similarly, specify tensor structure for output, weights and bias */</span></div></div><!-- fragment --></li>
<li>Create a memory primitive for data in user format as <code>nchw</code> (minibatch-channels-height-width). Create a memory descriptor for the convolution input, selecting <code>any</code> for the data format. The <code>any</code> format allows the convolution primitive to choose the data format that is most suitable for its input parameters (convolution kernel sizes, strides, padding, and so on). If the resulting format is different from <code>nchw</code>, the user data must be transformed to the format required for the convolution (as explained below). <div class="fragment"><div class="line"><span class="keyword">auto</span> user_src_memory = memory({ { { conv1_src_tz }, memory::data_type::f32,</div><div class="line">    memory::format::nchw }, cpu_engine}, user_src.data());</div><div class="line"><span class="keyword">auto</span> conv1_src_md = memory::desc({conv1_src_tz},</div><div class="line">    memory::data_type::f32, memory::format::any);</div><div class="line"><span class="comment">/* similarly create conv_weights_md and conv_dst_md in format::any */</span></div></div><!-- fragment --></li>
<li>Create a convolution descriptor by specifying the algorithm(<a class="el" href="winograd_convolution.html">convolution algorithms</a>, propagation kind, shapes of input, weights, bias, output, convolution strides, padding, and kind of padding. Propagation kind is set to <em>forward_inference</em> -optimized for inference execution and omits computations that are only necessary for backward propagation. */ <div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_desc = convolution_forward::desc(</div><div class="line">    <a class="code" href="group__cpp__api__enums.html#ggaeb087eae78f70a4d249a90aefa165cf8ad3fbf2689723f727189276e535b0058e">prop_kind::forward_inference</a>, <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a7c32719b6310ede805768dfbe7c909a8">algorithm::convolution_direct</a>,</div><div class="line">    conv1_src_md, conv1_weights_md, conv1_bias_md, conv1_dst_md,</div><div class="line">    conv1_strides, conv1_padding, <a class="code" href="group__cpp__api__enums.html#ggaa1ff931b28e0a90473ad5bddd21c60fcafbabef80df01bc0a5636d0ca9189355d">padding_kind::zero</a>);</div></div><!-- fragment --></li>
<li>Create a descriptor of the convolution primitive. Once created, this descriptor has specific formats instead of the <code>any</code> format specified in the convolution descriptor. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_prim_desc = convolution_forward::primitive_desc(conv1_desc, cpu_engine);</div></div><!-- fragment --></li>
<li>Create a convolution memory primitive from the user memory and check whether the user data format differs from the format that the convolution requires. In case it is different, create a reorder primitive that transforms the user data to the convolution format and add it to the net. Repeat this process for weights as well. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_src_memory = user_src_memory;</div><div class="line"></div><div class="line"><span class="comment">/* Check whether a reorder is necessary  */</span></div><div class="line"><span class="keywordflow">if</span> (memory::primitive_desc(conv1_prim_desc.src_primitive_desc())</div><div class="line">        != user_src_memory.get_primitive_desc()) {</div><div class="line">    <span class="comment">/* Yes, a reorder is necessary */</span></div><div class="line"></div><div class="line">    <span class="comment">/* The convolution primitive descriptor contains the descriptor of a memory</span></div><div class="line"><span class="comment">     * primitive it requires as input. Because a pointer to the allocated</span></div><div class="line"><span class="comment">     * memory is not specified, Intel MKL-DNN allocates the memory. */</span></div><div class="line">    conv1_src_memory = memory(conv1_prim_desc.src_primitive_desc());</div><div class="line"></div><div class="line">    <span class="comment">/* create a reorder between user and convolution data and put the reorder</span></div><div class="line"><span class="comment">     * into the net. The conv1_src_memory will be the input for the convolution */</span></div><div class="line">    net.push_back(reorder(user_src_memory, conv1_src_memory));</div><div class="line">}</div></div><!-- fragment --></li>
<li>Create a memory primitive for output. <div class="fragment"><div class="line"><span class="keyword">auto</span> conv1_dst_memory = memory(conv1_prim_desc.dst_primitive_desc());</div></div><!-- fragment --></li>
<li>Create a convolution primitive and add it to the net. <div class="fragment"><div class="line"><span class="comment">/* Note that the conv_reorder_src primitive</span></div><div class="line"><span class="comment"> * is an input dependency for the convolution primitive, which means that the</span></div><div class="line"><span class="comment"> * convolution primitive will not be executed before the data is ready. */</span></div><div class="line">net.push_bash(convolution_forward(conv1_prim_desc, conv1_src_memory, conv1_weights_memory,</div><div class="line">                              user_bias_memory, conv1_dst_memory));</div></div><!-- fragment --></li>
<li>Create relu primitive. For better performance keep ReLU (as well as for other operation primitives until another convolution or inner product is encountered) input data format in the same format as was chosen by convolution. Furthermore, ReLU is done in-place by using conv1 memory. <div class="fragment"><div class="line"><span class="keyword">auto</span> relu1_desc = eltwise_forward::desc(<a class="code" href="group__cpp__api__enums.html#ggaeb087eae78f70a4d249a90aefa165cf8ad3fbf2689723f727189276e535b0058e">prop_kind::forward_inference</a>,</div><div class="line">    <a class="code" href="group__cpp__api__enums.html#ggae45a07d6121fdd33e310782753178076a7949269c3f40dbf756f63a20e5dae2e0">algorithm::eltwise_relu</a>, conv1_dst_memory.get_primitive_desc().desc(), negative1_slope);</div><div class="line"><span class="keyword">auto</span> relu1_prim_desc = eltwise_forward::primitive_desc(relu1_desc, cpu_engine);</div><div class="line">net.push_back(eltwise_forward(relu1_prim_desc, conv1_dst_memory, conv1_dst_memory));</div></div><!-- fragment --></li>
<li>For training execution, pooling requires a private workspace memory to perform the backward pass. However, pooling should not use 'workspace' for inference as this is decremental to performance. <div class="fragment"><div class="line"><span class="comment">/* create pooling indices memory from pooling primitive descriptor */</span></div><div class="line"><span class="comment">// auto pool1_indices_memory = memory(pool1_pd.workspace_primitive_desc());</span></div><div class="line"><span class="keyword">auto</span> pool1_dst_memory = memory(pool1_pd.dst_primitive_desc());</div><div class="line"></div><div class="line"><span class="comment">/* create pooling primitive an add it to net */</span></div><div class="line">net.push_back(pooling_forward(pool1_pd, lrn1_dst_memory, pool1_dst_memory</div><div class="line">    <span class="comment">/* pool1_indices_memory */</span>));</div></div><!-- fragment --> The example continues to create more layers according to the AlexNet topology.</li>
<li>Finally, create a stream to execute weights data transformation. This is only required once. Create another stream that will exeute the 'net' primitives. For this example, the net is executed multiple times and each execution es timed individually. <div class="fragment"><div class="line"><span class="comment">/* Weight transformation - executed once */</span></div><div class="line">stream(stream::kind::eager).submit(net_weights).wait();</div><div class="line"></div><div class="line"><span class="comment">/* Execute the topology */</span></div><div class="line"><a class="code" href="structmkldnn_1_1stream.html">mkldnn::stream</a>(mkldnn::stream::kind::eager).<a class="code" href="structmkldnn_1_1stream.html#a018e38932cc072568a48015f55fbe798">submit</a>(net).<a class="code" href="structmkldnn_1_1stream.html#a577fe98393390dc1ba646266d57566e2">wait</a>();</div></div><!-- fragment --><hr/>
</li>
</ol>
<p><a class="el" href="legal_information.html">Legal information</a> </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
