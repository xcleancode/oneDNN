<!-- HTML header for doxygen 1.8.5-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<title>oneDNN: Inference and Training Aspects</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script src="assets/mathjax/MathJax.js?config=TeX-AMS_CHTML,dnnl"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
<link href="assets/customdoxygen.css" rel="stylesheet" type="text/css" />
<script type="text/javascript" src="assets/dnn.js"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
   <div id="projectname">oneAPI Deep Neural Network Library (oneDNN)
   &#160;<span id="projectnumber">1.6.5</span>
   </div>
   <div id="projectbrief">Performance library for Deep Learning</div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Inference and Training Aspects </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="dev_guide_inference_and_training_prop_kinds"></a></p><h2>Propagation Kinds</h2>
<p>oneDNN provides performance critical primitives to accelerate operations used both during <b>training</b> deep learning models and during the operations performed when the models are used for <b>inference</b>.</p>
<p>During inference, the input data is fed into the trained model which in turn produces a result (e.g. makes a prediction). This process is usually called forward propagation and corresponds to the <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa3b9fad4f80d45368f856b5403198ac4c" title="Forward data propagation (inference mode). ">dnnl::prop_kind::forward_inference</a> propagation kind in oneDNN.</p>
<p>Training usually consists of the following steps.</p><ol type="1">
<li>Make prediction based on the current state of the model. As in the case of inference, this step is called <b>forward</b> propagation, but corresponds to the <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa24775787fab8f13aa4809e1ce8f82aeb" title="Forward data propagation (training mode). ">dnnl::prop_kind::forward_training</a> propagation kind. Note the difference in the names' suffixes: <code>_training</code> here versus <code>_inference</code> mentioned above. The differences are covered below in the corresponding <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_forward_training_vs_inference">section</a> below.</li>
<li>Compute an error between predicted and the <em>actual</em> answer.</li>
<li>Perform the backward propagation of errors to compute the weights (learnable parameters) gradient. For a given operation (layer) the backward propagation in turn can be split into two steps:<ul>
<li>Propagating error with respect to data, i.e. computing <code>diff_src</code> from <code>diff_dst</code> (see <a class="el" href="dev_guide_conventions.html">Naming Conventions</a>). This step corresponds to the <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438faa12627cacf73ecb7ef088beedd650e96" title="Backward data propagation. ">dnnl::prop_kind::backward_data</a> propagation kind;</li>
<li>Propagating error with respect to weights, i.e. computing <code>diff_weights</code> from <code>diff_dst</code>. This step makes sense only for the operations that have learnable parameters and corresponds to the <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa1a002980f340e61153a9f7de4f794cf6" title="Backward weights propagation. ">dnnl::prop_kind::backward_weights</a> propagation kind.</li>
</ul>
</li>
<li>Use computed gradients to modify the weights according to the chosen solver to improve the accuracy of the model.</li>
</ol>
<h3>Difference Between Forward Propagation on Training and Inference</h3>
<p><a class="anchor" id="dev_guide_inference_and_training_forward_training_vs_inference"></a> Even though, mathematically, the forward propagation that happens during training and inference should be the same, in practice there are some differences mostly due to the performance considerations.</p>
<p>When executing inference, one may not care about values in the intermediate buffers during a model execution; hence one can reuse them as desired. However, if this is a forward propagation of a training it is beneficial to preserve input data, output data, or sometimes some intermediate data, that will later be used at the backward propagation to compute the gradients.</p>
<p>For example, let's take max pooling (<a class="el" href="dev_guide_pooling.html">Pooling</a> with algorithm kind <a class="el" href="group__dnnl__api__attributes.html#gga00377dd4982333e42e8ae1d09a309640a8c73d4bb88a0497586a74256bb338e88" title="Max pooling. ">dnnl::algorithm::pooling_max</a>) as an example. The forward pass consists of computing the maximum values in the sliding window over the source tensor. Hence the output is just another tensor that contain these maximum values. However, in order to compute source gradient on backward propagation one needs to know the position of these maximum values in the source tensor. Of course, it is possible to use the original source tensor to locate the maximums again, but this might be more expensive compared to preserving the positions of the maximum values in another tensor, that will be then used during the backward propagation. oneDNN uses the latter approach: for max pooling primitive when the propagation kind is set to <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa24775787fab8f13aa4809e1ce8f82aeb" title="Forward data propagation (training mode). ">dnnl::prop_kind::forward_training</a> the library produces one extra output called <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_workspace">Workspace</a> which will be covered later in this document.</p>
<dl class="section note"><dt>Note</dt><dd>Key takeaways:<ul>
<li>Always use <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa3b9fad4f80d45368f856b5403198ac4c" title="Forward data propagation (inference mode). ">dnnl::prop_kind::forward_inference</a> when running inference; use <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa24775787fab8f13aa4809e1ce8f82aeb" title="Forward data propagation (training mode). ">dnnl::prop_kind::forward_training</a> for forward pass of training.</li>
<li>The number of a primitive's outputs might be greater by 1 if it was created with <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa24775787fab8f13aa4809e1ce8f82aeb" title="Forward data propagation (training mode). ">dnnl::prop_kind::forward_training</a> because of the extra <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_workspace">Workspace</a> memory.</li>
</ul>
</dd></dl>
<h3>Different Backward Propagation Kinds</h3>
<p><a class="anchor" id="dev_guide_inference_and_training_aspects_difference_backward_prop_kinds"></a> As mentioned above, oneDNN separates error back-propagation with respect to data and error back-propagation with respect to weights. The former corresponds to <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438faa12627cacf73ecb7ef088beedd650e96" title="Backward data propagation. ">dnnl::prop_kind::backward_data</a>, while the latter corresponds to <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa1a002980f340e61153a9f7de4f794cf6" title="Backward weights propagation. ">dnnl::prop_kind::backward_weights</a> (for example: <a class="el" href="dev_guide_convolution.html">Convolution</a>).</p>
<h2>Inference-Specific Aspects</h2>
<p>The following list outlines the key specifics of running inference with oneDNN:</p>
<ol type="1">
<li>As described above, always use <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa3b9fad4f80d45368f856b5403198ac4c" title="Forward data propagation (inference mode). ">dnnl::prop_kind::forward_inference</a> as a propagation kind.</li>
<li>To get maximum performance, consider performing operations in-place whenever possible (e.g. <a class="el" href="dev_guide_eltwise.html">Eltwise</a> and <a class="el" href="dev_guide_batch_normalization.html">Batch Normalization</a>). Check the primitives documentation pages to check which primitives support in-place operations.</li>
<li>Create primitives once, and reuse them across multiple model invocations. This is especially relevant for the frameworks integration.</li>
<li>Some primitives can be chained/fused with others using the <a class="el" href="dev_guide_attributes_post_ops.html">post-ops attributes</a>. This allows reducing memory bandwidth pressure and typically leads to better performance.</li>
</ol>
<p>Most of these techniques are shown in the following examples:</p><ul>
<li><a class="el" href="cnn_inference_f32_cpp.html">CNN f32 inference example</a></li>
<li><a class="el" href="cnn_inference_int8_cpp.html">CNN int8 inference example</a></li>
</ul>
<p><a class="anchor" id="dev_guide_inference_and_training_aspects_training"></a></p><h2>Training-Specific Aspects</h2>
<p>The following list outlines the key specifics of running training with oneDNN:</p>
<ol type="1">
<li>During the forward propagation, use <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa24775787fab8f13aa4809e1ce8f82aeb" title="Forward data propagation (training mode). ">dnnl::prop_kind::forward_training</a> as a propagation kind.</li>
<li>During backward propagation, perform backward by data and backward by weights using the corresponding propagation kinds.<ul>
<li>Note that some primitives may combine these operations if that is beneficial from a performance perspective. For example, <a class="el" href="dev_guide_rnn.html">RNN</a> and <a class="el" href="dev_guide_batch_normalization.html">Batch Normalization</a> compute both <code>diff_src</code> and <code>diff_weights</code> at the same time. To highlight this behavior, the propagation kind is set to <a class="el" href="group__dnnl__api__attributes.html#ggac7db48f6583aa9903e54c2a39d65438fa195fe59b6f103787a914aead0f3db502" title="Backward propagation (with respect to all parameters). ">dnnl::prop_kind::backward</a>.</li>
</ul>
</li>
<li>Create primitives once, and reuse them across multiple model invocations. This is especially relevant for the frameworks integration.</li>
<li>The <a class="el" href="dev_guide_attributes_post_ops.html">post-ops attributes</a> in general are not applicable for training, because the fused computations they result in do not produce the intermediate tensors which may be required during the backward propagation. For example, if you fuse <a class="el" href="dev_guide_convolution.html">Convolution</a> and <a class="el" href="dev_guide_eltwise.html">Eltwise</a> the direct output of the convolution would not be produced. However, it might be required during the backward propagation of the corresponding element-wise. (To compute <code>diff_src</code>, one must pass <code>diff_dst</code> memory and the original <code>src</code> memory, which was exactly the intermediate one.)</li>
<li>To compute backward propagation, different primitives might require different tensors. The variety is caused by the mathematical formulas. For example, to compute backward propagation for <a class="el" href="dev_guide_eltwise.html">Eltwise</a> one needs to pass <code>diff_dst</code> and <code>src</code>, but to compute backward propagation for <a class="el" href="dev_guide_softmax.html">Softmax</a>, one needs to pass <code>diff_dst</code> and <code>dst</code>. Check the documentation for each primitive to see what is required for each particular primitive.</li>
<li>For the primitives that are created with <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> memory format tag, there are no guarantees that the memory format on forward and backward propagations will match. So the robust integration should always be ready to emit <a class="el" href="dev_guide_reorder.html">Reorder</a> when necessary. For example, it is not guaranteed that the <code>src</code> memory format of a convolution on forward propagation will always match the <code>src</code> memory format of the corresponding convolution on backward by weights propagation. Of course, the library tries to avoid unnecessary reorder, so in most cases the formats will be the same, but this would by no means always be true.</li>
<li>For the memory bandwidth bound primitives like <a class="el" href="dev_guide_eltwise.html">Eltwise</a>, <a class="el" href="dev_guide_pooling.html">Pooling</a>, and <a class="el" href="dev_guide_batch_normalization.html">Batch Normalization</a>, it is important to have <code>diff_dst</code> in the same memory format as the original <code>dst</code>. The mismatch of the formats would lead to significant performance issues. To ensure the proper format, users should always use <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> memory format for gradient tensors (<code>diff_dst</code>, <code>diff_src</code>). If a primitive requires original data tensors (e.g. <code>src</code> in <a class="el" href="dev_guide_eltwise.html">Eltwise</a> or <code>dst</code> in <a class="el" href="dev_guide_softmax.html">Softmax</a>) user <b>must</b> pass fully defined memory descriptor for these tensors. In other words <code>src</code> and <code>dst</code> memory descriptors cannot be initialized with <a class="el" href="structdnnl_1_1memory.html#a8e71077ed6a5f7fb7b3e6e1a5a2ecf3fa100b8cad7cf2a56f6df78f171f97a1ec" title="Placeholder memory format tag. ">dnnl::memory::format_tag::any</a> for backward propagation. Based on the format of the original tensors, if any, and on forward primitive descriptor hint (see bullet 9 below) a primitive picks the proper format for the gradients. Occasionally, it might appear that the <code>diff_dst</code> that comes in is in other memory format than the primitive requires, hence robust integration code must be prepared to emit a reorder.<ul>
<li>Alternatively, users may manually enforce <code>diff_dst</code> to have the same memory format as <code>dst</code>, though this is not recommended.</li>
</ul>
</li>
<li>Some primitives require an additional tensor to be passed between forward and backward propagation, which is called <a class="el" href="dev_guide_inference_and_training_aspects.html#dev_guide_inference_and_training_aspects_workspace">Workspace</a>.</li>
<li>When creating primitive descriptors on backward propagation, you might need to pass a primitive descriptor of the corresponding primitive from the forward propagation (in the API this primitive descriptor is typically called <em>hint</em>). This hint is required for the primitive to choose a proper implementation that would correspond to the one from the forward propagation. This is required only for the primitives that produce <code>workspace</code>, because it might be different for different implementations.</li>
<li>When creating your working memory and memory descriptor, specify the type of memory you want to work with. This can be either 16-bit Brain Float (bf16) or 32-bit Floating Point (fp32). More details about using bf16 for training are detailed in the section <a class="el" href="dev_guide_training_bf16.html">Bfloat16 Training</a>.</li>
</ol>
<p>Most of these techniques are shown in the following examples:</p><ul>
<li><a class="el" href="cnn_training_f32_cpp.html">CNN f32 training example</a></li>
<li><a class="el" href="cnn_training_bf16_cpp.html">CNN bf16 training example</a></li>
</ul>
<p><a class="anchor" id="dev_guide_inference_and_training_aspects_workspace"></a></p><h2>Workspace</h2>
<p>oneDNN uses the notion of <code>workspace</code> for some very particular cases. Specifically, the <code>workspace</code> is a tensor that the primitive fills in during forward propagation and that will then be used by the corresponding backward propagation operation. The example with max pooling was already discussed above.</p>
<p>The workflow for using workspace is:</p><ol type="1">
<li>When creating a primitive for the forward propagation, query the primitive descriptor about the workspace requirement using <code>.workspace_desc()</code>.<ul>
<li>If the returned memory descriptor is essentially empty (i.e. is equal to <code><a class="el" href="structdnnl_1_1memory_1_1desc.html" title="A memory descriptor. ">dnnl::memory::desc()</a></code> or for which <a class="el" href="structdnnl_1_1memory_1_1desc.html#ac20108bc192912382aa4a95ae27df804">dnnl::memory::desc::get_size()</a> returns 0), no extra action is required&ndash;the workspace is not required for this primitive in this configuration.</li>
<li>Otherwise, create a workspace memory based on the memory descriptor obtained and pass it to the execution function with <code>DNNL_ARG_WORKSPACE</code> tag.</li>
</ul>
</li>
<li>On backward propagation, attach that same workspace memory during the execution as well. The state of the workspace memory after backward computations are done is undefined.</li>
</ol>
<dl class="section note"><dt>Note</dt><dd>Even if workspace is not required, it is perfectly valid to create a <code>workspace</code> memory of zero size and follow the logic where the workspace is indeed required. Such an approach may simplify the integration because the common pass is used.</dd></dl>
<div class="fragment"><div class="line"><span class="comment">// FWD</span></div><div class="line"></div><div class="line"><span class="keyword">auto</span> forward_primitive_desc = ...::primitive_desc(); <span class="comment">// create a primitive desc</span></div><div class="line"><span class="keyword">auto</span> <a class="code" href="group__dnnl__api__primitives__common.html#gga94efdd650364f4d9776cfb9b711cbdc1a3874c56bb4069607213666573dff2a96">workspace_md</a> = forward_primitive_desc.workspace_desc(); <span class="comment">// query workspace</span></div><div class="line">memory workspace(workspace_md, engine); <span class="comment">// create a memory (even if empty)</span></div><div class="line"></div><div class="line">primitive_forward.execute(stream, {</div><div class="line">        ...,</div><div class="line">        {<a class="code" href="group__dnnl__api__primitives__common.html#ga550c80e1b9ba4f541202a7ac98be117f">DNNL_ARG_WORKSPACE</a>, workspace} <span class="comment">// this is output</span></div><div class="line">        });</div><div class="line"><span class="comment">// The workspace contains required information for the backward propagation,</span></div><div class="line"><span class="comment">// hence should not be used anywhere else.</span></div><div class="line"></div><div class="line"><span class="comment">// ...</span></div><div class="line"></div><div class="line"><span class="comment">// BWD</span></div><div class="line">primitive_backward.execute(stream, {</div><div class="line">        ...,</div><div class="line">        {<a class="code" href="group__dnnl__api__primitives__common.html#ga550c80e1b9ba4f541202a7ac98be117f">DNNL_ARG_WORKSPACE</a>, workspace} <span class="comment">// this input/output</span></div><div class="line">        });</div><div class="line"><span class="comment">// The state of the workspace is undefined here</span></div></div><!-- fragment --><dl class="section warning"><dt>Warning</dt><dd>Do not confuse <b>workspace</b> with the <a class="el" href="dev_guide_attributes_scratchpad.html">Primitive Attributes: Scratchpad</a>. The scratchpad is a temporary buffer that might be required by a primitive (no matter what propagation kind is) to perform an operation. It is used only during the primitive execution and should not be preserved across the calls. </dd></dl>
</div></div><!-- contents -->
<div class="footer">
    <div class="footer-wrapper">
        <ul id="footer-links">
            <li><a href="legal_information.html">Legal information</a></li>
        </ul>
    </div>
</div>